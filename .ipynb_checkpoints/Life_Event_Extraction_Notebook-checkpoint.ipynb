{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0731 19:59:59.763079 28844 plugins.py:96] Plugin allennlp_models available\n",
      "I0731 19:59:59.788581 28844 archival.py:211] loading archive file C:/Users/Windows dunya/Desktop/fourth year/5th/I17-1099.Datasets/EMNLP_dataset/coref-spanbert-large-2021.03.10(3).tar.gz from cache at C:\\Users\\Windows dunya\\Desktop\\fourth year\\5th\\I17-1099.Datasets\\EMNLP_dataset\\coref-spanbert-large-2021.03.10(3).tar.gz\n",
      "I0731 19:59:59.795759 28844 archival.py:300] extracting archive file C:\\Users\\Windows dunya\\Desktop\\fourth year\\5th\\I17-1099.Datasets\\EMNLP_dataset\\coref-spanbert-large-2021.03.10(3).tar.gz to temp dir C:\\Users\\WINDOW~1\\AppData\\Local\\Temp\\tmpykki_15k\n",
      "W0731 20:00:12.973821 28844 params.py:20] error loading _jsonnet (this is expected on Windows), treating C:\\Users\\WINDOW~1\\AppData\\Local\\Temp\\tmpykki_15k\\config.json as plain json\n",
      "I0731 20:00:12.984021 28844 params.py:221] dataset_reader.type = coref\n",
      "I0731 20:00:12.985994 28844 params.py:221] dataset_reader.max_instances = None\n",
      "I0731 20:00:12.986993 28844 params.py:221] dataset_reader.manual_distributed_sharding = False\n",
      "I0731 20:00:12.987996 28844 params.py:221] dataset_reader.manual_multiprocess_sharding = False\n",
      "I0731 20:00:12.987996 28844 params.py:221] dataset_reader.max_span_width = 30\n",
      "I0731 20:00:12.989056 28844 params.py:221] dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0731 20:00:12.990994 28844 params.py:221] dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0731 20:00:12.991433 28844 params.py:221] dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0731 20:00:12.991993 28844 params.py:221] dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0731 20:00:12.992992 28844 params.py:221] dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0731 20:00:12.994039 28844 params.py:221] dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "I0731 20:00:13.006026 28844 params.py:221] dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0731 20:00:13.006469 28844 params.py:221] dataset_reader.max_sentences = 110\n",
      "I0731 20:00:13.006992 28844 params.py:221] dataset_reader.remove_singleton_clusters = False\n",
      "I0731 20:00:13.007993 28844 params.py:221] validation_dataset_reader.type = coref\n",
      "I0731 20:00:13.009260 28844 params.py:221] validation_dataset_reader.max_instances = None\n",
      "I0731 20:00:13.010022 28844 params.py:221] validation_dataset_reader.manual_distributed_sharding = False\n",
      "I0731 20:00:13.011021 28844 params.py:221] validation_dataset_reader.manual_multiprocess_sharding = False\n",
      "I0731 20:00:13.011993 28844 params.py:221] validation_dataset_reader.max_span_width = 30\n",
      "I0731 20:00:13.011993 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0731 20:00:13.013996 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0731 20:00:13.013996 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0731 20:00:13.014998 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0731 20:00:13.016555 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0731 20:00:13.017423 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "I0731 20:00:13.020996 28844 params.py:221] validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0731 20:00:13.022043 28844 params.py:221] validation_dataset_reader.max_sentences = None\n",
      "I0731 20:00:13.022996 28844 params.py:221] validation_dataset_reader.remove_singleton_clusters = False\n",
      "I0731 20:00:13.023994 28844 params.py:221] type = from_instances\n",
      "I0731 20:00:13.025118 28844 vocabulary.py:349] Loading token dictionary from C:\\Users\\WINDOW~1\\AppData\\Local\\Temp\\tmpykki_15k\\vocabulary.\n",
      "I0731 20:00:13.035027 28844 params.py:221] model.type = coref\n",
      "I0731 20:00:13.036993 28844 params.py:221] model.regularizer = None\n",
      "I0731 20:00:13.036993 28844 params.py:221] model.ddp_accelerator = None\n",
      "I0731 20:00:13.037992 28844 params.py:221] model.text_field_embedder.type = basic\n",
      "I0731 20:00:13.038992 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "I0731 20:00:13.040142 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0731 20:00:13.041992 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "I0731 20:00:13.043040 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.sub_module = None\n",
      "I0731 20:00:13.043040 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
      "I0731 20:00:13.043997 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
      "I0731 20:00:13.044996 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
      "I0731 20:00:13.044996 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
      "I0731 20:00:13.045995 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
      "I0731 20:00:13.046994 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
      "I0731 20:00:13.048148 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
      "I0731 20:00:13.049001 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
      "I0731 20:00:13.049001 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
      "I0731 20:00:17.947052 28844 params.py:221] model.context_layer.type = pass_through\n",
      "I0731 20:00:17.947377 28844 params.py:221] model.context_layer.input_dim = 1024\n",
      "I0731 20:00:17.949461 28844 params.py:221] model.mention_feedforward.input_dim = 3092\n",
      "I0731 20:00:17.949620 28844 params.py:221] model.mention_feedforward.num_layers = 2\n",
      "I0731 20:00:17.950453 28844 params.py:221] model.mention_feedforward.hidden_dims = 1500\n",
      "I0731 20:00:17.950453 28844 params.py:221] model.mention_feedforward.activations = relu\n",
      "I0731 20:00:17.951452 28844 params.py:221] type = relu\n",
      "I0731 20:00:17.952692 28844 params.py:221] model.mention_feedforward.dropout = 0.3\n",
      "I0731 20:00:18.009455 28844 params.py:221] model.antecedent_feedforward.input_dim = 9296\n",
      "I0731 20:00:18.010515 28844 params.py:221] model.antecedent_feedforward.num_layers = 2\n",
      "I0731 20:00:18.011453 28844 params.py:221] model.antecedent_feedforward.hidden_dims = 1500\n",
      "I0731 20:00:18.012453 28844 params.py:221] model.antecedent_feedforward.activations = relu\n",
      "I0731 20:00:18.013453 28844 params.py:221] type = relu\n",
      "I0731 20:00:18.014454 28844 params.py:221] model.antecedent_feedforward.dropout = 0.3\n",
      "I0731 20:00:18.136775 28844 params.py:221] model.feature_size = 20\n",
      "I0731 20:00:18.137454 28844 params.py:221] model.max_span_width = 30\n",
      "I0731 20:00:18.138453 28844 params.py:221] model.spans_per_word = 0.4\n",
      "I0731 20:00:18.138540 28844 params.py:221] model.max_antecedents = 50\n",
      "I0731 20:00:18.139453 28844 params.py:221] model.coarse_to_fine = True\n",
      "I0731 20:00:18.141639 28844 params.py:221] model.inference_order = 2\n",
      "I0731 20:00:18.142453 28844 params.py:221] model.lexical_dropout = 0.2\n",
      "I0731 20:00:18.142453 28844 params.py:221] model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x000001670C72A8D0>\n",
      "I0731 20:00:18.239861 28844 initializers.py:482] Initializing parameters\n",
      "I0731 20:00:18.245347 28844 initializers.py:501] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0731 20:00:18.247437 28844 initializers.py:507]    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "I0731 20:00:18.248465 28844 initializers.py:507]    _antecedent_feedforward._module._linear_layers.0.weight\n",
      "I0731 20:00:18.249346 28844 initializers.py:507]    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "I0731 20:00:18.249346 28844 initializers.py:507]    _antecedent_feedforward._module._linear_layers.1.weight\n",
      "I0731 20:00:18.250348 28844 initializers.py:507]    _antecedent_scorer._module.bias\n",
      "I0731 20:00:18.250348 28844 initializers.py:507]    _antecedent_scorer._module.weight\n",
      "I0731 20:00:18.251347 28844 initializers.py:507]    _attentive_span_extractor._global_attention._module.bias\n",
      "I0731 20:00:18.251347 28844 initializers.py:507]    _attentive_span_extractor._global_attention._module.weight\n",
      "I0731 20:00:18.252346 28844 initializers.py:507]    _coarse2fine_scorer.bias\n",
      "I0731 20:00:18.252346 28844 initializers.py:507]    _coarse2fine_scorer.weight\n",
      "I0731 20:00:18.253347 28844 initializers.py:507]    _distance_embedding.weight\n",
      "I0731 20:00:18.254346 28844 initializers.py:507]    _endpoint_span_extractor._span_width_embedding.weight\n",
      "I0731 20:00:18.254346 28844 initializers.py:507]    _mention_feedforward._module._linear_layers.0.bias\n",
      "I0731 20:00:18.255347 28844 initializers.py:507]    _mention_feedforward._module._linear_layers.0.weight\n",
      "I0731 20:00:18.255347 28844 initializers.py:507]    _mention_feedforward._module._linear_layers.1.bias\n",
      "I0731 20:00:18.256346 28844 initializers.py:507]    _mention_feedforward._module._linear_layers.1.weight\n",
      "I0731 20:00:18.256346 28844 initializers.py:507]    _mention_scorer._module.bias\n",
      "I0731 20:00:18.257346 28844 initializers.py:507]    _mention_scorer._module.weight\n",
      "I0731 20:00:18.257346 28844 initializers.py:507]    _span_updating_gated_sum._gate.bias\n",
      "I0731 20:00:18.258614 28844 initializers.py:507]    _span_updating_gated_sum._gate.weight\n",
      "I0731 20:00:18.258614 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "I0731 20:00:18.259349 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "I0731 20:00:18.260361 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "I0731 20:00:18.261346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0731 20:00:18.261346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "I0731 20:00:18.262382 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.263347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.263347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0731 20:00:18.264346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0731 20:00:18.265348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0731 20:00:18.266349 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0731 20:00:18.267349 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0731 20:00:18.268347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0731 20:00:18.268347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0731 20:00:18.269670 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0731 20:00:18.270347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0731 20:00:18.270347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0731 20:00:18.271354 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "I0731 20:00:18.272348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0731 20:00:18.273346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0731 20:00:18.273346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0731 20:00:18.274347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.275348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.276351 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0731 20:00:18.277347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0731 20:00:18.278347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0731 20:00:18.279347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0731 20:00:18.279347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0731 20:00:18.280348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0731 20:00:18.281347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0731 20:00:18.282348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0731 20:00:18.282348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0731 20:00:18.283999 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0731 20:00:18.284347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0731 20:00:18.285347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0731 20:00:18.286346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0731 20:00:18.287348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0731 20:00:18.287810 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.288346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.289346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0731 20:00:18.289346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0731 20:00:18.290347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0731 20:00:18.291347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0731 20:00:18.291593 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0731 20:00:18.292537 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0731 20:00:18.293402 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0731 20:00:18.294403 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0731 20:00:18.295405 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "I0731 20:00:18.295405 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0731 20:00:18.296530 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0731 20:00:18.297402 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0731 20:00:18.298008 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0731 20:00:18.298402 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0731 20:00:18.299517 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.300403 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.300784 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0731 20:00:18.301892 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0731 20:00:18.301892 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0731 20:00:18.302931 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0731 20:00:18.302931 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0731 20:00:18.304155 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0731 20:00:18.304855 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0731 20:00:18.305376 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0731 20:00:18.305853 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "I0731 20:00:18.306562 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0731 20:00:18.306853 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0731 20:00:18.307713 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0731 20:00:18.307854 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0731 20:00:18.309114 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0731 20:00:18.309855 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.310919 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.312108 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "I0731 20:00:18.313109 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "I0731 20:00:18.314109 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "I0731 20:00:18.315109 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "I0731 20:00:18.315109 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "I0731 20:00:18.316231 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "I0731 20:00:18.317501 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "I0731 20:00:18.318810 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "I0731 20:00:18.319363 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "I0731 20:00:18.320218 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "I0731 20:00:18.320363 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "I0731 20:00:18.321362 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "I0731 20:00:18.321362 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "I0731 20:00:18.322393 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "I0731 20:00:18.323365 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.323798 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.324364 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "I0731 20:00:18.324364 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "I0731 20:00:18.326190 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "I0731 20:00:18.327617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "I0731 20:00:18.328619 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "I0731 20:00:18.329618 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "I0731 20:00:18.330617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
      "I0731 20:00:18.330617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "I0731 20:00:18.331617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "I0731 20:00:18.331617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "I0731 20:00:18.332751 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "I0731 20:00:18.333618 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "I0731 20:00:18.334017 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "I0731 20:00:18.334522 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "I0731 20:00:18.335578 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.336424 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.336529 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "I0731 20:00:18.337529 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "I0731 20:00:18.337529 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "I0731 20:00:18.338528 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "I0731 20:00:18.339620 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "I0731 20:00:18.339620 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "I0731 20:00:18.340718 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
      "I0731 20:00:18.340718 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "I0731 20:00:18.342051 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "I0731 20:00:18.342734 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "I0731 20:00:18.344582 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "I0731 20:00:18.344582 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "I0731 20:00:18.345582 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "I0731 20:00:18.346581 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "I0731 20:00:18.346900 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.347903 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.348067 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "I0731 20:00:18.348902 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "I0731 20:00:18.349900 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "I0731 20:00:18.350693 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "I0731 20:00:18.350903 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "I0731 20:00:18.352165 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "I0731 20:00:18.352903 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "I0731 20:00:18.352903 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "I0731 20:00:18.354045 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "I0731 20:00:18.354902 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "I0731 20:00:18.355268 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "I0731 20:00:18.355902 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "I0731 20:00:18.356902 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "I0731 20:00:18.357832 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "I0731 20:00:18.357859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.358862 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.360152 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "I0731 20:00:18.361469 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "I0731 20:00:18.361859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "I0731 20:00:18.362859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "I0731 20:00:18.362859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
      "I0731 20:00:18.363859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "I0731 20:00:18.363859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "I0731 20:00:18.365003 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "I0731 20:00:18.365875 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "I0731 20:00:18.367141 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "I0731 20:00:18.367860 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "I0731 20:00:18.368859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "I0731 20:00:18.369860 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "I0731 20:00:18.370860 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "I0731 20:00:18.371860 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.372146 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.373148 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "I0731 20:00:18.374148 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "I0731 20:00:18.375774 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "I0731 20:00:18.376252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "I0731 20:00:18.377473 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
      "I0731 20:00:18.378253 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "I0731 20:00:18.379251 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "I0731 20:00:18.379251 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "I0731 20:00:18.380252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "I0731 20:00:18.381252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "I0731 20:00:18.382319 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "I0731 20:00:18.382319 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "I0731 20:00:18.383499 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "I0731 20:00:18.384253 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "I0731 20:00:18.385251 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.386280 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.387252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "I0731 20:00:18.387730 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "I0731 20:00:18.388590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "I0731 20:00:18.389250 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "I0731 20:00:18.390026 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "I0731 20:00:18.390252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "I0731 20:00:18.391307 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "I0731 20:00:18.391307 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "I0731 20:00:18.392252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "I0731 20:00:18.393251 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "I0731 20:00:18.394258 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "I0731 20:00:18.395252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "I0731 20:00:18.395252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "I0731 20:00:18.396282 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "I0731 20:00:18.397252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.398394 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.399333 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "I0731 20:00:18.399333 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "I0731 20:00:18.400609 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
      "I0731 20:00:18.400609 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "I0731 20:00:18.401398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "I0731 20:00:18.402397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "I0731 20:00:18.402397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "I0731 20:00:18.403397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "I0731 20:00:18.403397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "I0731 20:00:18.404410 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "I0731 20:00:18.404410 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "I0731 20:00:18.405617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "I0731 20:00:18.406397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "I0731 20:00:18.406707 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "I0731 20:00:18.407397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.407709 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.408397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0731 20:00:18.409400 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0731 20:00:18.409400 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "I0731 20:00:18.410506 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0731 20:00:18.411397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0731 20:00:18.411693 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0731 20:00:18.412399 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0731 20:00:18.413398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0731 20:00:18.413990 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0731 20:00:18.414397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0731 20:00:18.414999 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0731 20:00:18.415397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0731 20:00:18.416174 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0731 20:00:18.416397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0731 20:00:18.417397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.417397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.418397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "I0731 20:00:18.418661 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "I0731 20:00:18.419397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "I0731 20:00:18.420399 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "I0731 20:00:18.420399 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "I0731 20:00:18.421482 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "I0731 20:00:18.422398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "I0731 20:00:18.422999 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "I0731 20:00:18.423398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "I0731 20:00:18.424398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "I0731 20:00:18.425398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "I0731 20:00:18.425398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "I0731 20:00:18.426398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "I0731 20:00:18.426878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "I0731 20:00:18.427880 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.428881 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.429175 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
      "I0731 20:00:18.429878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "I0731 20:00:18.430878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "I0731 20:00:18.431314 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "I0731 20:00:18.432342 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "I0731 20:00:18.432881 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "I0731 20:00:18.433574 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "I0731 20:00:18.433878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "I0731 20:00:18.434583 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "I0731 20:00:18.434583 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "I0731 20:00:18.435818 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "I0731 20:00:18.436592 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "I0731 20:00:18.436878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "I0731 20:00:18.437591 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "I0731 20:00:18.437791 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.438842 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.438842 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
      "I0731 20:00:18.439988 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "I0731 20:00:18.440590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "I0731 20:00:18.440590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "I0731 20:00:18.441590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "I0731 20:00:18.442943 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "I0731 20:00:18.443591 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "I0731 20:00:18.444591 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "I0731 20:00:18.445590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "I0731 20:00:18.445590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "I0731 20:00:18.446702 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "I0731 20:00:18.447589 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "I0731 20:00:18.447996 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "I0731 20:00:18.448591 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "I0731 20:00:18.449224 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.449590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.450590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "I0731 20:00:18.450590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "I0731 20:00:18.451630 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "I0731 20:00:18.451630 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "I0731 20:00:18.453015 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "I0731 20:00:18.453590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "I0731 20:00:18.454141 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "I0731 20:00:18.455220 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "I0731 20:00:18.455220 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "I0731 20:00:18.456520 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "I0731 20:00:18.458445 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "I0731 20:00:18.459143 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "I0731 20:00:18.460142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "I0731 20:00:18.461143 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "I0731 20:00:18.461581 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.462142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.462142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0731 20:00:18.463183 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0731 20:00:18.464142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0731 20:00:18.464570 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0731 20:00:18.465698 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0731 20:00:18.466142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0731 20:00:18.467433 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0731 20:00:18.468173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0731 20:00:18.468610 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0731 20:00:18.469172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0731 20:00:18.470172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0731 20:00:18.471171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0731 20:00:18.471577 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0731 20:00:18.472748 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0731 20:00:18.473289 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.474172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.475522 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0731 20:00:18.476171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0731 20:00:18.477172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0731 20:00:18.477172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0731 20:00:18.478172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0731 20:00:18.479173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0731 20:00:18.480171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0731 20:00:18.480623 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0731 20:00:18.481172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0731 20:00:18.482173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0731 20:00:18.482173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0731 20:00:18.483195 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0731 20:00:18.484568 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0731 20:00:18.485172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0731 20:00:18.485172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.486172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.487171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0731 20:00:18.488183 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0731 20:00:18.488183 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0731 20:00:18.489314 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0731 20:00:18.490432 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0731 20:00:18.491173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0731 20:00:18.491702 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0731 20:00:18.492172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0731 20:00:18.493173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0731 20:00:18.493173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0731 20:00:18.494173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0731 20:00:18.495281 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0731 20:00:18.495281 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0731 20:00:18.496459 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "I0731 20:00:18.497172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.497839 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.498173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0731 20:00:18.499172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0731 20:00:18.499172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0731 20:00:18.500282 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0731 20:00:18.500282 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0731 20:00:18.501173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0731 20:00:18.501173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0731 20:00:18.502443 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0731 20:00:18.503173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0731 20:00:18.503701 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0731 20:00:18.504173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0731 20:00:18.505173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0731 20:00:18.505173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0731 20:00:18.506533 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "I0731 20:00:18.507172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.508171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.509172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0731 20:00:18.509380 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0731 20:00:18.510173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0731 20:00:18.511171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0731 20:00:18.512196 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0731 20:00:18.512196 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0731 20:00:18.513598 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0731 20:00:18.514171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0731 20:00:18.515171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0731 20:00:18.515171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0731 20:00:18.516176 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0731 20:00:18.517174 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0731 20:00:18.517475 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0731 20:00:18.518171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0731 20:00:18.518171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.519211 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.520172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0731 20:00:18.520563 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0731 20:00:18.521778 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0731 20:00:18.522172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0731 20:00:18.523173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0731 20:00:18.523173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0731 20:00:18.524172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0731 20:00:18.524172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0731 20:00:18.525280 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0731 20:00:18.526453 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0731 20:00:18.526479 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0731 20:00:18.527452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "I0731 20:00:18.527623 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0731 20:00:18.529001 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0731 20:00:18.529452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.530139 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.531127 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0731 20:00:18.531452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0731 20:00:18.532451 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0731 20:00:18.532451 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0731 20:00:18.533452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0731 20:00:18.533452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0731 20:00:18.534473 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0731 20:00:18.535516 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0731 20:00:18.535516 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0731 20:00:18.536572 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0731 20:00:18.536572 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0731 20:00:18.537877 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "I0731 20:00:18.538614 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0731 20:00:18.539613 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0731 20:00:18.553882 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "I0731 20:00:18.554883 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "I0731 20:00:18.575756 28844 embedding.py:255] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0731 20:00:18.576756 28844 embedding.py:255] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0731 20:00:19.829408 28844 archival.py:243] removing temporary unarchived model dir at C:\\Users\\WINDOW~1\\AppData\\Local\\Temp\\tmpykki_15k\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy \n",
    "import os\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])\n",
    "\n",
    "\n",
    "java_path = \"Java/jre1.8.0_251/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "st = StanfordNERTagger(r'SentiSE-master/edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz',\n",
    "                           r'stanford-ner-2020-11-17/stanford-ner.jar',\n",
    "                           encoding='utf-8')\n",
    "\n",
    "\n",
    "predictor = Predictor.from_path(\"I17-1099.Datasets/EMNLP_dataset/coref-spanbert-large-2021.03.10(3).tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"77801468485442c6919e0352044ee305-0\" class=\"displacy\" width=\"1975\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">i</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">heard</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">she</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">passed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">bar</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">exam</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">married</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">recently</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-1\" stroke-width=\"2px\" d=\"M420,439.5 C420,264.5 735.0,264.5 735.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-2\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-3\" stroke-width=\"2px\" d=\"M245,439.5 C245,177.0 740.0,177.0 740.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,441.5 L748.0,429.5 732.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-4\" stroke-width=\"2px\" d=\"M945,439.5 C945,264.5 1260.0,264.5 1260.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-5\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-6\" stroke-width=\"2px\" d=\"M770,439.5 C770,177.0 1265.0,177.0 1265.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,441.5 L1273.0,429.5 1257.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-7\" stroke-width=\"2px\" d=\"M770,439.5 C770,89.5 1445.0,89.5 1445.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,441.5 L1453.0,429.5 1437.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-8\" stroke-width=\"2px\" d=\"M770,439.5 C770,2.0 1625.0,2.0 1625.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,441.5 L1633.0,429.5 1617.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-9\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,352.0 1780.0,352.0 1780.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1780.0,441.5 L1788.0,429.5 1772.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\"]\n",
    "COMPOUNDS = [\"compound\"]\n",
    "PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "\n",
    "doc=\"i heard she has passed the bar exam and married recently \"\n",
    "\n",
    "doc = nlp(doc)\n",
    "displacy.render(doc, style='dep',jupyter=True)\n",
    "\n",
    "\n",
    "#accept(nsubjectpass) finish approved completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i --> nsubj --> PRON --> I  ... heard ,,, 0\n",
      "heard --> ROOT --> VERB --> hear  ... heard ,,, 1\n",
      "she --> nsubj --> PRON --> she  ... passed ,,, 0\n",
      "has --> aux --> AUX --> have  ... passed ,,, 0\n",
      "passed --> ccomp --> VERB --> pass  ... heard ,,, 2\n",
      "the --> det --> DET --> the  ... exam ,,, 0\n",
      "bar --> compound --> NOUN --> bar  ... exam ,,, 0\n",
      "exam --> dobj --> NOUN --> exam  ... passed ,,, 2\n",
      "and --> cc --> CCONJ --> and  ... passed ,,, 0\n",
      "married --> conj --> VERB --> marry  ... passed ,,, 0\n",
      "recently --> advmod --> ADV --> recently  ... married ,,, 0\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(doc)\n",
    "for tok in doc: \n",
    "    head=tok.head\n",
    "    print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_,\"-->\",tok.lemma_,\" ...\", tok.head,\",,,\",tok.n_lefts) \n",
    "    if tok.pos_==\"VERB\":\n",
    "        for i in tok.rights:\n",
    "            if str(i.text)==\"divorce\" and i.dep_==\"dobj\":\n",
    "                print(\".........................\",i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intializing lists of  events keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_direct_relation=[\"wedding\",\"anniversary\",\"graduation\",\"birthday\",\"interview\",\"meeting\",\"resignation\",\"job\"]\n",
    "nouns=[\"pain\",\"birth\",\"surgery\",\"job\",\"cold\",\"flu\",\"fever\",\"wedding\",\"meeting\",\"interview\",\"offer\",\"promotion\",\"headache\",\"exam\",\"divorce\",\"contract\",\"relationship\",\"diploma\",\"masters\"]#as object to the aux verb\n",
    "verbs_without_need_objects=[\"wed\",\"divorce\",\"divorced\",\"elect\",\"engage\",\"die\",\"married\",\"nominate\",\"promote\",\"marry\",\"graduate\",\"bear\",\"hurt\",\"meet\",\"interview\"]\n",
    "verb_with_prepo=[\"move\",\"go\",\"travel\"]\n",
    "adjectives=[\"sick\",\"ill\",\"pregnant\"]\n",
    "pronoun=[\"my\",\"his\",\"her\",\"their\",\"your\"]\n",
    "prepo=[\"to\",\"into\",\"in\",\"on\",\"for\"]\n",
    "verbs_with_objects={\"celebrate\":[\"birthday\",\"birth\",\"graduation\",\"anniversary\",\"success\",\"news\"],\n",
    "                   \"graduate\":[\"collage\",\"school\",\"university\"],\n",
    "                   \"win\":[\"support\",\"award\",\"honor\",\"scholarship\",\"prize\",\"lawsuit\",\"pounds\",\"weight\"],\n",
    "                   \"lose\":[\"support\",\"award\",\"honor\",\"scholarship\",\"prize\",\"lawsuit\",\"pounds\",\"weight\"],\n",
    "                   \"admit\":[\"university\",\"collage\",\"offer\",\"school\"],\n",
    "                   \"pass\":[\"exam\",\"test\",\"semester\",\"midterms\",\"school\",\"interview\",\"meeting\"],\n",
    "                   \"present\":[\"essay\",\"thesis\",\"reading\",\"statment\",\"presentation\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"discuss\":[\"essay\",\"thesis\",\"reading\",\"statment\",\"presentation\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"defend\":[\"essay\",\"thesis\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"move\":[\"city\",\"home\",\"apartment\",\"town\"],\n",
    "                   \"travel\":[\"city\",\"home\",\"apartment\",\"town\"],\n",
    "                   \"contract\":[\"agreement\",\"meeting\"],\n",
    "                   \"act\":[\"role\",\"series\",\"movie\",\"theater\"],\n",
    "                   \"publish\":[\"book\",\"post\",\"cover\",\"copy\"],\n",
    "                   \"pregnant\":[\"baby\",\"boy\",\"girl\"],\n",
    "                   \"buy\":[\"car\",\"house\",\"phone\",\"laptope\"],\n",
    "                   \"accept\":[\"invite\",\"work\",\"school\",\"business \",\"university\",\"job\",\"offer\",\"college\",\"program\",\"project\",\"research\",\"paper\",\"invitation\"],\n",
    "                   \"visit\":[\"hospital\",\"doctor\",\"city\",\"country\"],\n",
    "                   \"go\":[\"trip\",\"vacation\",\"holiday\"],\n",
    "                   \"sing\":[\"song\",\"album\"],\n",
    "                   \"sold\":[\"car\",\"house\",\"phone\"],\n",
    "                   \"sign\":[\"deal\",\"contruct\"],\n",
    "                   \"finish\":[\"diploma\",\"masters\",\"trip\",\"vacation\",\"holiday\",\"college\",\"university\",\"job\",\"test\",\"exam\",\"school\",\"semester\",\"midterms\",\"album\",\"book\",\"essay\",\"thesis\",\"reading\",\"statment\",\"presentation\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"start\":[\"diploma\",\"masters\",\"trip\",\"vacation\",\"holiday\",\"college\",\"university\",\"job\",\"test\",\"exam\",\"school\",\"semester\",\"midterms\",\"album\",\"book\",\"essay\",\"thesis\",\"reading\",\"statment\",\"presentation\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"break\":[\"leg\",\"arm\",\"finger\",\"neck\",\"head\",\"Back\"],\n",
    "                   \"mark\":[\"anniversary\",\"birthday\",\"birth\",\"graduation\",\"success\",\"death\"],\n",
    "                   \"fail\":[\"test\",\"exam\",\"school\",\"semester\",\"midterms\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rules for extraction events patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rules:\n",
    "    def __init__(self,text,list_of_keywords=[],tokens=[],tokens_=[],pos_tags=[],dependency=[],lemma=[]):\n",
    "        self.text=text\n",
    "        self.list_of_keywords=list_of_keywords\n",
    "        self.tokens=[]\n",
    "        self.tokens_=[]\n",
    "        self.pos_tags=[]\n",
    "        self.dependency=[]\n",
    "        self.lemma=[]\n",
    "        for tok in self.text: \n",
    "            self.tokens.append(tok.text)\n",
    "            self.tokens_.append(tok)\n",
    "            self.pos_tags.append(tok.pos_)\n",
    "            self.dependency.append(tok.dep_)\n",
    "            self.lemma.append(tok.lemma_)\n",
    "        \n",
    "\n",
    "    def retrival_senetce_rule1(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            \n",
    "                if lemm in self.list_of_keywords and (self.pos_tags[i]=='VERB' ):\n",
    "                    #or self.pos_tags[i]=='ADV' or self.pos_tags[i]=='ADJ' or self.pos_tags[i]=='NOUN'\n",
    "                    return self.text,self.tokens[i]\n",
    "             \n",
    "\n",
    "            \n",
    "    def retrival_senetce_rule2(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if lemm in self.list_of_keywords and self.pos_tags[i]=='VERB':\n",
    "                for t in self.tokens_[i].rights:\n",
    "                    if t.dep_==\"dobj\" and str(t.lemma_) in self.list_of_keywords[lemm]:\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                    \n",
    "                    elif t.dep_==\"dobj\" :\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"prep\" :\n",
    "                                for t2 in t1.rights:\n",
    "                                    if t2.dep_==\"pobj\" and str(t2.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                        return self.text,self.tokens[i]+\" \"+t2.text\n",
    "\n",
    "                    elif t.dep_==\"oprd\" and str(t.lemma_) in self.list_of_keywords[lemm] :\n",
    "                        return self.text,lemmself.tokens[i]+\" \"+t.text\n",
    "\n",
    "                    elif t.dep_==\"prep\":\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"pobj\" and str(t1.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                return self.text,self.tokens[i]+\" \"+t1.text\n",
    "                            \n",
    "                            elif t1.dep_==\"pobj\":\n",
    "                                for t4 in t1.lefts:\n",
    "                                    if t4.dep_==\"compound\" and str(t4.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                        return self.text,self.tokens[i]+\" \"+t4.text\n",
    "                 \n",
    "                                    \n",
    "                for t in self.tokens_[i].lefts:\n",
    "                    if t.dep_==\"nsubjpass\" and str(t) in self.list_of_keywords[lemm]:\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                     \n",
    "            \n",
    "            \n",
    "    def retrival_senetce_rule3(self):  \n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if self.pos_tags[i]=='VERB'or self.pos_tags[i]== 'AUX':\n",
    "                for t in self.tokens_[i].rights:\n",
    "                    if t.dep_==\"dobj\" and str(t) in self.list_of_keywords:\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                    \n",
    "                    elif t.dep_==\"prep\":\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"pobj\" and str(t1) in self.list_of_keywords:\n",
    "                                return self.text,self.tokens[i]+\" \"+t1.text\n",
    "                    \n",
    "                    elif t.dep_==\"dobj\" :\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"prep\" :\n",
    "                                for t2 in t1.rights:\n",
    "                                    if t2.dep_==\"pobj\" and str(t2) in self.list_of_keywords:\n",
    "                                        return self.text,self.tokens[i]+\" \"+t2.text\n",
    "                                    \n",
    "                     \n",
    "            \n",
    "            \n",
    "        \n",
    "    def retrival_senetce_rule4(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if self.pos_tags[i]==\"AUX\":\n",
    "                for t in self.tokens_[i].rights:\n",
    "                    if t.dep_==\"acomp\" and str(t) in self.list_of_keywords and t.pos_==\"ADJ\":\n",
    "                        print(\"ad \", t.text)\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                    \n",
    "                \n",
    "            elif self.pos_tags[i]==\"VERB\":\n",
    "                for t in self.tokens_[i].rights:\n",
    "                     if t.dep_==\"oprd\" and str(t) in self.list_of_keywords and (t.pos_==\"NOUN\" or t.pos_==\"ADJ\"):\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                     \n",
    "            \n",
    "    \n",
    "    \n",
    "    def retrival_senetce_rule5(self): \n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if (self.pos_tags[i]==\"PRON\" or self.pos_tags[i]==\"PROPN\" ) :\n",
    "                head=self.tokens_[i].head\n",
    "                if self.tokens_[i].dep_==\"poss\" and str(head) in self.list_of_keywords and head.pos_==\"NOUN\":\n",
    "                        return self.text,self.tokens[i]+\" \"+head.text\n",
    "                    \n",
    "                elif self.tokens_[i].dep_==\"poss\" and head.pos_==\"NOUN\":\n",
    "                    for t in head.lefts:\n",
    "                        if str(t) in self.list_of_keywords and  t.dep_==\"compound\":\n",
    "                            return self.text,self.tokens[i]+\" \"+t.text\n",
    "                        \n",
    "                \n",
    "                            \n",
    "    def retrival_senetce_rule6(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if lemm in self.list_of_keywords and self.pos_tags[i]=='VERB':\n",
    "                for t in self.tokens_[i].rights:\n",
    "                    if t.dep_==\"prep\" and str(t) in prepo:\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                \"\"\"for t in self.tokens_[i].lefts:\n",
    "                    if t.dep_==\"aux\" and str(t) == \"to\":\n",
    "                        return None\"\"\"\n",
    "                    \n",
    "                    \n",
    "    def retrival_senetce_rule7(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if lemm in self.list_of_keywords and (self.pos_tags[i]=='VERB' or self.pos_tags[i]=='NOUN' or self.pos_tags[i]=='ADJ'):\n",
    "                    if self.tokens_[i].dep_==\"ROOT\" :\n",
    "                        for t in self.tokens_[i].rights:\n",
    "                            if t.dep_==\"dobj\" and str(t.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                return self.text\n",
    "\n",
    "                            elif t.dep_==\"dobj\" :\n",
    "                                for t1 in t.rights:\n",
    "                                    if t1.dep_==\"prep\" :\n",
    "                                        for t2 in t1.rights:\n",
    "                                            if t2.dep_==\"pobj\" and str(t2.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                                return self.text\n",
    "\n",
    "                            elif t.dep_==\"oprd\" and str(t.lemma_) in self.list_of_keywords[lemm] :\n",
    "                                return self.text\n",
    "\n",
    "                            elif t.dep_==\"prep\":\n",
    "                                for t1 in t.rights:\n",
    "                                    if t1.dep_==\"pobj\" and str(t1.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                        return self.text\n",
    "                                    elif t1.dep_==\"pobj\":\n",
    "                                        for t4 in t1.lefts:\n",
    "                                            if t4.dep_==\"compound\" and str(t4.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                                return self.text\n",
    "        \n",
    "    \"\"\"\n",
    "                            \n",
    "    def retrival_senetce_rule10(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if (self.pos_tags[i]=='VERB' or self.pos_tags[i]==\"AUX\"):\n",
    "                    for t in self.tokens_[i].rights:\n",
    "                        if t.dep_==\"acomp\" and str(t) in self.list_of_keywords and (t.pos_==\"ADJ\" or t.pos_==\"VERB\" or t.pos_==\"ADV\"): \n",
    "                            return self.text\n",
    "                        \n",
    "    def retrival_senetce_rule11(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if ( self.pos_tags[i]==\"AUX\" and self.dependency[i]==\"auxpass\"):\n",
    "                        head=self.tokens_[i].head\n",
    "                        if  str(head) in self.list_of_keywords and (head.pos_==\"ADJ\" or head.pos_==\"VERB\"): \n",
    "                            return self.text\n",
    "                            \n",
    "                            \"\"\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(you and me will married this month ., 'married')"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"you and me will married this month .\"\n",
    "doc=nlp(doc)\n",
    "p1=Rules(doc, verbs_without_need_objects)\n",
    "text=p1.retrival_senetce_rule1()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(he passed the bar exam, 'passed exam')"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"he passed the bar exam\"\n",
    "doc=nlp(doc)\n",
    "p2=Rules(doc, verbs_with_objects)\n",
    "text=p2.retrival_senetce_rule2()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(i got a surgery, 'got surgery')"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"i got a surgery\"\n",
    "doc=nlp(doc)\n",
    "p3=Rules(doc, nouns)\n",
    "text=p3.retrival_senetce_rule3()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad  sick\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(i am very sick, 'am sick')"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"i am very sick\"\n",
    "doc=nlp(doc)\n",
    "p4=Rules(doc, adjectives)\n",
    "text=p4.retrival_senetce_rule4()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(my wedding was last year, 'my wedding')"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"my wedding was last year\"\n",
    "doc=nlp(doc)\n",
    "p5=Rules(doc, noun_direct_relation)\n",
    "text=p5.retrival_senetce_rule5()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(i am moving to USA, 'moving to')"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"i am moving to USA\"\n",
    "doc=nlp(doc)\n",
    "p6=Rules(doc, verb_with_prepo)\n",
    "text=p6.retrival_senetce_rule6()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking persons subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[\"i\",\"we\",\"you\",\"they\",\"he\",\"she\",\"my\",\"his\",\"her\",\"their\",\"your\",\"our\",\"me\",\"him\",\"them\"]\n",
    "def is_person(text,subject,st=st):\n",
    "    x=0\n",
    "    tokens=[]\n",
    "    for token in text :\n",
    "        tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if str(token) == str(subject):  \n",
    "            lis=str(text).split()\n",
    "            for i,j in enumerate(lis):\n",
    "                j=j.replace(\"'s\",\"\")\n",
    "                lis[i]=j\n",
    "            #print(lis)\n",
    "            \n",
    "            #print(tagged)\n",
    "            if token.pos_==\"NOUN\":\n",
    "                tagged = st.tag(lis)\n",
    "                for i,j in enumerate(tagged):\n",
    "                    if 'PERSON' in j:\n",
    "                        if str(j[0])==str(subject):\n",
    "                            return True \n",
    "                        if str(j[0])!=str(subject):\n",
    "                            return False\n",
    "                for i,j in enumerate(tagged):\n",
    "                    if 'PERSON' not in j:\n",
    "                        x+=1\n",
    "                        if x==len(tokens):\n",
    "                            return False\n",
    "                    \n",
    "            elif token.pos_==\"PRON\" and (str(token) in p):\n",
    "                return True\n",
    "            elif token.pos_==\"PRON\" and (str(token) not in p):\n",
    "                return False\n",
    "            \n",
    "            elif token.pos_==\"PROPN\":\n",
    "                return True\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"george divorced katty and take her alone\"\n",
    "text=nlp(text)\n",
    "is_person(text,\"george\",st=st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rules for subjects extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pronoun=[\"me\",\"him\",\"her\",\"them\",\"you\",\"us\"]\n",
    "p_pronoun=[\"my\",\"his\",\"her\",\"their\",\"your\",\"our\"]\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def find_obj_for_verb(doc,verb):\n",
    "    tokens=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "            if str(token) == str(verb):\n",
    "                for t in tokens[i].rights:\n",
    "                    if t.dep_==\"dobj\" :\n",
    "                        return t\n",
    "\n",
    "                    elif t.dep_==\"prep\":\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"pobj\" :\n",
    "                                return t1\n",
    "                                    \n",
    "                for t in tokens[i].lefts:\n",
    "                    if t.dep_==\"nsubjpass\" :\n",
    "                        if token.n_rights>0:\n",
    "                            for t1 in token.rights:\n",
    "                                if t1.dep_ == \"agent\":\n",
    "                                    return t\n",
    "                                    \n",
    "    \n",
    "    \n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    rightDeps=[]\n",
    "    for sub in subs:\n",
    "        for tok in sub.rights:\n",
    "            if tok.dep_ ==\"conj\" :\n",
    "                    #.................................................................\n",
    "                    subs=chek_pronoun_verb(tok)\n",
    "                    moreSubs.append(subs)\n",
    "            if len(moreSubs) > 0:\n",
    "                \n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "            \n",
    "        \n",
    "\n",
    "def chek_pronoun_verb(tok):\n",
    "    if  tok.n_lefts>0:\n",
    "        for s in (tok.lefts):\n",
    "            if  (s.dep_==\"poss\") and str(s) in p_pronoun :\n",
    "                return s\n",
    "            elif  (s.dep_==\"poss\") and (s.pos_==\"PROPN\" or s.pos_==\"NOUN\"):\n",
    "                return s\n",
    "            else:\n",
    "                return tok\n",
    "    else :\n",
    "        return tok\n",
    "\n",
    "def chek_left_sub(tok):\n",
    "    if  tok.n_lefts>0:\n",
    "        for s in tok.lefts:\n",
    "            if  (s.dep_==\"compound\") and (s.pos_==\"NOUN\" or s.pos_==\"PROPN\") :\n",
    "                return s\n",
    "            else :\n",
    "                return tok\n",
    "    else :\n",
    "        return tok\n",
    "\n",
    "def find_sub_for_verb(doc,verb):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    objs=[]\n",
    "    s=[]\n",
    "    e=[]\n",
    "    p=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "            if str(token) == str(verb):\n",
    "                for tok in token.lefts:                    \n",
    "                    if tok.dep_ == \"auxpass\" :\n",
    "                        if tok.pos_=='AUX' :\n",
    "                            for t1 in tok.lefts:\n",
    "                                if t1.dep_==\"nsubj\" or t1.dep_==\"nsubjpass\" :\n",
    "                                    subs.append(t1)\n",
    "                                            \n",
    "                                    \n",
    "                    elif tok.dep_ == \"nsubjpass\":\n",
    "                        objs.append(tok)\n",
    "                        if token.n_rights>0:\n",
    "                            for t in token.rights:\n",
    "                                if t.dep_ == \"agent\":\n",
    "                                    for t1 in t.rights:\n",
    "                                        if t1.dep_==\"pobj\":\n",
    "                                            subs.append(t1)\n",
    "                                else:\n",
    "                                    subs.append(tok)\n",
    "\n",
    "                        else:\n",
    "                            subs.append(tok)\n",
    "                           \n",
    "                        \n",
    "                    elif tok.dep_ == \"nsubj\" :\n",
    "                        if is_person(doc,tok)==True:\n",
    "                                subs.append(tok)\n",
    "                        \n",
    "                        else :\n",
    "                            for t3 in tok.lefts:\n",
    "                                if t3.dep_==\"poss\" and (t3.pos_==\"NOUN\" or t3.pos_==\"PROPN\" or t3.pos_==\"PRON\"):\n",
    "                                    subs.append(tok)\n",
    "                                else:    \n",
    "                                    for t2 in token.rights:\n",
    "                                        if t2.dep_ == \"nsubj\" :\n",
    "                                            #if is_person(doc,t2)==True:\n",
    "                                            subs.append(t2)\n",
    "                                    \n",
    "                                \n",
    "                                \n",
    "                        \n",
    "            #kareem get to finish his masters \n",
    "            if len(subs)==0:\n",
    "                if token.dep_ ==\"acomp\" or token.dep_ ==\"xcomp\":\n",
    "                        head=token.head\n",
    "                        if head.pos_==\"VERB\" or head.pos_==\"AUX\":\n",
    "                            for t in head.lefts:\n",
    "                                if t.dep_==\"nsubj\" or t.dep_==\"nsubjpass\":\n",
    "                                    subs.append(t)\n",
    "                                    \n",
    "                elif token.dep_==\"conj\":\n",
    "                    #print(\"conj 1\")\n",
    "                    h=token.head\n",
    "                    if h.pos_==\"VERB\":\n",
    "                        sub=find_sub_for_verb(doc,h)[0]\n",
    "                        if sub :\n",
    "                            subs.append(sub) \n",
    "                        else:\n",
    "                            subs.append(None)\n",
    "                        \n",
    "                \n",
    "                                \n",
    "                \n",
    "                    \n",
    "                \n",
    "            if len(subs) > 0:\n",
    "                subs = list(pd.Series(subs).drop_duplicates())\n",
    "                subs.extend(getSubsFromConjunctions(subs))\n",
    "                for i in subs:\n",
    "                    s.append(chek_pronoun_verb(i))\n",
    "               \n",
    "                for i in s:\n",
    "                    if is_person(doc,i)==False :\n",
    "                        s.remove(i)\n",
    "                \n",
    "                for i in range(len(s)):\n",
    "                    if s[i]!=subs[i] and str(s[i]) in p_pronoun:\n",
    "                        p.append((s[i],subs[i]))\n",
    "                    elif s[i]!=subs[i] and (s[i].pos_==\"PROPN\" or s[i].pos_==\"NOUN\"):\n",
    "                        p.append((s[i],subs[i]))\n",
    "                        \n",
    "                    elif s[i]==subs[i]:\n",
    "                        \n",
    "                        p.append(s[i])\n",
    "                    else:\n",
    "                        p.append(s[i])\n",
    "                        p.append(subs)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def findSubs_for_verbEvents(doc,list_=[]):\n",
    "    \n",
    "    subs=[]\n",
    "    objs=[]\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\" and str(token.lemma_) in list_:\n",
    "            obj=find_obj_for_verb(doc,token)\n",
    "            sub=find_sub_for_verb(doc,token)\n",
    "            if sub:\n",
    "                return sub\n",
    "                \n",
    "            else:\n",
    "                if obj!=None:\n",
    "                    if str(obj) in obj_pronoun:\n",
    "                        return [obj]\n",
    "\n",
    "\n",
    "\n",
    "def findSubs_for_direct_relation(doc,list_=[]):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if (tokens[i].pos_==\"PRON\" or tokens[i].pos_==\"PROPN\" ) :\n",
    "            head=tokens[i].head\n",
    "            if tokens[i].dep_==\"poss\" and str(head) in list_ and head.pos_==\"NOUN\":\n",
    "                subs.append(token) \n",
    "                    \n",
    "            elif tokens[i].dep_==\"poss\" and head.pos_==\"NOUN\":\n",
    "                for t in head.lefts:\n",
    "                    if str(t) in list_ and  t.dep_==\"compound\":\n",
    "                        subs.append(token) \n",
    "        if len(subs) > 0:\n",
    "                return subs \n",
    "            \n",
    "        \n",
    "def findSubs_for_nounsEvents(doc,list_=[]):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if str(token) in list_:\n",
    "            \n",
    "            \n",
    "            s=chek_pronoun_verb(token)\n",
    "            if str(s) == str(token):\n",
    "                \n",
    "                for i,token in enumerate(tokens):\n",
    "                    if tokens[i].pos_=='VERB'or tokens[i].pos_== 'AUX':\n",
    "                        obj=find_obj_for_verb(doc,token)\n",
    "                        sub=find_sub_for_verb(doc,token)\n",
    "                        for t in tokens[i].rights:\n",
    "                            if t.dep_==\"dobj\" and str(t) in list_:\n",
    "                                if sub :\n",
    "                                    if len(sub)>0:\n",
    "                                        return sub \n",
    "\n",
    "\n",
    "                            elif t.dep_==\"prep\":\n",
    "                                for t1 in t.rights:\n",
    "                                    if t1.dep_==\"pobj\" and str(t1) in list_:\n",
    "                                        if sub:\n",
    "                                            if len(sub)>0:\n",
    "                                                return sub \n",
    "\n",
    "                            elif t.dep_==\"dobj\" :\n",
    "                                for t1 in t.rights:\n",
    "                                    if t1.dep_==\"prep\" :\n",
    "                                        for t2 in t1.rights:\n",
    "                                            if t2.dep_==\"pobj\" and str(t2) in list_:\n",
    "                                                if sub:\n",
    "                                                    if len(sub)>0:\n",
    "                                                        return sub \n",
    "            \n",
    "            elif s !=None and str(s) != str(token) :\n",
    "                return [s]\n",
    "\n",
    "                       \n",
    "                                    \n",
    "\n",
    "def findSubs_for_adjectiveEvents(doc,list_=[]):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "            if tokens[i].pos_==\"AUX\":\n",
    "                obj=find_obj_for_verb(doc,token)\n",
    "                sub=find_sub_for_verb(doc,token)\n",
    "                for t in tokens[i].rights:\n",
    "                    if t.dep_==\"acomp\" and str(t) in list_ and t.pos_==\"ADJ\":\n",
    "                        if sub:\n",
    "                            if len(sub )>0:\n",
    "                                return sub\n",
    "                \n",
    "            elif tokens[i].pos_==\"VERB\":\n",
    "                obj=find_obj_for_verb(doc,token)\n",
    "                sub=find_sub_for_verb(doc,token)\n",
    "                for t in tokens[i].rights:\n",
    "                     if t.dep_==\"oprd\" and str(t) in list_ and (t.pos_==\"NOUN\" or t.pos_==\"ADJ\"):\n",
    "                        if sub :\n",
    "                            if len(sub)>0:\n",
    "                                return sub\n",
    "     \n",
    "                            \n",
    "def findSubs_for_actionEvents(doc,list_=[]):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    for token in doc :\n",
    "        tokens.append(token)\n",
    "        for i,token in enumerate(tokens):\n",
    "            if str(tokens[i].lemma_) in list_ and tokens[i].pos_=='VERB':\n",
    "                obj=find_obj_for_verb(doc,token)\n",
    "                sub=find_sub_for_verb(doc,token)\n",
    "                for t in tokens[i].rights:\n",
    "                    if t.dep_==\"prep\" :\n",
    "                        if sub:\n",
    "                            if len(sub)>0:\n",
    "                                return sub                              \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking events extraction and subjects extraction rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's why we want to have a look at other hotels .\n",
      "We dont want to spend too much on an extravagant wedding reception .\n",
      "Iguess you're right .\n",
      "\n",
      "\n",
      "\n",
      "george and my husband will married this week .\n",
      "....  ['Rule1 ', (george and my husband will married this week ., 'married'), [george, my]]\n",
      "\n",
      "\n",
      "\n",
      "george and mari will married this month .\n",
      "....  ['Rule1 ', (george and mari will married this month ., 'married'), [george, mari]]\n",
      "i wish a lot .\n",
      "\n",
      "\n",
      "\n",
      "i and me will married this month .\n",
      "....  ['Rule1 ', (i and me will married this month ., 'married'), [i, me]]\n",
      "i wish a lot .\n",
      "\n",
      "\n",
      "\n",
      "you and me will married this month .\n",
      "....  ['Rule1 ', (you and me will married this month ., 'married'), [you, me]]\n",
      "i wish a lot .\n",
      "\n",
      "\n",
      "\n",
      "I'll stay home this morning and rest , but if I feel better in the afternoon , I'm going to the meeting .\n",
      "....  ['Rule3 ', (i'll stay home this morning and rest , but if i feel better in the afternoon , i'm going to the meeting ., 'going meeting'), [i]]\n",
      "....  ['Rule6 ', (i'll stay home this morning and rest , but if i feel better in the afternoon , i'm going to the meeting ., 'going to'), [i]]\n",
      "\n",
      "\n",
      "\n",
      "How was your meeting with Abigail?\n",
      "....  ['Rule5 ', (how was your meeting with abigail?, 'your meeting'), [your]]\n",
      "we would be in a meeting and she would say , yesterday I was chatting with Tom .\n",
      "....  ['Rule3 ', (we would be in a meeting and she would say , yesterday i was chatting with tom ., 'be meeting'), [we]]\n",
      "\n",
      "\n",
      "\n",
      "yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side .\n",
      "....  ['Rule5 ', (yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side ., 'our wedding'), [our]]\n",
      "\n",
      "\n",
      "\n",
      "yesterday my husband and sally renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side .\n",
      "....  ['Rule5 ', (yesterday my husband and sally renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side ., 'our wedding'), [our]]\n",
      "\n",
      "\n",
      "\n",
      "my husband and george will married this week .\n",
      "....  ['Rule1 ', (my husband and george will married this week ., 'married'), [(my, husband), george]]\n",
      "\n",
      "\n",
      "\n",
      "Mom, did my sister call you, what are the results of her competition?\n",
      "she won a couple of addy awards for her package design .\n",
      "....  ['Rule2 ', (she won a couple of addy awards for her package design ., 'won awards'), [she]]\n",
      "Wow , that's great .\n",
      "\n",
      "\n",
      "\n",
      "why karolina did not come ?\n",
      "karolina's son has an exam tomorrow .\n",
      "....  ['Rule3 ', (karolina's son has an exam tomorrow ., 'has exam'), [(karolina, son)]]\n",
      "\n",
      "\n",
      "\n",
      "We have to go to John's house for consolation .\n",
      "....  ['Rule6 ', (we have to go to john's house for consolation ., 'go to'), [we]]\n",
      "consolation ?\n",
      "yes , dani marked the anniversary of his mother's death today .\n",
      "....  ['Rule2 ', (yes , dani marked the anniversary of his mother's death today ., 'marked anniversary'), [dani]]\n",
      "\n",
      "\n",
      "\n",
      "should probably let you all know that nathan and i are now engaged .\n",
      "....  ['Rule1 ', (should probably let you all know that nathan and i are now engaged ., 'engaged'), [nathan, i]]\n",
      "\n",
      "\n",
      "\n",
      "How many months are you pregnant ?\n",
      "ad  pregnant\n",
      "....  ['Rule4 ', (how many months are you pregnant ?, 'are pregnant'), [you]]\n",
      "tow month .\n",
      "\n",
      "\n",
      "\n",
      "what was your wedding ceremony like , Abigail ?\n",
      "....  ['Rule5 ', (what was your wedding ceremony like , abigail ?, 'your wedding'), [your]]\n",
      "my husband and I got married in a registry office with just two friends there as witnesses .\n",
      "....  ['Rule1 ', (my husband and i got married in a registry office with just two friends there as witnesses ., 'married'), [(my, husband), i]]\n",
      "But then we had three parties to celebrate .\n",
      "\n",
      "\n",
      "\n",
      "We're having a party at our house today, and you're invited .\n",
      "for what ?\n",
      "we celebrating my brother's graduation from university .\n",
      "....  ['Rule2 ', (we celebrating my brother's graduation from university ., 'celebrating graduation'), [we]]\n",
      "surely , it is my pleasure .\n",
      "\n",
      "\n",
      "\n",
      "Where are you going ?\n",
      "Back to live with my parents .\n",
      "That's something else I used to do before we were married .\n",
      "....  ['Rule1 ', (that's something else i used to do before we were married ., 'married'), [i]]\n",
      "\n",
      "\n",
      "\n",
      "yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side .\n",
      "....  ['Rule5 ', (yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side ., 'our wedding'), [our]]\n",
      "\n",
      "\n",
      "\n",
      "what do you think about Abigail and george ?\n",
      "Do you think they two will get married ?\n",
      "....  ['Rule1 ', (do you think they two will get married ?, 'married'), [they]]\n",
      "Yeah , you can count on it .\n",
      "\n",
      "\n",
      "\n",
      "What about Pamela ?\n",
      "I heard she has passed the bar exam .\n",
      "....  ['Rule2 ', (i heard she has passed the bar exam ., 'passed exam'), [she]]\n",
      "....  ['Rule3 ', (i heard she has passed the bar exam ., 'passed exam'), [she]]\n",
      "Yes , and she married recently .\n",
      "....  ['Rule1 ', (yes , and she married recently ., 'married'), [she]]\n",
      "\n",
      "\n",
      "\n",
      "Frank's getting married , do you believe this ?\n",
      "....  ['Rule1 ', (frank's getting married , do you believe this ?, 'married'), [frank]]\n",
      "Is he really ?\n",
      "Yes , he is .\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "f = open(\"Dialogs_data.txt\", \"r\")\n",
    "sentence=[]\n",
    "sayers=[]\n",
    "for line in f:\n",
    "\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        sayer=stripped_line[:stripped_line.find(\":\")]\n",
    "        sayers.append(sayer)\n",
    "        \n",
    "        stripped_line=stripped_line[stripped_line.find(\":\")+1:]\n",
    "        sentence.append(stripped_line)\n",
    "\n",
    "f.close()\n",
    "n=0\n",
    "\n",
    "for i in sentence :\n",
    "    print(i)\n",
    "    list_tuple=[]\n",
    "    t=i.lower()\n",
    "    doc=nlp(t)\n",
    "\n",
    "    \n",
    "    p1=Rules(doc, verbs_without_need_objects)\n",
    "    text=p1.retrival_senetce_rule1()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule1 \",text , findSubs_for_verbEvents(nlp(text[0]),verbs_without_need_objects)])\n",
    "\n",
    "        \n",
    "    p2=Rules(doc, verbs_with_objects)\n",
    "    text=p2.retrival_senetce_rule2()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule2 \",text ,findSubs_for_verbEvents(nlp(text[0]),verbs_with_objects)])\n",
    "             \n",
    "                \n",
    "    p3=Rules(doc, nouns)\n",
    "    text=p3.retrival_senetce_rule3()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule3 \",text ,findSubs_for_nounsEvents(nlp(text[0]),nouns)])\n",
    "    \n",
    "    \n",
    "    p4=Rules(doc, adjectives)\n",
    "    text=p4.retrival_senetce_rule4()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule4 \",text ,findSubs_for_adjectiveEvents(nlp(text[0]),adjectives)])\n",
    "    \n",
    "    \n",
    "    p5=Rules(doc, noun_direct_relation)\n",
    "    text=p5.retrival_senetce_rule5()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule5 \",text , findSubs_for_direct_relation(nlp(text[0]),noun_direct_relation)])\n",
    "    \n",
    "    \n",
    "    p6=Rules(doc, verb_with_prepo)\n",
    "    text=p6.retrival_senetce_rule6()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule6 \",text ,findSubs_for_actionEvents(nlp(text[0]),verb_with_prepo)])\n",
    "    \n",
    "    \n",
    "    for i in list_tuple:\n",
    "        print(\".... \",i)\n",
    "        \n",
    "    if i==\"\":\n",
    "        n+=1\n",
    "        print(\"\\n\")\n",
    "print(n)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering all cases for rules conflict and aggregiation all possible (events and subjects) for the sentence in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_subjects(subject1,subject2):\n",
    "    p_feminine=[\"she\",\"her\"]\n",
    "    p_masculine=[\"he\",\"his\",\"him\"]\n",
    "    p_self_speaker=[\"i\",\"my\",\"me\"]\n",
    "    p_you=[\"you\",\"your\"]\n",
    "    p_they=[\"they\",\"their\"]\n",
    "    p_we=[\"we\",\"our\",\"us\"]\n",
    "    \n",
    "    if str(subject1)==str(subject2):\n",
    "        return True\n",
    "    elif str(subject1) in p_feminine and str(subject2) in p_feminine:\n",
    "        return True\n",
    "    elif str(subject1) in p_masculine and str(subject2) in p_masculine:\n",
    "        return True\n",
    "    elif str(subject1) in p_self_speaker and str(subject2) in p_self_speaker:\n",
    "        return True\n",
    "    elif str(subject1) in p_you and str(subject2) in p_you:\n",
    "        return True\n",
    "    elif str(subject1) in p_they and str(subject2) in p_they:\n",
    "        return True\n",
    "    elif str(subject1) in p_we and str(subject2) in p_we:\n",
    "        return True\n",
    "    else:\n",
    "        False\n",
    "\n",
    "\n",
    "    \n",
    "def aggregation_for_sentence(list_tuple):\n",
    "    ag=[]\n",
    "    events=[]\n",
    "    subjects=[]\n",
    "    sub=[]\n",
    "    s=[]\n",
    "    if len(list_tuple)>=1: \n",
    "        for i in range(len(list_tuple)):\n",
    "            #event\n",
    "            lis1=list_tuple[i][1]\n",
    "            #subject\n",
    "            lis2=list_tuple[i][2]\n",
    "            if lis2 !=None :\n",
    "                if lis1 !=None:\n",
    "                    ag.append([lis1[1],lis2])\n",
    "            \n",
    "            \n",
    "        for i in range(len(ag)):\n",
    "            events.append(ag[i][0])\n",
    "            \n",
    "            #has subject\n",
    "            if len(ag[i])==2:\n",
    "                sub_=ag[i][1]\n",
    "                for index in range(len(sub_)):\n",
    "                    if isinstance(sub_[index], tuple)==False:\n",
    "                        if sub_[index] not in s :\n",
    "                            s.append(sub_[index])\n",
    "                            if s!=None:\n",
    "                                if len(s)!=0:\n",
    "                                    for j in range(len(s)):\n",
    "   \n",
    "                                        subjects.append(s[j])\n",
    "                                  \n",
    "                                        \n",
    "                    else:\n",
    "                        subjects.append(sub_[index])\n",
    "                        \n",
    "        subjects = list(pd.Series(subjects).drop_duplicates())      \n",
    "        return events,subjects\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def filtering(list_tuple=[]):\n",
    "    \n",
    "    if len(list_tuple)>=6: \n",
    "        _6_rules(list_tuple)\n",
    "        if len(list_tuple)>=5:\n",
    "            _5_rules(list_tuple)\n",
    "            if len(list_tuple)>=4:\n",
    "                _4_rules(list_tuple)\n",
    "                if len(list_tuple)>=3:\n",
    "                    _3_rules(list_tuple)\n",
    "                    if len(list_tuple)>=2:\n",
    "                        _2_rules(list_tuple)\n",
    "    \n",
    "    \n",
    "    if len(list_tuple)>=5: \n",
    "        _5_rules(list_tuple)\n",
    "        if len(list_tuple)>=4:\n",
    "            _4_rules(list_tuple)\n",
    "            if len(list_tuple)>=3:\n",
    "                _3_rules(list_tuple)\n",
    "                if len(list_tuple)>=2:\n",
    "                    _2_rules(list_tuple)\n",
    "        \n",
    "    if len(list_tuple)>=4:\n",
    "        _4_rules(list_tuple)\n",
    "        if len(list_tuple)>=3:\n",
    "            _3_rules(list_tuple)\n",
    "            if len(list_tuple)>=2:\n",
    "                    _2_rules(list_tuple)\n",
    "        \n",
    "    if len(list_tuple)>=3:\n",
    "        _3_rules(list_tuple)\n",
    "        if len(list_tuple)>=2:\n",
    "            _2_rules(list_tuple)\n",
    "            \n",
    "    if len(list_tuple)>=2:\n",
    "        _2_rules(list_tuple)\n",
    "       \n",
    "                 \n",
    "def _6_rules(list_tuple=[]):   \n",
    "    \n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \" and list_tuple[3][0]==\"Rule4 \"  and list_tuple[4][0]==\"Rule5 \"  and list_tuple[5][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[5][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            list_tuple.remove(list_tuple[5])\n",
    "        \n",
    "def _5_rules(list_tuple=[]):\n",
    "    \n",
    "\n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \" and list_tuple[3][0]==\"Rule4 \"  and list_tuple[4][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \" and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule4 \"  and list_tuple[4][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \" and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule4 \"  and list_tuple[4][0]==\"Rule6 \"):\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \" and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule5 \"  and list_tuple[4][0]==\"Rule6 \"):\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \"  and list_tuple[4][0]==\"Rule6 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \"  and list_tuple[4][0]==\"Rule6 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "                            \n",
    "def _4_rules(list_tuple=[]):    \n",
    "                                \n",
    "\n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \" and list_tuple[2][0]==\"Rule3 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule4 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule5 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule5 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "                            \n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule4 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule4 \" and list_tuple[2][0]==\"Rule5 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[0])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "def _3_rules(list_tuple=[]): \n",
    "       \n",
    "\n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule4 \"  and list_tuple[2][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule4 \"  and list_tuple[2][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule4 \"  and list_tuple[2][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[0])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule5 \"  and list_tuple[2][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[0])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "                            \n",
    "def _2_rules(list_tuple=[]): \n",
    "    \n",
    "\n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \") :\n",
    "                        lis=str(list_tuple[1][1][1]).split()\n",
    "                        if str(list_tuple[0][1][1]) == str(lis[0]):\n",
    "                            list_tuple.remove(list_tuple[0])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[1][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[1][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[1][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[1][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pronoun resolution section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[\"he\",\"she\",\"her\",\"his\",\"him\",\"they\",\"their\",\"them\",\"us\",\"we\",\"our\",\"me\",\"my\",\"you\",\"your\",\"i\"]  \n",
    "num_of_sentence_for_pronoun_resolution=2\n",
    "\n",
    "# getting corference clusters \n",
    "def pronoun_resolution_allen(sent,predictor=predictor):\n",
    "    \n",
    "    pred = predictor.predict(\n",
    "        document= sent\n",
    "    )\n",
    "\n",
    "    clusters = pred['clusters']\n",
    "    document = pred['document']\n",
    "    n = 0\n",
    "    doc = {}\n",
    "    for obj in document:\n",
    "        doc.update({n :  obj}) #what I'm doing here is creating a dictionary of each word with its respective index, making it easier later.\n",
    "        n = n+1\n",
    "\n",
    "    clus_all = []\n",
    "    cluster = []\n",
    "    clus_one = {}\n",
    "    s=\"\"\n",
    "    for i in range(0, len(clusters)):\n",
    "        one_cl = clusters[i]\n",
    "        for count in range(0, len(one_cl)):\n",
    "            obj = one_cl[count]            \n",
    "            for num in range((obj[0]), (obj[1]+1)):                \n",
    "                for n in doc:\n",
    "                    if num == n:\n",
    "                        if obj[1]-obj[0]>0:\n",
    "                            s+=\" \"+str(doc[n])\n",
    "                            if len(s.split())>=2 :\n",
    "                                cluster.append(s)\n",
    "                        elif obj[1]-obj[0]==0: \n",
    "                            cluster.append(doc[n])\n",
    "        clus_all.append(cluster)\n",
    "        cluster = [] \n",
    "    return clus_all\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#for checking if the results of pronoun resolution need more resolution like\n",
    "#my sister yesterday comes , she won awards (the results of subject is she .... after pronoun resolution we get my sister ... but until now we dont know \n",
    "#which the real subject so we use this function to again make pronoun resolution using my rule )\n",
    "def check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog):\n",
    "    count = element.count(' and ')                 \n",
    "    while (count >=0 ):                     \n",
    "        if \" my \" in element :\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" my \",\" \"+str(subb)+\" \")\n",
    "\n",
    "        elif element.endswith(' my')==True:\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" my\",\" \"+str(subb))\n",
    "\n",
    "        elif \" i \" in element :\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" i \",\" \"+str(subb)+\" \")\n",
    "            print(\"element .... \",element)\n",
    "\n",
    "        elif  element.endswith(' i')==True:\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" i\",\" \"+str(subb))\n",
    "            print(\"element .... \",element)\n",
    "\n",
    "        elif \" me \" in element :\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" me \",\" \"+str(subb)+\" \")\n",
    "            \n",
    "        elif element.endswith(' me')==True:\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" me\",\" \"+str(subb))\n",
    "\n",
    "\n",
    "        elif \" you \" in element :\n",
    "            subb=my_pronoun_resolution_for_you(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" you \",\" \"+str(subb)+\" \")\n",
    "            \n",
    "        elif element.endswith(' you')==True:\n",
    "            subb=my_pronoun_resolution_for_you(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" you\",\" \"+str(subb))\n",
    "\n",
    "\n",
    "        elif \" your \" in element :\n",
    "            subb=my_pronoun_resolution_for_you(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" your \",\" \"+str(subb)+\" \")\n",
    "            \n",
    "        elif element.endswith(' your')==True:\n",
    "            subb=my_pronoun_resolution_for_you(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" your\",\" \"+str(subb))\n",
    "\n",
    "        elif (\" my \" not in element )and (\" i \" not in element) and (\" me \" not in element) and (\" your \" not in element) and (\" you \" not in element) and(element.endswith(' you')==False) and(element.endswith(' your')==False) and(element.endswith(' i')==False) and(element.endswith(' my')==False) and(element.endswith(' me')==False):\n",
    "            element=\" \"\n",
    "            \n",
    "        count-=1\n",
    "        \n",
    "    return element            \n",
    "\n",
    "\n",
    "\n",
    "# my rules for pronoun resolution for i\n",
    "def my_pronoun_resolution_for_i(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog):\n",
    "    subjects=\"\"\n",
    "    if str(sub2)==\" \":\n",
    "        subjects=sayers_of_dialog[dialog_number][index_of_sentence]\n",
    "    elif str(sub2) !=\" \":\n",
    "        c=sayers_of_dialog[dialog_number][index_of_sentence]+\" \"+str(sub2)\n",
    "        subjects=c\n",
    "    return subjects\n",
    "        \n",
    "    \n",
    "    \n",
    "# my rules for pronoun resolution for you\n",
    "def my_pronoun_resolution_for_you(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog,len_of_dialog):\n",
    "    subjects=\"\"\n",
    "    if len_of_dialog[dialog_number]-1==index_of_sentence:\n",
    "        if sayers_of_dialog[dialog_number][index_of_sentence-1]!=sayers_of_dialog[dialog_number][index_of_sentence]:\n",
    "            if str(sub2)==\" \": \n",
    "                subjects=sayers_of_dialog[dialog_number][index_of_sentence-1]\n",
    "            elif str(sub2)!=\" \":\n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence-1]+\" \"+str(sub2)\n",
    "                subjects=c\n",
    "    if len_of_dialog[dialog_number]-1>index_of_sentence:\n",
    "        if sayers_of_dialog[dialog_number][index_of_sentence+1]!=sayers_of_dialog[dialog_number][index_of_sentence]:\n",
    "            if str(sub2)==\" \": \n",
    "                subjects=sayers_of_dialog[dialog_number][index_of_sentence+1]\n",
    "            elif str(sub2)!=\" \": \n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence+1]+\" \"+str(sub2)\n",
    "                subjects=c\n",
    "    return subjects\n",
    "            \n",
    "            \n",
    "def my_pronoun_resolution_rules_for_we(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog,len_of_dialog):\n",
    "    subjects=\"\"\n",
    "    if len_of_dialog[dialog_number]-1==index_of_sentence:\n",
    "        if sayers_of_dialog[dialog_number][index_of_sentence-1]!=sayers_of_dialog[dialog_number][index_of_sentence]:\n",
    "            if str(sub2)==\" \": \n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence-1]+\" and \"+sayers_of_dialog[dialog_number][index_of_sentence]\n",
    "                subjects=c\n",
    "            elif str(sub2)!=\" \":\n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence-1]+\" and \"+sayers_of_dialog[dialog_number][index_of_sentence]+\" \"+str(sub2)\n",
    "                subjects=c\n",
    "\n",
    "    if len_of_dialog[dialog_number]-1>index_of_sentence:\n",
    "        if sayers_of_dialog[dialog_number][index_of_sentence+1]!=sayers_of_dialog[dialog_number][index_of_sentence]:\n",
    "            if str(sub2)==\" \": \n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence+1]+\" and \"+sayers_of_dialog[dialog_number][index_of_sentence]\n",
    "                subjects=c\n",
    "            elif str(sub2)!=\" \": \n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence+1]+\" and \"+sayers_of_dialog[dialog_number][index_of_sentence]+\" \"+str(sub2)\n",
    "                subjects=c\n",
    "    return subjects\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def check_type_of_subject(sub):\n",
    "    \n",
    "    \n",
    "    if isinstance(sub, tuple)==False:\n",
    "        return sub,\" \"\n",
    "    if isinstance(sub, tuple)==True:\n",
    "        return sub[0],sub[1]\n",
    "    \n",
    "\n",
    "\n",
    "def getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number):\n",
    "    sentence=[]\n",
    "    text=\"\"\n",
    "    index_of_sentence1=index_of_sentence\n",
    "    while(index_of_sentence1>=0 and num_of_sentence_for_pronoun_resolution>=0):\n",
    "        sentence.append(sentences_of_dialogs[dialog_number][index_of_sentence1])\n",
    "        index_of_sentence1-=1\n",
    "        num_of_sentence_for_pronoun_resolution-=1\n",
    "    for i in range(len(sentence)):\n",
    "        text+=\" \"+str(sentence[len(sentence)-i-1])\n",
    "    return text    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "def pronoun_resolution_for_all_cases_of_pronouns(aggrigation_list=[],dialog_number=0,index_of_sentence=0,sentences_of_dialogs=[],num_of_sentence_for_pronoun_resolution=3,sayers_of_dialog=[],len_of_dialog=[]):\n",
    "    b=aggrigation_list\n",
    "    \n",
    "    if len(b[0]) !=0 and  len(b[1]) !=0:\n",
    "        subjects=[]\n",
    "        \n",
    "        for i in range(len(b[1])):\n",
    "            \n",
    "            sub,sub2=check_type_of_subject(b[1][i])\n",
    "\n",
    "            if str(sub)==\"i\" or str(sub)==\"my\" or str(sub)==\"me\":\n",
    "                subjects.append(my_pronoun_resolution_for_i(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog))\n",
    "\n",
    "                \n",
    "            elif str(sub)==\"you\" or str(sub)==\"your\" :\n",
    "                subjects.append(my_pronoun_resolution_for_you(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog,len_of_dialog))\n",
    "\n",
    "                \n",
    "            elif str(sub)==\"her\" or str(sub)==\"she\" :\n",
    "                \n",
    "                text=getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number)\n",
    "                \n",
    "                print(\"....... more than one sentnce for making pro resolution ....... (\", (text) ,\")\",\"\\n\" )    \n",
    "                \n",
    "                resol=pronoun_resolution_allen(text)\n",
    "\n",
    "                if len(resol)!=0:\n",
    "                    for i in resol:\n",
    "                        if \"her\" in i or \"she\" in i :\n",
    "                            for element in i :\n",
    "                                if len(element.split())==1:\n",
    "                                    if element not in p:\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(element)\n",
    "                                        elif str(sub2)!=\" \": \n",
    "                                            c=element+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                elif len(element.split())==2:\n",
    "                                    ele=check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "                                    #like my sister after pronoun resolution should make pronoun again using my rules \n",
    "                                    if ele==\" \":\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(element)\n",
    "                                        elif str(sub2)!=\" \":\n",
    "                                            c=element+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                    else:\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(ele)\n",
    "                                        elif str(sub2)!=\" \":\n",
    "                                            c=ele+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                        \n",
    "                                    \n",
    "                                    \n",
    "\n",
    "\n",
    "\n",
    "            elif str(sub)==\"he\" or str(sub)==\"his\" or str(sub)==\"him\" :\n",
    "                \n",
    "                text=getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number)\n",
    "                \n",
    "                print(\"............... more than one sentnce for making pro resolution ...............(\", (text) ,\")\" ,\"\\n\" )    \n",
    "                \n",
    "                resol=pronoun_resolution_allen(text)\n",
    "\n",
    "                if len(resol)!=0:\n",
    "                    for i in resol:\n",
    "                        if \"his\" in i or \"he\" in i  or \"him\" in i :\n",
    "                            for element in i :\n",
    "                                if len(element.split())==1:\n",
    "                                    if element not in p:\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(element)\n",
    "                                        elif str(sub2)!=\" \": \n",
    "                                            c=element+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                elif len(element.split())==2:\n",
    "                                    #like my sister after pronoun resolution should make pronoun again using my rules \n",
    "                                    \n",
    "                                    ele=check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "                                    \n",
    "                                    if ele==\" \":   \n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(element)\n",
    "                                        elif str(sub2)!=\" \":\n",
    "                                            c=element+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                    else:\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(ele)\n",
    "                                        elif str(sub2)!=\" \":\n",
    "                                            c=ele+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                        \n",
    "                                    \n",
    "\n",
    "\n",
    "            \n",
    "            elif str(sub)==\"they\" or str(sub)==\"their\" :\n",
    "                \n",
    "                text=getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number)\n",
    "                \n",
    "                print(\"............... more than one sentnce for making pro resolution ............... (\", (text) ,\")\",\"\\n\" )    \n",
    "                \n",
    "                resol=pronoun_resolution_allen(text)\n",
    "                \n",
    "                if len(resol)!=0:\n",
    "                    for i in resol:\n",
    "                        for element in i :\n",
    "                            if (\" and \" in element) and (\" they \" not in element) and (\" their \" not in element) and (element.endswith('and')==False ):\n",
    "                                if element.endswith('they')==False and element.endswith('their')==False:\n",
    "                                    if len(element.split())>=3:\n",
    "                                        ele=check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "                                            \n",
    "                                        if ele ==\" \":\n",
    "                                            if str(sub2)==\" \": \n",
    "                                                subjects.append(element)\n",
    "                                            elif str(sub2)!=\" \": \n",
    "                                                c=element+\" \"+str(sub2)\n",
    "                                                subjects.append(c)\n",
    "                                        else:\n",
    "                                            if str(sub2)==\" \": \n",
    "                                                subjects.append(ele)\n",
    "                                            elif str(sub2)!=\" \": \n",
    "                                                c=ele+\" \"+str(sub2)\n",
    "                                                subjects.append(c)\n",
    "\n",
    "\n",
    "            elif str(sub)==\"we\" or str(sub)==\"our\" or str(sub)==\"us\":\n",
    "                \n",
    "                text=getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number)\n",
    "                \n",
    "                    \n",
    "                print(\"............... more than one sentnce for making pro resolution ............... (\", (text) ,\")\" ,\"\\n\" )    \n",
    "                \n",
    "                resol=pronoun_resolution_allen(text)\n",
    "\n",
    "                if len(resol)!=0 :\n",
    "                    for i in resol:\n",
    "                        for element in i :\n",
    "                            if (\" and \" in element) and (\" we \" not in element) and (\" our \" not in element ) and (\" us \" not in element ) and element.endswith('and')==False :\n",
    "                                if element.endswith('we')==False and element.endswith('our')==False and element.endswith('us')==False:\n",
    "                                    if len(element.split())>=3:\n",
    "                                        ele=check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "\n",
    "                                        if ele==\" \":\n",
    "                                            if str(sub2)==\" \": \n",
    "                                                subjects.append(element)\n",
    "                                            elif str(sub2)!=\" \": \n",
    "                                                c=element+\" \"+str(sub2)\n",
    "                                                subjects.append(c)\n",
    "                                        else:\n",
    "                                            if str(sub2)==\" \": \n",
    "                                                subjects.append(ele)\n",
    "                                            elif str(sub2)!=\" \": \n",
    "                                                c=ele+\" \"+str(sub2)\n",
    "                                                subjects.append(c)\n",
    "                                            \n",
    "                if len(subjects)==0:\n",
    "                    subjects.append(my_pronoun_resolution_rules_for_we(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog,len_of_dialog))\n",
    "                    \n",
    "\n",
    "        subjects = list(pd.Series(subjects).drop_duplicates())  \n",
    "        return subjects\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregation the results of pronoun resolution and the results of all previous steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_between_filtering_and_resolution(aggregation_for_sentence,resolution):\n",
    "            agg=[]\n",
    "            b=aggregation_for_sentence\n",
    "            if resolution !=None and len(resolution)==0 and (len(b)==2):\n",
    "                agg.append(b)\n",
    "            if resolution !=None and len(resolution)!=0 and (len(b)==2):\n",
    "                if b[0]!=0 and b[1]!=0:\n",
    "                    if len(b[1])==1:\n",
    "                        if isinstance(b[1][0], tuple)==True:\n",
    "                            agg.append((b[0] ,resolution))\n",
    "                        elif isinstance(b[1][0], tuple)==False:\n",
    "                            if str(b[1][0]) in p :\n",
    "                                agg.append((b[0] ,resolution))\n",
    "                            elif str(b[1][0]) not in p :\n",
    "                                agg.append((b[0] ,b[1]))\n",
    "                    elif len(b[1])>1:\n",
    "                        if len(b[1])==len(resolution):\n",
    "                            agg.append((b[0] ,resolution))\n",
    "                            \n",
    "                        elif len(b[1])!=len(resolution):\n",
    "                            for i in range(len(b[1])):\n",
    "                                if i+1 < len(b[1]): \n",
    "                                    if equal_subjects(b[1][i],b[1][i+1]):\n",
    "                                        b[1].remove(b[1][i])\n",
    "                            for i in range(len(resolution)):\n",
    "                                for j in range(len(b[1])):\n",
    "                                    if isinstance(b[1][j] , tuple)==False:\n",
    "                                        if str(b[1][j]) in p :\n",
    "                                            b[1][j]=resolution[i]\n",
    "                                            continue\n",
    "                                    elif isinstance(b[1][j] , tuple)==True:\n",
    "                                        c=b[1][j]\n",
    "                                        if str(c[0]) in p :\n",
    "                                            \n",
    "                                            b[1][j]=resolution[i]\n",
    "                                            \n",
    "                                            continue\n",
    "                            agg.append((b[0] ,b[1]))           \n",
    "            return agg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "output=pandas.read_csv('labels.csv') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking all previous steps (extract events , and subjects with pronoun resolution) , applaying filtering and aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 23:53:36.985095 28844 warnings.py:110] C:\\Users\\Windows dunya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:164: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark   Susan is going to get a divorce .\n",
      "...............subject result befor resolution.............. [susan] \n",
      "\n",
      " ..................the final results....................  [(['get divorce'], [susan])] \n",
      "\n",
      "Jhon   How do you know that ?\n",
      "Mark   She told me that Peter and she has a quarrel last nigh , and she left this morning , bag and baggage .\n",
      "Jhon   I see . But I think you are making a fuss .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   these days too many people are getting divorced .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 23:53:43.505668 28844 warnings.py:110] C:\\Users\\Windows dunya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jhon   If they live together , then at least they're finding out if they're really compatible or not .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   what is happend with Mari and george ?\n",
      "Mark   They got a divorce at last .\n",
      "...............subject result befor resolution.............. [they] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  what is happend with Mari and george ? They got a divorce at last . ) \n",
      "\n",
      " ..................the final results....................  [(['got divorce'], [' Mari and george'])] \n",
      "\n",
      "Jhon   It's inevitable .\n",
      "Mark   Their love was built on the sand , and this is why their marriage has landed on the rocks.\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   what is happend with Mari?\n",
      "Mark   I heard she has passed the bar exam and married recently .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  what is happend with Mari? I heard she has passed the bar exam and married recently . ) \n",
      "\n",
      " ..................the final results....................  [(['married', 'passed exam'], ['Mari'])] \n",
      "\n",
      "Jhon   Oh yes .\n",
      "Mark   She had a beautiful wedding in Cozumel Mexico and we all attended .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  I heard she has passed the bar exam and married recently . Oh yes . She had a beautiful wedding in Cozumel Mexico and we all attended . ) \n",
      "\n",
      " ..................the final results....................  [(['had wedding'], ['She'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   That's why we want to have a look at other hotels .\n",
      "Abigail   We dont want to spend too much on an extravagant wedding reception .\n",
      "Jhon   Iguess you're right .\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   my husband's parents flew out to see my family when we got married in my hometown , so that was great .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  my husband's parents flew out to see my family when we got married in my hometown , so that was great . ) \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Jhon and Abigail'])] \n",
      "\n",
      "Jhon   Some people spend ridiculous amounts of money on extravagant wedding receptions , but we agreed that makes sense .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   what was your wedding ceremony like , Abigail ?\n",
      "...............subject result befor resolution.............. [your] \n",
      "\n",
      " ..................the final results....................  [(['your wedding'], ['Abigail'])] \n",
      "\n",
      "Abigail   my husband and I got married in a registry office with just two friends there as witnesses .\n",
      "...............subject result befor resolution.............. [(my, husband), i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Abigail husband', 'Abigail'])] \n",
      "\n",
      "Abigail   But then we had three parties to celebrate .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Is that your wedding ring Abigail ?\n",
      "...............subject result befor resolution.............. [your] \n",
      "\n",
      " ..................the final results....................  [(['your wedding'], ['Abigail'])] \n",
      "\n",
      "Abigail   I'm not married yet .\n",
      "Abigail   It's my engagement ring .\n",
      "Jhon   Well , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   You remember ? The island , the sound of the waves , the salty sea air and the sunshine.\n",
      "Mark   yes , it was wonderful but it's already been a year .\n",
      "Jhon   why not go again to celebrate out one-year anniversary ?\n",
      "Mark   We can go to the same beach , stay in the same hotel and enjoy a dinner in the same restaurant\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  yes , it was wonderful but it's already been a year . why not go again to celebrate out one-year anniversary ? We can go to the same beach , stay in the same hotel and enjoy a dinner in the same restaurant ) \n",
      "\n",
      " ..................the final results....................  [(['go to'], ['Jhon and Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   For the 100th anniversary of the opening of the library we are going to have a party .\n",
      "Jhon   That's a wonderful way to celebrate this grand old library.\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Actually , even though we have parades , most people just use the national day holiday to visit family or go shopping .\n",
      "Jhon   In the evening , many people watch special TV shows which celebrate national day .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   what did you say when he asked you how long it took you to learn English ?\n",
      "Jhon   I told him 28 years . And , he knows I'm 28 years old since I just celebrated my birthday last week .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['celebrated birthday'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Do you want to go out to celebrate my good news ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['celebrate news'], ['Jhon'])] \n",
      "\n",
      "Jhon   sure , where would you like to go ?\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Which university did you graduate from ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['graduate'], ['Jhon'])] \n",
      "\n",
      "Jhon   I graduated from Peking University .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['graduated university'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   How you celebrate your Valentines Day with your wife ?\n",
      "Jhon   I will take a rain check .\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   Margaret , do you think I should enroll in the science course ?\n",
      "Margaret   I think so , If you want to graduated this year , you've got to take a science course .\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['graduated'], ['Abigail'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   When did you get started ?\n",
      "Abigail   I began blogging when I first went to the US for my strides .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['went to'], ['Abigail'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   I heard your son recently graduated.\n",
      "...............subject result befor resolution.............. [(your, son)] \n",
      "\n",
      " ..................the final results....................  [(['graduated'], ['Margaret son'])] \n",
      "\n",
      "Margaret   Yes , my little Paul is finally a doctor .\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   What about Pamela ?\n",
      "Margaret   I heard she has passed the bar exam .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  What about Pamela ? I heard she has passed the bar exam . ) \n",
      "\n",
      " ..................the final results....................  [(['passed exam'], ['Pamela'])] \n",
      "\n",
      "Abigail   Yes , and she married recently .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  What about Pamela ? I heard she has passed the bar exam . Yes , and she married recently . ) \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Pamela'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   What do you mean by \" us \" ?\n",
      "Abigail   Well , we used to talk to each other .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Where are you going ?\n",
      "Abigail   Back to live with my parents .\n",
      "Mark   That's something else I used to do before we were married .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I'm going to get married next month .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Mark'])] \n",
      "\n",
      "Jhon   Good news ! And congratulations !\n",
      "Mark   You are invited to my wedding .\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['invited wedding'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   when are you getting married ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Margaret'])] \n",
      "\n",
      "Margaret   some time next year .\n",
      "Margaret   We haven't set the date yet .\n",
      "Abigail   congratulations\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   How are you Abigail?\n",
      "Abigail   I'm doing really well .\n",
      "Abigail   I got married about three years ago .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Abigail'])] \n",
      "\n",
      "Abigail   I have two kids now .\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   I've fallen in love with george .\n",
      "Margaret   really ? Is he married ?\n",
      "...............subject result befor resolution.............. [he] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ...............(  I've fallen in love with george . really ? Is he married ? ) \n",
      "\n",
      " ..................the final results....................  [(['married'], ['george'])] \n",
      "\n",
      "Abigail   no , of course not . He is still single .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   what do you think about Abigail and george ?\n",
      "Jhon   Do you think they two will get married ?\n",
      "...............subject result befor resolution.............. [they] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  what do you think about Abigail and george ? Do you think they two will get married ? ) \n",
      "\n",
      " ..................the final results....................  [(['married'], [' Abigail and george'])] \n",
      "\n",
      "Mark   Yeah , you can count on it .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Frank's getting married , do you believe this ?\n",
      "...............subject result befor resolution.............. [frank] \n",
      "\n",
      " ..................the final results....................  [(['married'], [frank])] \n",
      "\n",
      "Mark   Is he really ?\n",
      "Jhon   Yes , he is .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Any good news ?\n",
      "Mark   Yes . I'Ve won the first prize in the math contest .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Mark'])] \n",
      "\n",
      "Jhon   Really ? Congratulations !\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I won first prize in the poetry contest .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Mark'])] \n",
      "\n",
      "Jhon   Come on ! You're pulling my leg .\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   I won a prize last week but it was a prize for beginners .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Abigail'])] \n",
      "\n",
      "Abigail   My prize was for the best player in the country .\n",
      "Mark   Now let's start playing chess seriously .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   george has loved writing since he was a very little boy .\n",
      "Jhon   yes , he won the first prize in a national composition contest when I was in middle school .\n",
      "...............subject result befor resolution.............. [he] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ...............(  george has loved writing since he was a very little boy . yes , he won the first prize in a national composition contest when I was in middle school . ) \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I buy a lottery ticket every week and I'm amazed that I haven won a small prize yet .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   mari won the decathlon in the 1976 Olympics , right ?\n",
      "Jhon   I read that she trained so much that he used to dream about jumping hurdles .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   You're a very good player\n",
      "Jhon   Not really , but once I won a prize .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   Any good news ?\n",
      "Margaret   I'Ve won the first prize in the math contest .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Margaret'])] \n",
      "\n",
      "Abigail   Really ? Congratulations !\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   What time shall we meet at the bus stop ?\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  What time shall we meet at the bus stop ? ) \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Margaret and Abigail'])] \n",
      "\n",
      "Margaret   Let's meet at 12 thirty .\n",
      "Abigail   it will probably take us three or four hours to see all of the exhibits .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   How was your meeting with Abigail?\n",
      "...............subject result befor resolution.............. [your] \n",
      "\n",
      " ..................the final results....................  [(['your meeting'], ['Mark'])] \n",
      "\n",
      "Mark   we would be in a meeting and she would say , yesterday I was chatting with Tom .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  How was your meeting with Abigail? we would be in a meeting and she would say , yesterday I was chatting with Tom . ) \n",
      "\n",
      " ..................the final results....................  [(['be meeting'], [''])] \n",
      "\n",
      "Mark   She meant Tom Solomon\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I just want to get to my meeting !\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['get meeting'], ['Mark'])] \n",
      "\n",
      "Mark   I can't go to your party .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['go to'], ['Mark'])] \n",
      "\n",
      "Jhon   That's too bad .\n",
      "   \n",
      "\n",
      "\n",
      "george   I'll stay home this morning and rest , but if I feel better in the afternoon , I'm going to the meeting .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['going meeting'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Let's go today to the pool .\n",
      "Jhon   I couldn't do that .\n",
      "Jhon   I have an important meeting to go to today .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have meeting'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   I'll have to meet my girlfriend at the airport then .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Jhon'])] \n",
      "\n",
      "Jhon   We'd like to invite you for our dress party tomorrow evening .\n",
      "Mark   I will definitely come .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   I wanna finish my drink first .\n",
      "Mark   I'll meet you at Sammy's .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Mark'])] \n",
      "\n",
      "Jhon   I wait you .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Don't you know our movie starts at seven ?\n",
      "Jhon   And we were going to meet at the theater at five to seven .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  Don't you know our movie starts at seven ? And we were going to meet at the theater at five to seven . ) \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Mark and Jhon'])] \n",
      "\n",
      "Mark   oh , I forget that .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I was thinking that I'd like to invite you to watch a movie .\n",
      "george   I can meet you at the cinema gate .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mari   I'm sure you can do it very well .\n",
      "Mari   Then I'll meet you at six .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Mari'])] \n",
      "\n",
      "Mark   Is that at all right ?\n",
      "Mari   Yes .\n",
      "Mark   OK .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   where and when should I meet you ?\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Jhon'])] \n",
      "\n",
      "Mark   we'll pick you up at your place at noon . Be there or be square !\n",
      "   \n",
      "\n",
      "\n",
      "george   what about Mark ?\n",
      "george   did you tell him?\n",
      "Mari   he knows I'm coming . Our meeting is set for 2 o'clock .\n",
      "...............subject result befor resolution.............. [our] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  what about Mark ? did you tell him? he knows I'm coming . Our meeting is set for 2 o'clock . ) \n",
      "\n",
      " ..................the final results....................  [(['our meeting'], ['george and Mari'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   We're only switching two days .\n",
      "Kriss   You can do legs on Friday .\n",
      "george   Aright . I'll meet you at the gym at 3:30 then .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   When can we start working on this , george?\n",
      "george   Well , we could probably get started with a preparatory meeting this afternoon at 2:00 .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  When can we start working on this , george? Well , we could probably get started with a preparatory meeting this afternoon at 2:00 . ) \n",
      "\n",
      " ..................the final results....................  [(['started meeting'], ['Kriss and george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   I got hurt when fixing the light , even during office hours , I wouldn't get compensation from our company since repairing is not my responsibility .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['hurt'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   There are so many nasties on the internet and so many people who are trying to use the internet to hurt other users .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   What's the matter ?\n",
      "george   I've got a really bad headache and my stomach hurts .\n",
      "...............subject result befor resolution.............. [(my, stomach), i] \n",
      "\n",
      " ..................the final results....................  [(['hurts', 'got headache'], ['george stomach', 'george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   what's happend with Jhon .\n",
      "george   I think he's hurt his back .\n",
      "...............subject result befor resolution.............. [he] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ...............(  what's happend with Jhon . I think he's hurt his back . ) \n",
      "\n",
      " ..................the final results....................  [(['hurt'], ['Jhon'])] \n",
      "\n",
      "Kriss   What shall we do ?\n",
      "george   We'd better not move him .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I was really very , very nervous just before I had the surgery , but the anaesthetist gave me an anaesthetic and the next thing I remember was waking up after the surgery .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['had surgery'], ['Mark'])] \n",
      "\n",
      "Mark   It must have really hurt afterwards .\n",
      "   \n",
      "\n",
      "\n",
      "george   Tell me a little bit about yourself , please .\n",
      "Kriss   I was born and raised in Beijing . I attended Peking University and received my bachelor's degree in Economics .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['born'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   Have you ever designed any programs concerning network ?\n",
      "Mark   Yes , I have designed some programs for the network with Visual C + + and I have passed the test for programmers - MUSE .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['passed test'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   What band did you pass in Japanese Language Proficiency Test ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['pass test'], ['george'])] \n",
      "\n",
      "george   I passed the Band two in LPT , but I will try to achieve Band one which is the highest level .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   How are your English and computer skills ?\n",
      "george   I have passed the CET - 4 and 6.As far as computer is concerned I can use the computer for word processing .\n",
      "   \n",
      "\n",
      "\n",
      "george   I'Ve got good news .\n",
      "Mark   what ?\n",
      "george   I have successfully passed the first two rounds of interview with ABC Company .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['passed interview'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mari   How many months are you pregnant ?\n",
      "ad  pregnant\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['are pregnant'], ['Abigail'])] \n",
      "\n",
      "Abigail   Three months .\n",
      "   \n",
      "\n",
      "\n",
      "george   It will be fun , you will get to know lots of people .\n",
      "Mark   Sounds great , I'd very much like to accept your invitation , thanks\n",
      "   \n",
      "\n",
      "\n",
      "george   I want to know whether you will come to the interview . So have you accepted offers from other companies ?\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offers', 'come interview'], ['george'])] \n",
      "\n",
      "Mark   No , I haven't got one by now .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   can I help you find something ?\n",
      "Jhon   Yes , actually I'm looking to buy a camera .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Hello . This is Mrs . Wilson .\n",
      "Wilson   I'd like to buy a new car .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['buy car'], ['Wilson'])] \n",
      "\n",
      "Wilson   Could you offer me a new type of the car , please ?\n",
      "   \n",
      "\n",
      "\n",
      "george   How bad did I do ?\n",
      "Mark   To be completely honest , you failed your test .\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['failed test'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   I need a new suit .\n",
      "george   why ?\n",
      "Tina   I have an important interview next week , so I really need to look sharp .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have interview'], ['Tina'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   That's why job interviews are important to let people know the real you that they can't see from a piece of paper .\n",
      "   \n",
      "\n",
      "\n",
      "george   Good morning . Thank you for the interview .\n",
      "Kriss   No problem . Now , do you prefer working with others or flying solo ?\n",
      "george   Actually , I enjoy both .\n",
      "   \n",
      "\n",
      "\n",
      "george   Hey , could you help me try and figure out how to get ready for my job interview ?\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['my interview'], ['george'])] \n",
      "\n",
      "Kriss   The most important thing to do is to make sure you know the company and what services or products it provides .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   If you pass the interview , the personnel department will inform you within two weeks .\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['pass interview'], ['george'])] \n",
      "\n",
      "george   But if I don't pass , will you call me ?\n",
      "Kriss   I'm sorry we won't .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Hi , George . I'm going to have a job interview next week .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have interview'], ['Kriss'])] \n",
      "\n",
      "Kriss   Could you give me some advice ?\n",
      "george   Sure . First of all , it's very important for you not to be late .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Any good news ?\n",
      "Mark   today i have been married to the most beautiful women in the world for 15 years .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Any good news ?\n",
      "george   my daughter will be born tonight at midnight .\n",
      "...............subject result befor resolution.............. [(my, daughter)] \n",
      "\n",
      " ..................the final results....................  [(['born'], ['george daughter'])] \n",
      "\n",
      "Kriss   Well , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   the romance studio interviewed me as a featured author .\n",
      "...............subject result befor resolution.............. [me] \n",
      "\n",
      " ..................the final results....................  [(['interviewed'], ['Kriss'])] \n",
      "\n",
      "george   oh , good i am very happy for you .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   I did not tell you ?\n",
      "Mari   about what ?\n",
      "Jhon   i'm interviewed on montana public radio today about my life.\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['interviewed'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   i'm getting married today .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Margaret'])] \n",
      "\n",
      "Mari   Well , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   today me and matt got engaged and here is my ring .\n",
      "...............subject result befor resolution.............. [me, matt] \n",
      "\n",
      " ..................the final results....................  [(['engaged'], ['Margaret', matt])] \n",
      "\n",
      "Mari   Well , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   should probably let you all know that nathan and i are now engaged .\n",
      "...............subject result befor resolution.............. [nathan, i] \n",
      "\n",
      " ..................the final results....................  [(['engaged'], [nathan, 'Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   I did not tell you what happen with tomas?\n",
      "Kriss   no , tell me.\n",
      "george   tomas divorced katty and take her alone .\n",
      "...............subject result befor resolution.............. [tomas] \n",
      "\n",
      " ..................the final results....................  [(['divorced'], [tomas])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Where was the engaged held?\n",
      "Mari   george and i got engaged in roma .\n",
      "...............subject result befor resolution.............. [george, i] \n",
      "\n",
      " ..................the final results....................  [(['engaged'], [george, 'Mari'])] \n",
      "\n",
      "Kriss   great , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   When did you get married?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Jhon'])] \n",
      "\n",
      "Jhon   two days from now .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   i have good news for you .\n",
      "Mark   really , what ?\n",
      "Jhon   i finally got accepted into howard university .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted university'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   i am very happy for mario .\n",
      "Jhon   what happens with him ?\n",
      "george   mario passed the final exam for this year .\n",
      "...............subject result befor resolution.............. [mario] \n",
      "\n",
      " ..................the final results....................  [(['passed exam'], [mario])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Why are you happy ?\n",
      "Mari   i've just signed my deal with authentik artists .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['signed deal'], ['Mari'])] \n",
      "\n",
      "Kriss   very good .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   what is the lastest news jhon ?\n",
      "Jhon   i just accepted the company's ( generous ) offer and will be starting my new job on monday 18 january .\n",
      "...............subject result befor resolution.............. [i, my] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer', 'my job'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Do you know what the news of our friend tom ?\n",
      "Kriss   tom got accepted into grad school and a new job in the past three days .\n",
      "...............subject result befor resolution.............. [tom] \n",
      "\n",
      " ..................the final results....................  [(['accepted school'], [tom])] \n",
      "\n",
      "great    great .\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   i got accepted into syracuse university .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted university'], ['Margaret'])] \n",
      "\n",
      "Mari   I am proud of you .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   i got accepted into the business program at duke univ .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted program'], ['Mark'])] \n",
      "\n",
      "Kriss   very good , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Mari   what is the lastest news of bati?\n",
      "Kati   bati won the 2010 calgary choice awards in arts and culture .\n",
      "...............subject result befor resolution.............. [bati] \n",
      "\n",
      " ..................the final results....................  [(['won awards'], [bati])] \n",
      "\n",
      "Mari   that's great .\n",
      "   \n",
      "\n",
      "\n",
      "george   We're having a party at our house today, and you're invited .\n",
      "Mark   for what ?\n",
      "george   we celebrating my brother's graduation from university .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  We're having a party at our house today, and you're invited . for what ? we celebrating my brother's graduation from university . ) \n",
      "\n",
      " ..................the final results....................  [(['celebrating graduation'], ['Mark and george'])] \n",
      "\n",
      "Mark   surely , it is my pleasure .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   why you so sad ?\n",
      "Mari   i failed that exam .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['failed exam'], ['Mari'])] \n",
      "\n",
      "Bati   oh , i am sorry for you .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   Hi , Tina , I'Ve got good news .\n",
      "Tina   tell me .\n",
      "Bati   i was very honoured and humbled to win this award today .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['win award'], ['Bati'])] \n",
      "\n",
      "Tina   wow , i am very happy for you .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   What is the latest news of your working life ?\n",
      "Tina   i accepted a job offer in columbus and will begin august 2 .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer'], ['Tina'])] \n",
      "\n",
      "Kriss   great .\n",
      "   \n",
      "\n",
      "\n",
      "george   I'm so happy for karla , she finally has good news .\n",
      "Kriss   What is her latest news ?\n",
      "george   finally, karla's son passed the exam .\n",
      "...............subject result befor resolution.............. [(karla, son)] \n",
      "\n",
      " ..................the final results....................  [(['passed exam'], [(karla, son)])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Bati   where mira ? Why didn't she come with you ?\n",
      "Tina   mira celebrating her son's birthday today .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Hi George, how are you, why didn't you come with our friends ? we are waiting for you .\n",
      "George   No , i can't , i celebrating my one year anniversary with my beautiful wife .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['celebrating anniversary'], ['George'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   Mom, did my sister call you, what are the results of her competition?\n",
      "Tina   she won a couple of addy awards for her package design .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  Mom, did my sister call you, what are the results of her competition? she won a couple of addy awards for her package design . ) \n",
      "\n",
      " ..................the final results....................  [(['won awards'], [' Tina sister'])] \n",
      "\n",
      "george   Wow , that's great .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   What is the latest news of your university life?\n",
      "Jhon   i got my first college acceptance letter and i got accepted into penn state university .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted university'], ['Jhon'])] \n",
      "\n",
      "Bati   great , it is a very good university .\n",
      "   \n",
      "\n",
      "\n",
      "Tina   you know what happen with tom ?\n",
      "Kriss   No , tell me .\n",
      "Tina   tom has been accepted to the georgia state university college of law .\n",
      "...............subject result befor resolution.............. [tom] \n",
      "\n",
      " ..................the final results....................  [(['accepted college'], [tom])] \n",
      "\n",
      "Kriss   great .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   i am very happy for our freind tom , finally he take back his right .\n",
      "Jhon   What are you talking about ?\n",
      "Kriss   tom won his lawsuit for stealing his car .\n",
      "...............subject result befor resolution.............. [tom] \n",
      "\n",
      " ..................the final results....................  [(['won lawsuit'], [tom])] \n",
      "\n",
      "Jhon   it is a good news .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Did you find a job recently ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['find job'], ['Mark'])] \n",
      "\n",
      "Mark   yes , i started the new job on july 1 .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['started job'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   What are all these preparations?\n",
      "Jhon   today my wife and i celebrate our 1st wedding anniversary .\n",
      "...............subject result befor resolution.............. [(my, wife), i] \n",
      "\n",
      " ..................the final results....................  [(['celebrate anniversary'], ['Jhon wife', 'Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   John, you change a lot .\n",
      "Jhon   yes , i lost weight a lot after dieting .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['lost weight'], ['Jhon'])] \n",
      "\n",
      "Kriss   This is better for your health .\n",
      "   \n",
      "\n",
      "\n",
      "Tina   i want to celebrate my birthday tomorrow , so you are invite kriss .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['celebrate birthday'], ['Tina'])] \n",
      "\n",
      "Kriss   Certainly , it's my pleasure .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Will you work in your uncle's company ?\n",
      "Mark   No , i've accepted the job at stanford .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted job'], ['Mark'])] \n",
      "\n",
      "Jhon   really ?\n",
      "Mark   Yes , I am very excited .\n",
      "   \n",
      "\n",
      "\n",
      "george   i accepted their offer and am now the newest member of the hays reading branch finance and accountancy team .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tom   Mari left her house, you know ?\n",
      "Kriss   really ? why ?\n",
      "Tom   she bought a new house .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  Mari left her house, you know ? really ? why ? she bought a new house . ) \n",
      "\n",
      " ..................the final results....................  [(['bought house'], ['Mari'])] \n",
      "\n",
      "Kriss   I hope it's better .\n",
      "Tom   Yes , and it's very beautiful .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Did you start a new job?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['start job'], ['george'])] \n",
      "\n",
      "george   Yes , on monday i start my new job as web producer for twin cities public tv's new weekly arts series .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['start job'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   What happened to you ?\n",
      "Mark   i had broken my leg two days ago .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['broken leg'], ['Mark'])] \n",
      "\n",
      "george   Oh , i am so sorry .\n",
      "   \n",
      "\n",
      "\n",
      "Tom   i accepted an offer from google and started working .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer'], ['Tom'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tom   why you are happy ?\n",
      "Kriss   i am happy for mari , she won the national award for fashion .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won award'], ['Kriss'])] \n",
      "\n",
      "Tom   this is good news .\n",
      "   \n",
      "\n",
      "\n",
      "Frank   i accepted their offer and i am now the newest member of the hays reading branch finance and accountancy team .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer'], ['Frank'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   i will start a new job tomorrow after looking for several months .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['start job'], ['Tomas'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   We have to go to John's house for consolation .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  We have to go to John's house for consolation . ) \n",
      "\n",
      " ..................the final results....................  [(['go to'], ['Mark and Jhon'])] \n",
      "\n",
      "Mark   consolation ?\n",
      "Jhon   yes , dani marked the anniversary of his mother's death today .\n",
      "...............subject result befor resolution.............. [dani] \n",
      "\n",
      " ..................the final results....................  [(['marked anniversary'], [dani])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   You won't congratulate me ?\n",
      "Bati   for what ?\n",
      "Kriss   I won a prize for the best student of this year .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Kriss'])] \n",
      "\n",
      "Bati   congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   I heard you graduated this year .\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['graduated'], ['Tina'])] \n",
      "\n",
      "Tina   Yes , defending my thesis went great .\n",
      "Tomas   congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   I like Maria, she is a beautiful girl .\n",
      "Tomas   tony is in relationship with maria .\n",
      "...............subject result befor resolution.............. [tony] \n",
      "\n",
      " ..................the final results....................  [(['is relationship'], [tony])] \n",
      "\n",
      "Jhon   are you really sure?\n",
      "Tomas   Yes .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   why karolina did not come ?\n",
      "Tomas   karolina's son has an exam tomorrow , she help him with his lessons .\n",
      "...............subject result befor resolution.............. [(karolina, son)] \n",
      "\n",
      " ..................the final results....................  [(['has exam'], [(karolina, son)])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   i got a promotion at job .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['got promotion'], ['Tina'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   I will leave this university ?\n",
      "Tina   why ?\n",
      "Tomas   i've got an offer from cambridge .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['got offer'], ['Tomas'])] \n",
      "\n",
      "Tina   that's great .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   you look so tired .\n",
      "Tina   i am very sick .\n",
      "ad  sick\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['am sick'], ['Tina'])] \n",
      "\n",
      "Tina   i have a fever .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have fever'], ['Tina'])] \n",
      "\n",
      "Bati   i will call the doctor .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   i'm excited ! i have a job interview tomorrow .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have interview'], ['Jhon'])] \n",
      "\n",
      "Kriss   i wish you the best .\n",
      "   \n",
      "\n",
      "\n",
      "george   i have a bad news .\n",
      "Tina   What's up ?\n",
      "george   mira will have an surgery next week .\n",
      "...............subject result befor resolution.............. [mira] \n",
      "\n",
      " ..................the final results....................  [(['have surgery'], [mira])] \n",
      "\n",
      "Tina   oh , my darling .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   jim make a contract with his manager .\n",
      "...............subject result befor resolution.............. [jim] \n",
      "\n",
      " ..................the final results....................  [(['make contract'], [jim])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   What's the news of you and your wife?\n",
      "Kriss   i started the new job in the American Communications Company .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['started job'], ['Kriss'])] \n",
      "\n",
      "Jhon   ezabela got a job at boston college and she will be moving back to USA soon .\n",
      "...............subject result befor resolution.............. [ezabela] \n",
      "\n",
      " ..................the final results....................  [(['got job'], [ezabela])] \n",
      "\n",
      "Kriss   It's very good news .\n",
      "   \n",
      "\n",
      "\n",
      "Tina   What is the news of your children, sister?\n",
      "Mari   mario passed the final exam for this year .\n",
      "...............subject result befor resolution.............. [mario] \n",
      "\n",
      " ..................the final results....................  [(['passed exam'], [mario])] \n",
      "\n",
      "Mari   kareem get to finish his masters .\n",
      "...............subject result befor resolution.............. [his] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ...............(  What is the news of your children, sister? mario passed the final exam for this year . kareem get to finish his masters . ) \n",
      "\n",
      " ..................the final results....................  [(['finish masters'], ['kareem'])] \n",
      "\n",
      "Mari   ezabela got a 95% on her first tough exam of the semester .\n",
      "...............subject result befor resolution.............. [her] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  mario passed the final exam for this year . kareem get to finish his masters . ezabela got a 95% on her first tough exam of the semester . ) \n",
      "\n",
      " ..................the final results....................  [(['got exam'], ['ezabela'])] \n",
      "\n",
      "Tina   I am happy for you my sister and proud of your children .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side .\n",
      "...............subject result befor resolution.............. [our] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side . ) \n",
      "\n",
      "element ....   Bati husband and Bati\n",
      " ..................the final results....................  [(['our wedding'], [' Bati husband and Bati'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   today is my wedding day .\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['my wedding'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   today is my birthday .\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['my birthday'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   today is my 11th wedding anniversary .\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['my anniversary'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   i am happy for bati .\n",
      "Jhon   why ?\n",
      "Tina   she finally start moving into her new house.\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  i am happy for bati . why ? she finally start moving into her new house. ) \n",
      "\n",
      " ..................the final results....................  [(['moving into'], ['bati'])] \n",
      "\n",
      "Jhon   oh , she finished preparing the new house ?\n",
      "Tina   Yes .\n",
      "   \n",
      "\n",
      "\n",
      "george   Where will you spend your vacation ?\n",
      "Mark   i will be moving to london at the end of the week , and i will stay there the next three month .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['moving to'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   i'll be traveling to USA to see a moral monday protest .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Why are you packing your bags ?\n",
      "Kriss   i am moving to bristol .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['moving to'], ['Kriss'])] \n",
      "\n",
      "Jhon   I will miss you a lot .\n",
      "   \n",
      "\n",
      "\n",
      "george   what is the lastest news about mira ?\n",
      "Bati   finally , she got the apartment! she is moving on february .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  what is the lastest news about mira ? finally , she got the apartment! she is moving on february . ) \n",
      "\n",
      " ..................the final results....................  [(['moving on'], ['mira'])] \n",
      "\n",
      "george   very good , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   luke and i will be moving to san francisco later this month .\n",
      "...............subject result befor resolution.............. [luke, i] \n",
      "\n",
      " ..................the final results....................  [(['moving to'], [luke, 'Tomas'])] \n",
      "\n",
      "Jhon   when will you come back ?\n",
      "Tomas   after three month .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   i am moving to US in 3 weeks .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['moving to'], ['Mark'])] \n",
      "\n",
      "Mari   Are you going to settle there ?\n",
      "Mark   i Have not decided yet .\n",
      "   \n",
      "\n",
      "\n",
      "Tom   I'm sad for Joy .\n",
      "Bati   why ? what's happend ?\n",
      "Tom   Joy's father died and he is very depressed .\n",
      "...............subject result befor resolution.............. [(joy, father)] \n",
      "\n",
      " ..................the final results....................  [(['died'], [(joy, father)])] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"Dialogs_data.txt\", \"r\")\n",
    "\n",
    "def splitting_sayers_and_sentences():\n",
    "    sentence=[]\n",
    "    sayers=[]\n",
    "    for line in f:\n",
    "        stripped_line = line.strip()\n",
    "        sayer=stripped_line[:stripped_line.find(\":\")]\n",
    "        sayers.append(sayer)\n",
    "        stripped_line=stripped_line[stripped_line.find(\":\")+1:]\n",
    "        sentence.append(stripped_line)    \n",
    "    return sayers,sentence\n",
    "\n",
    "sayers,sentence = splitting_sayers_and_sentences() \n",
    "\n",
    "\n",
    "def get_sayers_of_all_dialogs(sayers):\n",
    "    sayers_of_all_dialogs=[]\n",
    "    sy=[]\n",
    "    for i in sayers :\n",
    "        if i!=\"\":\n",
    "            sy.append(i)\n",
    "        if i==\"\" and len(sy)!=0 :\n",
    "            sayers_of_all_dialogs.append(sy)\n",
    "            sy=[]\n",
    "    return sayers_of_all_dialogs\n",
    "            \n",
    "    \n",
    "sayers_of_all_dialogs = get_sayers_of_all_dialogs(sayers)        \n",
    "            \n",
    "    \n",
    "def get_len_of_all_dialogs(sayers):\n",
    "    len_of_all_dialogs=[]\n",
    "    n=0\n",
    "    for i in sayers :\n",
    "        if i!=\"\":\n",
    "            n+=1\n",
    "        if i==\"\" and len_of_all_dialogs!=0:\n",
    "            len_of_all_dialogs.append(n)\n",
    "            n=0\n",
    "    return len_of_all_dialogs\n",
    "\n",
    "\n",
    "len_of_all_dialogs = get_len_of_all_dialogs(sayers)\n",
    "            \n",
    "        \n",
    "def get_sentences_of_all_dialogs(sentence):\n",
    "    sentences_of_all_dialogs=[]\n",
    "    sentences_of_each_dialog=[]\n",
    "    for i in sentence :\n",
    "        if i!=\"\":\n",
    "            sentences_of_each_dialog.append(i)\n",
    "        if i==\"\":\n",
    "            sentences_of_all_dialogs.append(sentences_of_each_dialog)\n",
    "            sentences_of_each_dialog=[]\n",
    "    return sentences_of_all_dialogs    \n",
    "\n",
    "sentences_of_all_dialogs = get_sentences_of_all_dialogs(sentence)\n",
    "\n",
    "\n",
    "def get_final_results(sentence):\n",
    "    index=-1\n",
    "    subject_result_befor_resolution=[]\n",
    "    final_results=[]   \n",
    "    #counter for index of sentences\n",
    "\n",
    "\n",
    "    index_of_sentence_in_dialog=[]\n",
    "    index_of_sentences_in_all_dialogs=[]\n",
    "    #  dialog number \n",
    "    n=0\n",
    "\n",
    "    aggreigation_dialog=[]\n",
    "    \n",
    "    for j,i in enumerate(sentence) :\n",
    "        print(sayers[j],\" \",i)\n",
    "        list_tuple=[]\n",
    "        t=i.lower()\n",
    "        doc=nlp(t)\n",
    "\n",
    "        if i!=\"\":\n",
    "            index+=1\n",
    "\n",
    "            index_of_sentence_in_dialog.append(index)\n",
    "\n",
    "        p1=Rules(doc, verbs_without_need_objects)\n",
    "        text=p1.retrival_senetce_rule1()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule1 \",text , findSubs_for_verbEvents(nlp(text[0]),verbs_without_need_objects)])\n",
    "\n",
    "\n",
    "        p2=Rules(doc, verbs_with_objects)\n",
    "        text=p2.retrival_senetce_rule2()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule2 \",text ,findSubs_for_verbEvents(nlp(text[0]),verbs_with_objects)])    \n",
    "\n",
    "        p3=Rules(doc, nouns)\n",
    "        text=p3.retrival_senetce_rule3()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule3 \",text ,findSubs_for_nounsEvents(nlp(text[0]),nouns)])\n",
    "\n",
    "\n",
    "        p4=Rules(doc, adjectives)\n",
    "        text=p4.retrival_senetce_rule4()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule4 \",text ,findSubs_for_adjectiveEvents(nlp(text[0]),adjectives)])\n",
    "\n",
    "\n",
    "        p5=Rules(doc, noun_direct_relation)\n",
    "        text=p5.retrival_senetce_rule5()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule5 \",text , findSubs_for_direct_relation(nlp(text[0]),noun_direct_relation)])\n",
    "\n",
    "\n",
    "        p6=Rules(doc, verb_with_prepo)\n",
    "        text=p6.retrival_senetce_rule6()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule6 \",text ,findSubs_for_actionEvents(nlp(text[0]),verb_with_prepo)])\n",
    "        # for rules conflict\n",
    "        filtering(list_tuple)\n",
    "        # for aggregation all subjects or all events for the sentence \n",
    "        if aggregation_for_sentence(list_tuple)!=None:\n",
    "\n",
    "            if aggregation_for_sentence(list_tuple)[0]!=\"\":\n",
    "                b=aggregation_for_sentence(list_tuple)\n",
    "\n",
    "                if len(b[1])!=0:\n",
    "                    id__=str(n+1)+\"_\"+str(index)\n",
    "\n",
    "                    print(\"...............subject result befor resolution..............\",b[1],\"\\n\" )\n",
    "                    subject_result_befor_resolution.append((b[1],id__))\n",
    "\n",
    "                # make pronoun resolution for getting the real person name    \n",
    "                resolution=pronoun_resolution_for_all_cases_of_pronouns(b,n,index,sentences_of_all_dialogs,num_of_sentence_for_pronoun_resolution,sayers_of_all_dialogs,len_of_all_dialogs)\n",
    "                # for aggregating the results of filtering and aggregating sentence with the results of pronoun resolution\n",
    "                aggrigation=aggregation_between_filtering_and_resolution(b,resolution)\n",
    "\n",
    "                if len(aggrigation)!=0:\n",
    "                    print(\" ..................the final results.................... \",aggrigation,\"\\n\" )\n",
    "                    id_=str(n+1)+\"_\"+str(index)\n",
    "                    final_results.append((aggrigation,id_))\n",
    "\n",
    "\n",
    "\n",
    "        if i==\"\" and len(index_of_sentence_in_dialog)!=0 :\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "            index_of_sentences_in_all_dialogs.append(index_of_sentence_in_dialog)\n",
    "            index=-1\n",
    "\n",
    "            index_of_sentence_in_dialog=[]\n",
    "            n+=1\n",
    "            print(\"\\n\")\n",
    "    return  final_results \n",
    "\n",
    "final_results=get_final_results(sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the results of our task and the labeled dataset for comparing results and evaluating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['get divorce'], [susan])]\n",
      "events ==>  ['get divorce']  ||| subjects ==>  [susan]  ||| dialog number ==>  1_0\n",
      "\n",
      "\n",
      "[(['got divorce'], [' Mari and george'])]\n",
      "events ==>  ['got divorce']  ||| subjects ==>  [' Mari and george']  ||| dialog number ==>  3_1\n",
      "\n",
      "\n",
      "[(['married', 'passed exam'], ['Mari'])]\n",
      "events ==>  ['married', 'passed exam']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  4_1\n",
      "\n",
      "\n",
      "[(['had wedding'], ['She'])]\n",
      "events ==>  ['had wedding']  ||| subjects ==>  ['She']  ||| dialog number ==>  4_3\n",
      "\n",
      "\n",
      "[(['married'], ['Jhon and Abigail'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Jhon and Abigail']  ||| dialog number ==>  6_0\n",
      "\n",
      "\n",
      "[(['your wedding'], ['Abigail'])]\n",
      "events ==>  ['your wedding']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  7_0\n",
      "\n",
      "\n",
      "[(['married'], ['Abigail husband', 'Abigail'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Abigail husband', 'Abigail']  ||| dialog number ==>  7_1\n",
      "\n",
      "\n",
      "[(['your wedding'], ['Abigail'])]\n",
      "events ==>  ['your wedding']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  8_0\n",
      "\n",
      "\n",
      "[(['go to'], ['Jhon and Mark'])]\n",
      "events ==>  ['go to']  ||| subjects ==>  ['Jhon and Mark']  ||| dialog number ==>  9_3\n",
      "\n",
      "\n",
      "[(['celebrated birthday'], ['Jhon'])]\n",
      "events ==>  ['celebrated birthday']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  12_1\n",
      "\n",
      "\n",
      "[(['celebrate news'], ['Jhon'])]\n",
      "events ==>  ['celebrate news']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  13_0\n",
      "\n",
      "\n",
      "[(['graduate'], ['Jhon'])]\n",
      "events ==>  ['graduate']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  14_0\n",
      "\n",
      "\n",
      "[(['graduated university'], ['Jhon'])]\n",
      "events ==>  ['graduated university']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  14_1\n",
      "\n",
      "\n",
      "[(['graduated'], ['Abigail'])]\n",
      "events ==>  ['graduated']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  16_1\n",
      "\n",
      "\n",
      "[(['went to'], ['Abigail'])]\n",
      "events ==>  ['went to']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  17_1\n",
      "\n",
      "\n",
      "[(['graduated'], ['Margaret son'])]\n",
      "events ==>  ['graduated']  ||| subjects ==>  ['Margaret son']  ||| dialog number ==>  18_0\n",
      "\n",
      "\n",
      "[(['passed exam'], ['Pamela'])]\n",
      "events ==>  ['passed exam']  ||| subjects ==>  ['Pamela']  ||| dialog number ==>  19_1\n",
      "\n",
      "\n",
      "[(['married'], ['Pamela'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Pamela']  ||| dialog number ==>  19_2\n",
      "\n",
      "\n",
      "[(['married'], ['Mark'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  21_2\n",
      "\n",
      "\n",
      "[(['married'], ['Mark'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  22_0\n",
      "\n",
      "\n",
      "[(['invited wedding'], ['Mark'])]\n",
      "events ==>  ['invited wedding']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  22_2\n",
      "\n",
      "\n",
      "[(['married'], ['Margaret'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Margaret']  ||| dialog number ==>  23_0\n",
      "\n",
      "\n",
      "[(['married'], ['Abigail'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  24_2\n",
      "\n",
      "\n",
      "[(['married'], ['george'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['george']  ||| dialog number ==>  25_1\n",
      "\n",
      "\n",
      "[(['married'], [' Abigail and george'])]\n",
      "events ==>  ['married']  ||| subjects ==>  [' Abigail and george']  ||| dialog number ==>  26_1\n",
      "\n",
      "\n",
      "[(['married'], [frank])]\n",
      "events ==>  ['married']  ||| subjects ==>  [frank]  ||| dialog number ==>  27_0\n",
      "\n",
      "\n",
      "[(['won prize'], ['Mark'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  28_1\n",
      "\n",
      "\n",
      "[(['won prize'], ['Mark'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  29_0\n",
      "\n",
      "\n",
      "[(['won prize'], ['Abigail'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  30_0\n",
      "\n",
      "\n",
      "[(['won prize'], ['george'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['george']  ||| dialog number ==>  31_1\n",
      "\n",
      "\n",
      "[(['won prize'], ['Mark'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  32_0\n",
      "\n",
      "\n",
      "[(['won prize'], ['Jhon'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  34_1\n",
      "\n",
      "\n",
      "[(['won prize'], ['Margaret'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Margaret']  ||| dialog number ==>  35_1\n",
      "\n",
      "\n",
      "[(['meet'], ['Margaret and Abigail'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Margaret and Abigail']  ||| dialog number ==>  36_0\n",
      "\n",
      "\n",
      "[(['your meeting'], ['Mark'])]\n",
      "events ==>  ['your meeting']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  37_0\n",
      "\n",
      "\n",
      "[(['be meeting'], [''])]\n",
      "events ==>  ['be meeting']  ||| subjects ==>  ['']  ||| dialog number ==>  37_1\n",
      "\n",
      "\n",
      "[(['get meeting'], ['Mark'])]\n",
      "events ==>  ['get meeting']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  38_0\n",
      "\n",
      "\n",
      "[(['go to'], ['Mark'])]\n",
      "events ==>  ['go to']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  38_1\n",
      "\n",
      "\n",
      "[(['going meeting'], ['george'])]\n",
      "events ==>  ['going meeting']  ||| subjects ==>  ['george']  ||| dialog number ==>  39_0\n",
      "\n",
      "\n",
      "[(['have meeting'], ['Jhon'])]\n",
      "events ==>  ['have meeting']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  40_2\n",
      "\n",
      "\n",
      "[(['meet'], ['Jhon'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  41_0\n",
      "\n",
      "\n",
      "[(['meet'], ['Mark'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  42_1\n",
      "\n",
      "\n",
      "[(['meet'], ['Mark and Jhon'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Mark and Jhon']  ||| dialog number ==>  43_1\n",
      "\n",
      "\n",
      "[(['meet'], ['george'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['george']  ||| dialog number ==>  44_1\n",
      "\n",
      "\n",
      "[(['meet'], ['Mari'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  45_1\n",
      "\n",
      "\n",
      "[(['meet'], ['Jhon'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  46_0\n",
      "\n",
      "\n",
      "[(['our meeting'], ['george and Mari'])]\n",
      "events ==>  ['our meeting']  ||| subjects ==>  ['george and Mari']  ||| dialog number ==>  47_2\n",
      "\n",
      "\n",
      "[(['meet'], ['george'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['george']  ||| dialog number ==>  48_2\n",
      "\n",
      "\n",
      "[(['started meeting'], ['Kriss and george'])]\n",
      "events ==>  ['started meeting']  ||| subjects ==>  ['Kriss and george']  ||| dialog number ==>  49_1\n",
      "\n",
      "\n",
      "[(['hurt'], ['Kriss'])]\n",
      "events ==>  ['hurt']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  50_0\n",
      "\n",
      "\n",
      "[(['hurts', 'got headache'], ['george stomach', 'george'])]\n",
      "events ==>  ['hurts', 'got headache']  ||| subjects ==>  ['george stomach', 'george']  ||| dialog number ==>  52_1\n",
      "\n",
      "\n",
      "[(['hurt'], ['Jhon'])]\n",
      "events ==>  ['hurt']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  53_1\n",
      "\n",
      "\n",
      "[(['had surgery'], ['Mark'])]\n",
      "events ==>  ['had surgery']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  54_0\n",
      "\n",
      "\n",
      "[(['born'], ['Kriss'])]\n",
      "events ==>  ['born']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  55_1\n",
      "\n",
      "\n",
      "[(['passed test'], ['Mark'])]\n",
      "events ==>  ['passed test']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  56_1\n",
      "\n",
      "\n",
      "[(['pass test'], ['george'])]\n",
      "events ==>  ['pass test']  ||| subjects ==>  ['george']  ||| dialog number ==>  57_0\n",
      "\n",
      "\n",
      "[(['passed interview'], ['george'])]\n",
      "events ==>  ['passed interview']  ||| subjects ==>  ['george']  ||| dialog number ==>  59_2\n",
      "\n",
      "\n",
      "[(['are pregnant'], ['Abigail'])]\n",
      "events ==>  ['are pregnant']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  60_0\n",
      "\n",
      "\n",
      "[(['accepted offers', 'come interview'], ['george'])]\n",
      "events ==>  ['accepted offers', 'come interview']  ||| subjects ==>  ['george']  ||| dialog number ==>  62_0\n",
      "\n",
      "\n",
      "[(['buy car'], ['Wilson'])]\n",
      "events ==>  ['buy car']  ||| subjects ==>  ['Wilson']  ||| dialog number ==>  64_1\n",
      "\n",
      "\n",
      "[(['failed test'], ['george'])]\n",
      "events ==>  ['failed test']  ||| subjects ==>  ['george']  ||| dialog number ==>  65_1\n",
      "\n",
      "\n",
      "[(['have interview'], ['Tina'])]\n",
      "events ==>  ['have interview']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  66_2\n",
      "\n",
      "\n",
      "[(['my interview'], ['george'])]\n",
      "events ==>  ['my interview']  ||| subjects ==>  ['george']  ||| dialog number ==>  69_0\n",
      "\n",
      "\n",
      "[(['pass interview'], ['george'])]\n",
      "events ==>  ['pass interview']  ||| subjects ==>  ['george']  ||| dialog number ==>  70_0\n",
      "\n",
      "\n",
      "[(['have interview'], ['Kriss'])]\n",
      "events ==>  ['have interview']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  71_0\n",
      "\n",
      "\n",
      "[(['married'], ['Mark'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  72_1\n",
      "\n",
      "\n",
      "[(['born'], ['george daughter'])]\n",
      "events ==>  ['born']  ||| subjects ==>  ['george daughter']  ||| dialog number ==>  73_1\n",
      "\n",
      "\n",
      "[(['interviewed'], ['Kriss'])]\n",
      "events ==>  ['interviewed']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  74_0\n",
      "\n",
      "\n",
      "[(['interviewed'], ['Jhon'])]\n",
      "events ==>  ['interviewed']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  75_2\n",
      "\n",
      "\n",
      "[(['married'], ['Margaret'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Margaret']  ||| dialog number ==>  76_0\n",
      "\n",
      "\n",
      "[(['engaged'], ['Margaret', matt])]\n",
      "events ==>  ['engaged']  ||| subjects ==>  ['Margaret', matt]  ||| dialog number ==>  77_0\n",
      "\n",
      "\n",
      "[(['engaged'], [nathan, 'Kriss'])]\n",
      "events ==>  ['engaged']  ||| subjects ==>  [nathan, 'Kriss']  ||| dialog number ==>  78_0\n",
      "\n",
      "\n",
      "[(['divorced'], [tomas])]\n",
      "events ==>  ['divorced']  ||| subjects ==>  [tomas]  ||| dialog number ==>  79_2\n",
      "\n",
      "\n",
      "[(['engaged'], [george, 'Mari'])]\n",
      "events ==>  ['engaged']  ||| subjects ==>  [george, 'Mari']  ||| dialog number ==>  80_1\n",
      "\n",
      "\n",
      "[(['married'], ['Jhon'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  81_0\n",
      "\n",
      "\n",
      "[(['accepted university'], ['Jhon'])]\n",
      "events ==>  ['accepted university']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  82_2\n",
      "\n",
      "\n",
      "[(['passed exam'], [mario])]\n",
      "events ==>  ['passed exam']  ||| subjects ==>  [mario]  ||| dialog number ==>  83_2\n",
      "\n",
      "\n",
      "[(['signed deal'], ['Mari'])]\n",
      "events ==>  ['signed deal']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  84_1\n",
      "\n",
      "\n",
      "[(['accepted offer', 'my job'], ['Jhon'])]\n",
      "events ==>  ['accepted offer', 'my job']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  85_1\n",
      "\n",
      "\n",
      "[(['accepted school'], [tom])]\n",
      "events ==>  ['accepted school']  ||| subjects ==>  [tom]  ||| dialog number ==>  86_1\n",
      "\n",
      "\n",
      "[(['accepted university'], ['Margaret'])]\n",
      "events ==>  ['accepted university']  ||| subjects ==>  ['Margaret']  ||| dialog number ==>  87_0\n",
      "\n",
      "\n",
      "[(['accepted program'], ['Mark'])]\n",
      "events ==>  ['accepted program']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  88_0\n",
      "\n",
      "\n",
      "[(['won awards'], [bati])]\n",
      "events ==>  ['won awards']  ||| subjects ==>  [bati]  ||| dialog number ==>  89_1\n",
      "\n",
      "\n",
      "[(['celebrating graduation'], ['Mark and george'])]\n",
      "events ==>  ['celebrating graduation']  ||| subjects ==>  ['Mark and george']  ||| dialog number ==>  90_2\n",
      "\n",
      "\n",
      "[(['failed exam'], ['Mari'])]\n",
      "events ==>  ['failed exam']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  91_1\n",
      "\n",
      "\n",
      "[(['win award'], ['Bati'])]\n",
      "events ==>  ['win award']  ||| subjects ==>  ['Bati']  ||| dialog number ==>  92_2\n",
      "\n",
      "\n",
      "[(['accepted offer'], ['Tina'])]\n",
      "events ==>  ['accepted offer']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  93_1\n",
      "\n",
      "\n",
      "[(['passed exam'], [(karla, son)])]\n",
      "events ==>  ['passed exam']  ||| subjects ==>  [(karla, son)]  ||| dialog number ==>  94_2\n",
      "\n",
      "\n",
      "[(['celebrating anniversary'], ['George'])]\n",
      "events ==>  ['celebrating anniversary']  ||| subjects ==>  ['George']  ||| dialog number ==>  96_1\n",
      "\n",
      "\n",
      "[(['won awards'], [' Tina sister'])]\n",
      "events ==>  ['won awards']  ||| subjects ==>  [' Tina sister']  ||| dialog number ==>  97_1\n",
      "\n",
      "\n",
      "[(['accepted university'], ['Jhon'])]\n",
      "events ==>  ['accepted university']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  98_1\n",
      "\n",
      "\n",
      "[(['accepted college'], [tom])]\n",
      "events ==>  ['accepted college']  ||| subjects ==>  [tom]  ||| dialog number ==>  99_2\n",
      "\n",
      "\n",
      "[(['won lawsuit'], [tom])]\n",
      "events ==>  ['won lawsuit']  ||| subjects ==>  [tom]  ||| dialog number ==>  100_2\n",
      "\n",
      "\n",
      "[(['find job'], ['Mark'])]\n",
      "events ==>  ['find job']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  101_0\n",
      "\n",
      "\n",
      "[(['started job'], ['Mark'])]\n",
      "events ==>  ['started job']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  101_1\n",
      "\n",
      "\n",
      "[(['celebrate anniversary'], ['Jhon wife', 'Jhon'])]\n",
      "events ==>  ['celebrate anniversary']  ||| subjects ==>  ['Jhon wife', 'Jhon']  ||| dialog number ==>  102_1\n",
      "\n",
      "\n",
      "[(['lost weight'], ['Jhon'])]\n",
      "events ==>  ['lost weight']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  103_1\n",
      "\n",
      "\n",
      "[(['celebrate birthday'], ['Tina'])]\n",
      "events ==>  ['celebrate birthday']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  104_0\n",
      "\n",
      "\n",
      "[(['accepted job'], ['Mark'])]\n",
      "events ==>  ['accepted job']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  105_1\n",
      "\n",
      "\n",
      "[(['accepted offer'], ['george'])]\n",
      "events ==>  ['accepted offer']  ||| subjects ==>  ['george']  ||| dialog number ==>  106_0\n",
      "\n",
      "\n",
      "[(['bought house'], ['Mari'])]\n",
      "events ==>  ['bought house']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  107_2\n",
      "\n",
      "\n",
      "[(['start job'], ['george'])]\n",
      "events ==>  ['start job']  ||| subjects ==>  ['george']  ||| dialog number ==>  108_0\n",
      "\n",
      "\n",
      "[(['start job'], ['george'])]\n",
      "events ==>  ['start job']  ||| subjects ==>  ['george']  ||| dialog number ==>  108_1\n",
      "\n",
      "\n",
      "[(['broken leg'], ['Mark'])]\n",
      "events ==>  ['broken leg']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  109_1\n",
      "\n",
      "\n",
      "[(['accepted offer'], ['Tom'])]\n",
      "events ==>  ['accepted offer']  ||| subjects ==>  ['Tom']  ||| dialog number ==>  110_0\n",
      "\n",
      "\n",
      "[(['won award'], ['Kriss'])]\n",
      "events ==>  ['won award']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  111_1\n",
      "\n",
      "\n",
      "[(['accepted offer'], ['Frank'])]\n",
      "events ==>  ['accepted offer']  ||| subjects ==>  ['Frank']  ||| dialog number ==>  112_0\n",
      "\n",
      "\n",
      "[(['start job'], ['Tomas'])]\n",
      "events ==>  ['start job']  ||| subjects ==>  ['Tomas']  ||| dialog number ==>  113_0\n",
      "\n",
      "\n",
      "[(['go to'], ['Mark and Jhon'])]\n",
      "events ==>  ['go to']  ||| subjects ==>  ['Mark and Jhon']  ||| dialog number ==>  114_0\n",
      "\n",
      "\n",
      "[(['marked anniversary'], [dani])]\n",
      "events ==>  ['marked anniversary']  ||| subjects ==>  [dani]  ||| dialog number ==>  114_2\n",
      "\n",
      "\n",
      "[(['won prize'], ['Kriss'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  115_2\n",
      "\n",
      "\n",
      "[(['graduated'], ['Tina'])]\n",
      "events ==>  ['graduated']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  116_0\n",
      "\n",
      "\n",
      "[(['is relationship'], [tony])]\n",
      "events ==>  ['is relationship']  ||| subjects ==>  [tony]  ||| dialog number ==>  117_1\n",
      "\n",
      "\n",
      "[(['has exam'], [(karolina, son)])]\n",
      "events ==>  ['has exam']  ||| subjects ==>  [(karolina, son)]  ||| dialog number ==>  118_1\n",
      "\n",
      "\n",
      "[(['got promotion'], ['Tina'])]\n",
      "events ==>  ['got promotion']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  119_0\n",
      "\n",
      "\n",
      "[(['got offer'], ['Tomas'])]\n",
      "events ==>  ['got offer']  ||| subjects ==>  ['Tomas']  ||| dialog number ==>  120_2\n",
      "\n",
      "\n",
      "[(['am sick'], ['Tina'])]\n",
      "events ==>  ['am sick']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  121_1\n",
      "\n",
      "\n",
      "[(['have fever'], ['Tina'])]\n",
      "events ==>  ['have fever']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  121_2\n",
      "\n",
      "\n",
      "[(['have interview'], ['Jhon'])]\n",
      "events ==>  ['have interview']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  122_0\n",
      "\n",
      "\n",
      "[(['have surgery'], [mira])]\n",
      "events ==>  ['have surgery']  ||| subjects ==>  [mira]  ||| dialog number ==>  123_2\n",
      "\n",
      "\n",
      "[(['make contract'], [jim])]\n",
      "events ==>  ['make contract']  ||| subjects ==>  [jim]  ||| dialog number ==>  124_0\n",
      "\n",
      "\n",
      "[(['started job'], ['Kriss'])]\n",
      "events ==>  ['started job']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  125_1\n",
      "\n",
      "\n",
      "[(['got job'], [ezabela])]\n",
      "events ==>  ['got job']  ||| subjects ==>  [ezabela]  ||| dialog number ==>  125_2\n",
      "\n",
      "\n",
      "[(['passed exam'], [mario])]\n",
      "events ==>  ['passed exam']  ||| subjects ==>  [mario]  ||| dialog number ==>  126_1\n",
      "\n",
      "\n",
      "[(['finish masters'], ['kareem'])]\n",
      "events ==>  ['finish masters']  ||| subjects ==>  ['kareem']  ||| dialog number ==>  126_2\n",
      "\n",
      "\n",
      "[(['got exam'], ['ezabela'])]\n",
      "events ==>  ['got exam']  ||| subjects ==>  ['ezabela']  ||| dialog number ==>  126_3\n",
      "\n",
      "\n",
      "[(['our wedding'], [' Bati husband and Bati'])]\n",
      "events ==>  ['our wedding']  ||| subjects ==>  [' Bati husband and Bati']  ||| dialog number ==>  127_0\n",
      "\n",
      "\n",
      "[(['my wedding'], ['Kriss'])]\n",
      "events ==>  ['my wedding']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  128_0\n",
      "\n",
      "\n",
      "[(['my birthday'], ['Kriss'])]\n",
      "events ==>  ['my birthday']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  129_0\n",
      "\n",
      "\n",
      "[(['my anniversary'], ['Kriss'])]\n",
      "events ==>  ['my anniversary']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  130_0\n",
      "\n",
      "\n",
      "[(['moving into'], ['bati'])]\n",
      "events ==>  ['moving into']  ||| subjects ==>  ['bati']  ||| dialog number ==>  131_2\n",
      "\n",
      "\n",
      "[(['moving to'], ['Mark'])]\n",
      "events ==>  ['moving to']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  132_1\n",
      "\n",
      "\n",
      "[(['moving to'], ['Kriss'])]\n",
      "events ==>  ['moving to']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  134_1\n",
      "\n",
      "\n",
      "[(['moving on'], ['mira'])]\n",
      "events ==>  ['moving on']  ||| subjects ==>  ['mira']  ||| dialog number ==>  135_1\n",
      "\n",
      "\n",
      "[(['moving to'], [luke, 'Tomas'])]\n",
      "events ==>  ['moving to']  ||| subjects ==>  [luke, 'Tomas']  ||| dialog number ==>  136_0\n",
      "\n",
      "\n",
      "[(['moving to'], ['Mark'])]\n",
      "events ==>  ['moving to']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  137_0\n",
      "\n",
      "\n",
      "[(['died'], [(joy, father)])]\n",
      "events ==>  ['died']  ||| subjects ==>  [(joy, father)]  ||| dialog number ==>  138_2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# my results events and subjects \n",
    "def get_sub_event_id_from_result(final_results):\n",
    "    dialog_id_results = []\n",
    "    events_results = []\n",
    "    subjects_results = []\n",
    "    for i in final_results:\n",
    "        b = i[0]\n",
    "        print(b)\n",
    "        dialog_id_results.append(i[1])\n",
    "        print(\"events ==> \", str(b[0][0]), \" ||| subjects ==> \", str(b[0][1]), \" ||| dialog number ==> \", i[1])\n",
    "        print(\"\\n\")\n",
    "        events_results.append(b[0][0])\n",
    "        subjects_results.append(b[0][1])\n",
    "    return dialog_id_results, events_results, subjects_results\n",
    "\n",
    "dialog_id_results, events_results, subjects_results = get_sub_event_id_from_result(final_results)\n",
    "\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events ==>  get divorce  ||| subjects ==>  Susan  ||| subjects befor resolution ==>  Susan  ||| dialog number ==>  1_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  2\n",
      "\n",
      "\n",
      "events ==>  got divorce  ||| subjects ==>  Mari,george  ||| subjects befor resolution ==>  they  ||| dialog number ==>  3_1\n",
      "\n",
      "\n",
      "events ==>  married,passed exam  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  she  ||| dialog number ==>  4_1\n",
      "\n",
      "\n",
      "events ==>  had wedding  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  she  ||| dialog number ==>  4_3\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  5\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail,Abigail husband  ||| subjects befor resolution ==>  we  ||| dialog number ==>  6_0\n",
      "\n",
      "\n",
      "events ==>  your wedding  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  your  ||| dialog number ==>  7_0\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail husband,Abigail  ||| subjects befor resolution ==>  my husband,i  ||| dialog number ==>  7_1\n",
      "\n",
      "\n",
      "events ==>  your wedding  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  your  ||| dialog number ==>  8_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  9_2\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  9_3\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  10\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  11\n",
      "\n",
      "\n",
      "events ==>  celebrated birthday  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  12_1\n",
      "\n",
      "\n",
      "events ==>  celebrate news  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  my  ||| dialog number ==>  13_0\n",
      "\n",
      "\n",
      "events ==>  graduated  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  you  ||| dialog number ==>  14_0\n",
      "\n",
      "\n",
      "events ==>  graduated University  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  14_1\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  15\n",
      "\n",
      "\n",
      "events ==>  graduated  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  you  ||| dialog number ==>  16_1\n",
      "\n",
      "\n",
      "events ==>  went to  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  i  ||| dialog number ==>  17_1\n",
      "\n",
      "\n",
      "events ==>  graduated  ||| subjects ==>  Margaret son  ||| subjects befor resolution ==>  your son  ||| dialog number ==>  18_0\n",
      "\n",
      "\n",
      "events ==>  passed exam  ||| subjects ==>  Pamela  ||| subjects befor resolution ==>  she  ||| dialog number ==>  19_1\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Pamela  ||| subjects befor resolution ==>  she  ||| dialog number ==>  19_2\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  20\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail,Mark  ||| subjects befor resolution ==>  we  ||| dialog number ==>  21_2\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  22_0\n",
      "\n",
      "\n",
      "events ==>  invited wedding  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  my  ||| dialog number ==>  22_2\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Margaret  ||| subjects befor resolution ==>  you  ||| dialog number ==>  23_0\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  i  ||| dialog number ==>  24_2\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  george  ||| subjects befor resolution ==>  he  ||| dialog number ==>  25_1\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail,george  ||| subjects befor resolution ==>  they  ||| dialog number ==>  26_1\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Frank  ||| subjects befor resolution ==>  frank  ||| dialog number ==>  27_0\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  28_1\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  29_0\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  i  ||| dialog number ==>  30_0\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  george  ||| subjects befor resolution ==>  he  ||| dialog number ==>  31_1\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  32_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  33\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  34_1\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Margaret  ||| subjects befor resolution ==>  i  ||| dialog number ==>  35_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Margaret,Abigail  ||| subjects befor resolution ==>  we  ||| dialog number ==>  36_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  36_1\n",
      "\n",
      "\n",
      "events ==>  your meeting  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  your  ||| dialog number ==>  37_0\n",
      "\n",
      "\n",
      "events ==>  be meeting  ||| subjects ==>  Mark,Abigail  ||| subjects befor resolution ==>  we  ||| dialog number ==>  37_1\n",
      "\n",
      "\n",
      "events ==>  get meeting  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  my  ||| dialog number ==>  38_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  38_1\n",
      "\n",
      "\n",
      "events ==>  going meeting  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  39_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  40_0\n",
      "\n",
      "\n",
      "events ==>  have meeting  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  40_2\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  41_0\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  42_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Jhon,Mark  ||| subjects befor resolution ==>  we  ||| dialog number ==>  43_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  44_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  i  ||| dialog number ==>  45_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  46_0\n",
      "\n",
      "\n",
      "events ==>  our meeting  ||| subjects ==>  Mark,Mari  ||| subjects befor resolution ==>  our  ||| dialog number ==>  47_2\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  48_2\n",
      "\n",
      "\n",
      "events ==>  started meeting  ||| subjects ==>  george,Kriss  ||| subjects befor resolution ==>  we  ||| dialog number ==>  49_1\n",
      "\n",
      "\n",
      "events ==>  hurt  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  50_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  51_0\n",
      "\n",
      "\n",
      "events ==>  hurts,got headache  ||| subjects ==>  george stomach,george  ||| subjects befor resolution ==>  my stomach,i  ||| dialog number ==>  52_1\n",
      "\n",
      "\n",
      "events ==>  hurt  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  he  ||| dialog number ==>  53_1\n",
      "\n",
      "\n",
      "events ==>  had surgery  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  54_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  54_1\n",
      "\n",
      "\n",
      "events ==>  born  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  55_1\n",
      "\n",
      "\n",
      "events ==>  passed test  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  56_1\n",
      "\n",
      "\n",
      "events ==>  passed test  ||| subjects ==>  george  ||| subjects befor resolution ==>  you  ||| dialog number ==>  57_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  58\n",
      "\n",
      "\n",
      "events ==>  passed interview  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  59_2\n",
      "\n",
      "\n",
      "events ==>  are pregnant  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  you  ||| dialog number ==>  60_0\n",
      "\n",
      "\n",
      "events ==>  accept invitation  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  61_1\n",
      "\n",
      "\n",
      "events ==>  accepted offers,come interview  ||| subjects ==>  mark  ||| subjects befor resolution ==>  you  ||| dialog number ==>  62_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  63\n",
      "\n",
      "\n",
      "events ==>  buy car  ||| subjects ==>  Wilson  ||| subjects befor resolution ==>  i  ||| dialog number ==>  64_1\n",
      "\n",
      "\n",
      "events ==>  failed test  ||| subjects ==>  george  ||| subjects befor resolution ==>  you  ||| dialog number ==>  65_1\n",
      "\n",
      "\n",
      "events ==>  have interview  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  66_2\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  67\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  68_0\n",
      "\n",
      "\n",
      "events ==>  my interview  ||| subjects ==>  george  ||| subjects befor resolution ==>  my  ||| dialog number ==>  69_0\n",
      "\n",
      "\n",
      "events ==>  pass interview  ||| subjects ==>  george  ||| subjects befor resolution ==>  you  ||| dialog number ==>  70_0\n",
      "\n",
      "\n",
      "events ==>  have interview  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  71_0\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  72_1\n",
      "\n",
      "\n",
      "events ==>  born  ||| subjects ==>  george daughter  ||| subjects befor resolution ==>  my daughter  ||| dialog number ==>  73_1\n",
      "\n",
      "\n",
      "events ==>  interviewed  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  me  ||| dialog number ==>  74_0\n",
      "\n",
      "\n",
      "events ==>  interviewed  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  75_2\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Margaret  ||| subjects befor resolution ==>  i  ||| dialog number ==>  76_0\n",
      "\n",
      "\n",
      "events ==>  engaged  ||| subjects ==>  Margaret,matt  ||| subjects befor resolution ==>  me,matt  ||| dialog number ==>  77_0\n",
      "\n",
      "\n",
      "events ==>  engaged  ||| subjects ==>  nathan,Kriss  ||| subjects befor resolution ==>  nathan,i  ||| dialog number ==>  78_0\n",
      "\n",
      "\n",
      "events ==>  divorced  ||| subjects ==>  tomas  ||| subjects befor resolution ==>  tomas  ||| dialog number ==>  79_2\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  80_0\n",
      "\n",
      "\n",
      "events ==>  engaged  ||| subjects ==>  george,Mari  ||| subjects befor resolution ==>  george,i  ||| dialog number ==>  80_1\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  you  ||| dialog number ==>  81_0\n",
      "\n",
      "\n",
      "events ==>  accepted university  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  82_2\n",
      "\n",
      "\n",
      "events ==>  passed exam  ||| subjects ==>  mario  ||| subjects befor resolution ==>  mario  ||| dialog number ==>  83_2\n",
      "\n",
      "\n",
      "events ==>  signed deal  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  i  ||| dialog number ==>  84_1\n",
      "\n",
      "\n",
      "events ==>  accepted offer,my job  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  I,my  ||| dialog number ==>  85_1\n",
      "\n",
      "\n",
      "events ==>  accepted school  ||| subjects ==>  tom  ||| subjects befor resolution ==>  tom  ||| dialog number ==>  86_1\n",
      "\n",
      "\n",
      "events ==>  accepted university  ||| subjects ==>  Margaret  ||| subjects befor resolution ==>  i  ||| dialog number ==>  87_0\n",
      "\n",
      "\n",
      "events ==>  accepted program  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  88_0\n",
      "\n",
      "\n",
      "events ==>  won awards  ||| subjects ==>  bati  ||| subjects befor resolution ==>  bati  ||| dialog number ==>  89_1\n",
      "\n",
      "\n",
      "events ==>  celebrating graduation  ||| subjects ==>  george's family  ||| subjects befor resolution ==>  we  ||| dialog number ==>  90_2\n",
      "\n",
      "\n",
      "events ==>  failed exam  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  i  ||| dialog number ==>  91_1\n",
      "\n",
      "\n",
      "events ==>  win award  ||| subjects ==>  Bati  ||| subjects befor resolution ==>  i  ||| dialog number ==>  92_2\n",
      "\n",
      "\n",
      "events ==>  accepted offer  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  93_1\n",
      "\n",
      "\n",
      "events ==>  passed exam  ||| subjects ==>  karla son  ||| subjects befor resolution ==>  karla son  ||| dialog number ==>  94_2\n",
      "\n",
      "\n",
      "events ==>  celebrating birthday  ||| subjects ==>  mira  ||| subjects befor resolution ==>  mira  ||| dialog number ==>  95_1\n",
      "\n",
      "\n",
      "events ==>  celebrating anniversary  ||| subjects ==>  George  ||| subjects befor resolution ==>  i  ||| dialog number ==>  96_1\n",
      "\n",
      "\n",
      "events ==>  won awards  ||| subjects ==>  george sister  ||| subjects befor resolution ==>  she  ||| dialog number ==>  97_1\n",
      "\n",
      "\n",
      "events ==>  accepted university  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  98_1\n",
      "\n",
      "\n",
      "events ==>  accepted college  ||| subjects ==>  tom  ||| subjects befor resolution ==>  tom  ||| dialog number ==>  99_2\n",
      "\n",
      "\n",
      "events ==>  won lawsuit  ||| subjects ==>  tom  ||| subjects befor resolution ==>  tom  ||| dialog number ==>  100_2\n",
      "\n",
      "\n",
      "events ==>  find job  ||| subjects ==>  mark  ||| subjects befor resolution ==>  you  ||| dialog number ==>  101_0\n",
      "\n",
      "\n",
      "events ==>  started job  ||| subjects ==>  mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  101_1\n",
      "\n",
      "\n",
      "events ==>  celebrate anniversary  ||| subjects ==>  Jhon wife,Jhon  ||| subjects befor resolution ==>  my wife,i  ||| dialog number ==>  102_1\n",
      "\n",
      "\n",
      "events ==>  lost weight  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  103_1\n",
      "\n",
      "\n",
      "events ==>  celebrate birthday  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  104_0\n",
      "\n",
      "\n",
      "events ==>  accepted job  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  105_1\n",
      "\n",
      "\n",
      "events ==>  accepted offer  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  106_0\n",
      "\n",
      "\n",
      "events ==>  bought house  ||| subjects ==>  mari  ||| subjects befor resolution ==>  she  ||| dialog number ==>  107_2\n",
      "\n",
      "\n",
      "events ==>  start job  ||| subjects ==>  george  ||| subjects befor resolution ==>  you  ||| dialog number ==>  108_0\n",
      "\n",
      "\n",
      "events ==>  start job  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  108_1\n",
      "\n",
      "\n",
      "events ==>  broken leg  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  109_1\n",
      "\n",
      "\n",
      "events ==>  accepted offer  ||| subjects ==>  Tom  ||| subjects befor resolution ==>  i  ||| dialog number ==>  110_0\n",
      "\n",
      "\n",
      "events ==>  won award  ||| subjects ==>  mari  ||| subjects befor resolution ==>  she  ||| dialog number ==>  111_1\n",
      "\n",
      "\n",
      "events ==>  accepted offer  ||| subjects ==>  Frank  ||| subjects befor resolution ==>  i  ||| dialog number ==>  112_0\n",
      "\n",
      "\n",
      "events ==>  start job  ||| subjects ==>  Tomas  ||| subjects befor resolution ==>  i  ||| dialog number ==>  113_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  114_0\n",
      "\n",
      "\n",
      "events ==>  marked anniversary  ||| subjects ==>  dani  ||| subjects befor resolution ==>  dani  ||| dialog number ==>  114_2\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  115_2\n",
      "\n",
      "\n",
      "events ==>  graduated  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  you  ||| dialog number ==>  116_0\n",
      "\n",
      "\n",
      "events ==>  defending thesis  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  my  ||| dialog number ==>  116_1\n",
      "\n",
      "\n",
      "events ==>  is relationship  ||| subjects ==>  tony  ||| subjects befor resolution ==>  tony  ||| dialog number ==>  117_1\n",
      "\n",
      "\n",
      "events ==>  has exam  ||| subjects ==>  karolina son  ||| subjects befor resolution ==>  karolina son  ||| dialog number ==>  118_1\n",
      "\n",
      "\n",
      "events ==>  got promotion  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  119_0\n",
      "\n",
      "\n",
      "events ==>  got offer  ||| subjects ==>  Tomas  ||| subjects befor resolution ==>  i  ||| dialog number ==>  120_2\n",
      "\n",
      "\n",
      "events ==>  am sick  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  121_1\n",
      "\n",
      "\n",
      "events ==>  have fever  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  121_2\n",
      "\n",
      "\n",
      "events ==>  have interview  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  122_0\n",
      "\n",
      "\n",
      "events ==>  have surgery  ||| subjects ==>  mira  ||| subjects befor resolution ==>  mira  ||| dialog number ==>  123_2\n",
      "\n",
      "\n",
      "events ==>  make contract  ||| subjects ==>  Jim  ||| subjects befor resolution ==>  jim  ||| dialog number ==>  124_0\n",
      "\n",
      "\n",
      "events ==>  started job  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  125_1\n",
      "\n",
      "\n",
      "events ==>  got job  ||| subjects ==>  ezabela  ||| subjects befor resolution ==>  ezabela  ||| dialog number ==>  125_2\n",
      "\n",
      "\n",
      "events ==>  passed exam  ||| subjects ==>  mario  ||| subjects befor resolution ==>  mario  ||| dialog number ==>  126_1\n",
      "\n",
      "\n",
      "events ==>  finish masters  ||| subjects ==>  kareem  ||| subjects befor resolution ==>  his  ||| dialog number ==>  126_2\n",
      "\n",
      "\n",
      "events ==>  got exam  ||| subjects ==>  ezabela  ||| subjects befor resolution ==>  her  ||| dialog number ==>  126_3\n",
      "\n",
      "\n",
      "events ==>  our wedding  ||| subjects ==>  Bati husband,Bati  ||| subjects befor resolution ==>  our  ||| dialog number ==>  127_0\n",
      "\n",
      "\n",
      "events ==>  my wedding  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  my  ||| dialog number ==>  128_0\n",
      "\n",
      "\n",
      "events ==>  my birthday  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  my  ||| dialog number ==>  129_0\n",
      "\n",
      "\n",
      "events ==>  my anniversary  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  my  ||| dialog number ==>  130_0\n",
      "\n",
      "\n",
      "events ==>  moving into  ||| subjects ==>  bati  ||| subjects befor resolution ==>  she  ||| dialog number ==>  131_2\n",
      "\n",
      "\n",
      "events ==>  moving to  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  132_1\n",
      "\n",
      "\n",
      "events ==>  traveling to  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  133_0\n",
      "\n",
      "\n",
      "events ==>  moving to  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  134_1\n",
      "\n",
      "\n",
      "events ==>  moving on  ||| subjects ==>  mira  ||| subjects befor resolution ==>  she  ||| dialog number ==>  135_1\n",
      "\n",
      "\n",
      "events ==>  moving to  ||| subjects ==>  luke,Tomas  ||| subjects befor resolution ==>  luke, i  ||| dialog number ==>  136_0\n",
      "\n",
      "\n",
      "events ==>  moving to  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  137_0\n",
      "\n",
      "\n",
      "events ==>  died  ||| subjects ==>  Joy father  ||| subjects befor resolution ==>  joy father  ||| dialog number ==>  138_2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#getting events and subjects from labeled dataset\n",
    "def get_sub_event_id_from_labeledData(output):\n",
    "    events=[]\n",
    "    subjects=[]\n",
    "    dialog_id=[]\n",
    "    subjects_no_resolution=[]\n",
    "    for i in range(len(output)):\n",
    "        events.append(output[\"events\"][i])\n",
    "        subjects.append(output[\"subjects\"][i])\n",
    "        dialog_id.append(output[\"dialog_id\"][i])\n",
    "        subjects_no_resolution.append(output[\"subjects_no_resolution\"][i])\n",
    "    for i in range(len(events)):\n",
    "        print(\"events ==> \",events[i], \" ||| subjects ==> \",subjects[i],\" ||| subjects befor resolution ==> \",subjects_no_resolution[i],\" ||| dialog number ==> \",dialog_id[i])\n",
    "        print(\"\\n\")\n",
    "    return dialog_id,events,subjects,subjects_no_resolution\n",
    "\n",
    "dialog_id,events,subjects,subjects_no_resolution = get_sub_event_id_from_labeledData(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocessing on labeled dataset for easier matching with results\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing_on_labeledData(events,subjects,subjects_no_resolution):\n",
    "    events_preprocessed = []\n",
    "    subjects_preprocessed = []\n",
    "    subjects_no_resolution_preprocessed = []\n",
    "    for i, j in enumerate(events):\n",
    "\n",
    "        # sentence has more one events like 'passed exam,married' transformed to [passed exam,married]\n",
    "        if \",\" in str(events[i]):\n",
    "            b = []\n",
    "            c = events[i]\n",
    "            b.append(c[:c.find(\",\")])\n",
    "            b.append(c[c.find(\",\") + 1:])\n",
    "\n",
    "            events_preprocessed.append(b)\n",
    "        if \",\" not in str(events[i]):\n",
    "            events_preprocessed.append([events[i]])\n",
    "\n",
    "        # the same for subjects after resolution\n",
    "    for i, j in enumerate(subjects):\n",
    "        if \",\" in str(subjects[i]):\n",
    "            d = []\n",
    "            c = subjects[i]\n",
    "            d.append(c[:c.find(\",\")])\n",
    "            d.append(c[c.find(\",\") + 1:])\n",
    "\n",
    "            subjects_preprocessed.append(d)\n",
    "        if \",\" not in str(subjects[i]):\n",
    "            subjects_preprocessed.append([subjects[i]])\n",
    "\n",
    "        # the same for subjects befor resolution\n",
    "    for i, j in enumerate(subjects_no_resolution):\n",
    "        if \",\" in str(subjects_no_resolution[i]):\n",
    "            e = []\n",
    "            c = subjects_no_resolution[i]\n",
    "            e.append(c[:c.find(\",\")])\n",
    "            e.append(c[c.find(\",\") + 1:])\n",
    "\n",
    "            subjects_no_resolution_preprocessed.append(e)\n",
    "\n",
    "        if \",\" not in str(subjects_no_resolution[i]):\n",
    "            subjects_no_resolution_preprocessed.append([subjects_no_resolution[i]])\n",
    "    return events_preprocessed,subjects_preprocessed,subjects_no_resolution_preprocessed\n",
    "\n",
    "events_preprocessed,subjects_preprocessed,subjects_no_resolution_preprocessed = preprocessing_on_labeledData(events,subjects,subjects_no_resolution)\n",
    "\n",
    "\n",
    "\n",
    "# all this for subjects results after pronoun resolution\n",
    "\n",
    "# we and they resolution maybe get subjects like \"yara and hadia\" will transformed to ['yara','hadia']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_on_resultSubjects(subjects_results):\n",
    "    subjects_results_preprocessed = []\n",
    "    for i, j in enumerate(subjects_results):\n",
    "        if \" and \" in str(subjects_results[i]):\n",
    "            b = []\n",
    "            c = subjects_results[i][0]\n",
    "            b.append(c[:c.find(\" and \")])\n",
    "            b.append(c[c.find(\" and \") + 5:])\n",
    "            b.sort()\n",
    "            subjects_results_preprocessed.append(b)\n",
    "        else:\n",
    "            subjects_results_preprocessed.append(subjects_results[i])\n",
    "\n",
    "        # subjects like joy's father .. my results return it like [('joy','father')] so will transformed to ['joy father']\n",
    "        for k, l in enumerate(subjects_results[i]):\n",
    "            if isinstance(subjects_results[i][k], tuple) == True:\n",
    "                c=subjects_results[i][k]\n",
    "                \n",
    "                subjects_results_preprocessed[i][k] = str(c[0]) + \" \" + str(c[1])\n",
    "                \n",
    "            else:\n",
    "                subjects_results_preprocessed[i][k]=subjects_results_preprocessed[i][k]\n",
    "                \n",
    "    return subjects_results_preprocessed\n",
    "subjects_results_preprocessed = preprocessing_on_resultSubjects(subjects_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[susan], [they], [she], [she], [we], [your], [(my, husband), i], [your], [we], [i], [you], [you], [i], [you], [i], [(your, son)], [she], [she], [i], [i], [my], [you], [i], [he], [they], [frank], [i], [i], [i], [he], [i], [i], [i], [we], [your], [we], [my], [i], [i], [i], [i], [i], [we], [i], [i], [i], [our], [i], [we], [i], [(my, stomach), i], [he], [i], [i], [i], [you], [i], [you], [i], [i], [you], [i], [my], [you], [i], [i], [(my, daughter)], [me], [i], [i], ['Margaret', matt], [nathan, 'Kriss'], [tomas], [george, 'Mari'], [you], [i], [mario], [i], ['Jhon'], [tom], [i], [i], [bati], [we], [i], [i], [i], ['karla son'], [i], [she], [i], [tom], [tom], [you], [i], [(my, wife), i], [i], [i], [i], [i], [she], [you], [i], [i], [i], [i], [i], [i], [we], [dani], [i], [you], [tony], ['karolina son'], [i], [i], [i], [i], [i], [mira], [jim], [i], [ezabela], [mario], [his], [her], [our], [my], [my], [my], [she], [i], [i], [she], [luke, 'Tomas'], [i], ['joy father']]\n"
     ]
    }
   ],
   "source": [
    "def get_subject_result_befor_resolution_withoutID(subject_result_befor_resolution):\n",
    "    subject_result_befor_resolution_withoutID = []\n",
    "    for i in subject_result_befor_resolution:\n",
    "        b = i[0]\n",
    "        subject_result_befor_resolution_withoutID.append(b)\n",
    "    return subject_result_befor_resolution_withoutID\n",
    "\n",
    "subject_result_befor_resolution_withoutID = get_subject_result_befor_resolution_withoutID(subject_result_befor_resolution)\n",
    "\n",
    "\n",
    "print(subject_result_befor_resolution_withoutID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_on_subject_result_befor_resolution_withoutID(subject_result_befor_resolution_withoutID):\n",
    "    for i,j in enumerate(subject_result_befor_resolution_withoutID):        \n",
    "        for k,l in enumerate(subject_result_befor_resolution_withoutID[i]):\n",
    "            if k<=len(subject_result_befor_resolution_withoutID[i])   :\n",
    "\n",
    "                if isinstance(l,tuple)==True:\n",
    "\n",
    "                    subject_result_befor_resolution_withoutID[i][k]=str(l[0])+\" \"+str(l[1])\n",
    "                else:  \n",
    "                    subject_result_befor_resolution_withoutID[i][k]=subject_result_befor_resolution_withoutID[i][k]\n",
    "\n",
    "                \n",
    "    return subject_result_befor_resolution_withoutID\n",
    "\n",
    "\n",
    "subject_result_befor_resolution_withoutID = preprocessing_on_subject_result_befor_resolution_withoutID(subject_result_befor_resolution_withoutID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[susan], [they], [she], [she], [we], [your], ['my husband', i], [your], [we], [i], [you], [you], [i], [you], [i], ['your son'], [she], [she], [i], [i], [my], [you], [i], [he], [they], [frank], [i], [i], [i], [he], [i], [i], [i], [we], [your], [we], [my], [i], [i], [i], [i], [i], [we], [i], [i], [i], [our], [i], [we], [i], ['my stomach', i], [he], [i], [i], [i], [you], [i], [you], [i], [i], [you], [i], [my], [you], [i], [i], ['my daughter'], [me], [i], [i], ['Margaret', matt], [nathan, 'Kriss'], [tomas], [george, 'Mari'], [you], [i], [mario], [i], ['Jhon'], [tom], [i], [i], [bati], [we], [i], [i], [i], ['karla son'], [i], [she], [i], [tom], [tom], [you], [i], ['my wife', i], [i], [i], [i], [i], [she], [you], [i], [i], [i], [i], [i], [i], [we], [dani], [i], [you], [tony], ['karolina son'], [i], [i], [i], [i], [i], [mira], [jim], [i], [ezabela], [mario], [his], [her], [our], [my], [my], [my], [she], [i], [i], [she], [luke, 'Tomas'], [i], ['joy father']]\n"
     ]
    }
   ],
   "source": [
    "print(subject_result_befor_resolution_withoutID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill lists with None values with the same labeled data long\n",
    "def fill_with_null(dialog_id):\n",
    "    null_subject=[]\n",
    "    null_subject_befor_resolution=[]\n",
    "    null_events=[]\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        null_subject.append([['None'],dialog_id[i]])\n",
    "        null_events.append([['None'],dialog_id[i]])\n",
    "        null_subject_befor_resolution.append([['None'],dialog_id[i]])\n",
    "    return null_subject,null_events,null_subject_befor_resolution\n",
    "\n",
    "null_subject,null_events,null_subject_befor_resolution = fill_with_null(dialog_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[susan], '1_0'], [['None'], '2'], [[' Mari', 'george'], '3_1'], [['Mari'], '4_1'], [['She'], '4_3'], [['None'], '5'], [['Abigail', 'Jhon'], '6_0'], [['Abigail'], '7_0'], [['Abigail husband', 'Abigail'], '7_1'], [['Abigail'], '8_0'], [['None'], '9_2'], [['Jhon', 'Mark'], '9_3'], [['None'], '10'], [['None'], '11'], [['Jhon'], '12_1'], [['Jhon'], '13_0'], [['Jhon'], '14_0'], [['Jhon'], '14_1'], [['None'], '15'], [['Abigail'], '16_1'], [['Abigail'], '17_1'], [['Margaret son'], '18_0'], [['Pamela'], '19_1'], [['Pamela'], '19_2'], [['None'], '20'], [['Mark'], '21_2'], [['Mark'], '22_0'], [['Mark'], '22_2'], [['Margaret'], '23_0'], [['Abigail'], '24_2'], [['george'], '25_1'], [[' Abigail', 'george'], '26_1'], [[frank], '27_0'], [['Mark'], '28_1'], [['Mark'], '29_0'], [['Abigail'], '30_0'], [['george'], '31_1'], [['Mark'], '32_0'], [['None'], '33'], [['Jhon'], '34_1'], [['Margaret'], '35_1'], [['Abigail', 'Margaret'], '36_0'], [['None'], '36_1'], [['Mark'], '37_0'], [[''], '37_1'], [['Mark'], '38_0'], [['Mark'], '38_1'], [['george'], '39_0'], [['None'], '40_0'], [['Jhon'], '40_2'], [['Jhon'], '41_0'], [['Mark'], '42_1'], [['Jhon', 'Mark'], '43_1'], [['george'], '44_1'], [['Mari'], '45_1'], [['Jhon'], '46_0'], [['Mari', 'george'], '47_2'], [['george'], '48_2'], [['Kriss', 'george'], '49_1'], [['Kriss'], '50_0'], [['None'], '51_0'], [['george stomach', 'george'], '52_1'], [['Jhon'], '53_1'], [['Mark'], '54_0'], [['None'], '54_1'], [['Kriss'], '55_1'], [['Mark'], '56_1'], [['george'], '57_0'], [['None'], '58'], [['george'], '59_2'], [['Abigail'], '60_0'], [['None'], '61_1'], [['george'], '62_0'], [['None'], '63'], [['Wilson'], '64_1'], [['george'], '65_1'], [['Tina'], '66_2'], [['None'], '67'], [['None'], '68_0'], [['george'], '69_0'], [['george'], '70_0'], [['Kriss'], '71_0'], [['Mark'], '72_1'], [['george daughter'], '73_1'], [['Kriss'], '74_0'], [['Jhon'], '75_2'], [['Margaret'], '76_0'], [['Margaret', matt], '77_0'], [[nathan, 'Kriss'], '78_0'], [[tomas], '79_2'], [['None'], '80_0'], [[george, 'Mari'], '80_1'], [['Jhon'], '81_0'], [['Jhon'], '82_2'], [[mario], '83_2'], [['Mari'], '84_1'], [['Jhon'], '85_1'], [[tom], '86_1'], [['Margaret'], '87_0'], [['Mark'], '88_0'], [[bati], '89_1'], [['Mark', 'george'], '90_2'], [['Mari'], '91_1'], [['Bati'], '92_2'], [['Tina'], '93_1'], [['karla son'], '94_2'], [['None'], '95_1'], [['George'], '96_1'], [[' Tina sister'], '97_1'], [['Jhon'], '98_1'], [[tom], '99_2'], [[tom], '100_2'], [['Mark'], '101_0'], [['Mark'], '101_1'], [['Jhon wife', 'Jhon'], '102_1'], [['Jhon'], '103_1'], [['Tina'], '104_0'], [['Mark'], '105_1'], [['george'], '106_0'], [['Mari'], '107_2'], [['george'], '108_0'], [['george'], '108_1'], [['Mark'], '109_1'], [['Tom'], '110_0'], [['Kriss'], '111_1'], [['Frank'], '112_0'], [['Tomas'], '113_0'], [['Jhon', 'Mark'], '114_0'], [[dani], '114_2'], [['Kriss'], '115_2'], [['Tina'], '116_0'], [['None'], '116_1'], [[tony], '117_1'], [['karolina son'], '118_1'], [['Tina'], '119_0'], [['Tomas'], '120_2'], [['Tina'], '121_1'], [['Tina'], '121_2'], [['Jhon'], '122_0'], [[mira], '123_2'], [[jim], '124_0'], [['Kriss'], '125_1'], [[ezabela], '125_2'], [[mario], '126_1'], [['kareem'], '126_2'], [['ezabela'], '126_3'], [[' Bati husband', 'Bati'], '127_0'], [['Kriss'], '128_0'], [['Kriss'], '129_0'], [['Kriss'], '130_0'], [['bati'], '131_2'], [['Mark'], '132_1'], [['None'], '133_0'], [['Kriss'], '134_1'], [['mira'], '135_1'], [[luke, 'Tomas'], '136_0'], [['Mark'], '137_0'], [['joy father'], '138_2']]\n"
     ]
    }
   ],
   "source": [
    "def fill_null_with_value_of_subjects_results(null_subject,subjects_results_preprocessing):\n",
    "\n",
    "    for i,j in enumerate(null_subject):\n",
    "        for k,l in enumerate(subjects_results_preprocessing):\n",
    "            if str(dialog_id_results[k]).strip() == str(null_subject[i][1]).strip():\n",
    "                null_subject[i][0]=subjects_results_preprocessing[k]\n",
    "                null_events[i][0]=events_results[k]\n",
    "                null_subject_befor_resolution[i][0]=subject_result_befor_resolution_withoutID[k]\n",
    "    return null_subject,null_events,null_subject_befor_resolution\n",
    "final_subjects_res_preprocessing,final_events_res_preprocessing,final_subject_befor_resolution_res_preprocessing = fill_null_with_value_of_subjects_results(null_subject,subjects_results_preprocessed)\n",
    "print(final_subjects_res_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating the 3 steps ( events extraction ,subject extraction , pronoun resolution )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compar null_subject with subjects_ from labeled data\n",
    "#compar null_event with events_ from labeled data\n",
    "\n",
    "def evaluation(dialog_id=[],final_subjects_res_preprocessing=[],subjects_preprocessed=[]):\n",
    "\n",
    "    count_true_negative=0\n",
    "    count_true_positive=0\n",
    "    count_false_positive=0\n",
    "    count_false_negative=0\n",
    "    a_list=set()\n",
    "    b_list=set()\n",
    "    c_list=set()\n",
    "    d_list=set()\n",
    "    o_list=set()\n",
    "\n",
    "    count_other=0\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                    for c,m in enumerate(final_subjects_res_preprocessing[i][0]):\n",
    "                        if str(subjects_preprocessed[i][k]).lower().strip()==str(final_subjects_res_preprocessing[i][0][c]).lower().strip() and str(subjects_preprocessed[i][k]).lower().strip()==\"none\" and str(final_subjects_res_preprocessing[i][0][c]).lower().strip()==\"none\":\n",
    "                            a_list.add(dialog_id[i])\n",
    "                            count_true_negative+=1\n",
    "\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if dialog_id[i] not in a_list:\n",
    "            if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                    for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                        for c,m in enumerate(final_subjects_res_preprocessing[i][0]):                        \n",
    "                            if str(subjects_preprocessed[i][k]).lower().strip()==str(final_subjects_res_preprocessing[i][0][c]).lower().strip() and str(subjects_preprocessed[i][k]).lower().strip()!=\"none\" and str(final_subjects_res_preprocessing[i][0][c]).lower().strip()!=\"none\":\n",
    "                                count_true_positive+=1\n",
    "                                b_list.add(dialog_id[i])\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if dialog_id[i] not in a_list and dialog_id[i] not in b_list:\n",
    "            if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                    for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                        for c,m in enumerate(final_subjects_res_preprocessing[i][0]):                    \n",
    "                            if str(subjects_preprocessed[i][k]).lower().strip()!=str(final_subjects_res_preprocessing[i][0][c]).lower().strip() and str(subjects_preprocessed[i][k]).lower().strip()==\"none\" and str(final_subjects_res_preprocessing[i][0][c]).lower().strip()!=\"none\":\n",
    "                                count_false_positive+=1\n",
    "                                c_list.add(dialog_id[i])\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if dialog_id[i] not in a_list and dialog_id[i] not in b_list and dialog_id[i] not in c_list:\n",
    "            if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                    for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                        for c,m in enumerate(final_subjects_res_preprocessing[i][0]):                        \n",
    "                            if str(subjects_preprocessed[i][k]).lower().strip()!=str(final_subjects_res_preprocessing[i][0][c]).lower().strip() and str(subjects_preprocessed[i][k]).lower().strip()!=\"none\" and str(final_subjects_res_preprocessing[i][0][c]).lower().strip()==\"none\":\n",
    "                                count_false_negative+=1\n",
    "                                d_list.add(dialog_id[i])\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if dialog_id[i] not in a_list and dialog_id[i] not in b_list and dialog_id[i] not in c_list and dialog_id[i] not in d_list:\n",
    "            if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                    for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                        for c,m in enumerate(final_subjects_res_preprocessing[i][0]):                            \n",
    "                            if (str(subjects_preprocessed[i][k]).lower().strip()!=str(final_subjects_res_preprocessing[i][0][c]).lower().strip()) and (str(subjects_preprocessed[i][k]).lower().strip()!=\"none\") and (str(final_subjects_res_preprocessing[i][0][c]).lower().strip()!=\"none\"):\n",
    "                                count_other+=1\n",
    "                                o_list.add(dialog_id[i])\n",
    "                    \n",
    "    \n",
    "    \n",
    "    return a_list,b_list,c_list,d_list,o_list\n",
    "    \n",
    "#compare between subjects_ which is the labeled and preprocessed subject in data  .. with subject results after fill empty results \n",
    "#with Null value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_negative for subjects with pronoun resolution  :  17\n",
      "true_positive for subjects with pronoun resolution  :  127\n",
      "false_positive for subjects with pronoun resolution  :  10\n",
      "false_negative for subjects with pronoun resolution  :  4\n",
      "Precision  0.927007299270073\n",
      "Recall  0.9694656488549618\n"
     ]
    }
   ],
   "source": [
    "a_list,b_list,c_list,d_list,o_list=evaluation(dialog_id,final_subjects_res_preprocessing,subjects_preprocessed)\n",
    "\n",
    "print(\"true_negative for subjects with pronoun resolution  : \",len(a_list))\n",
    "print(\"true_positive for subjects with pronoun resolution  : \",len(b_list))\n",
    "print(\"false_positive for subjects with pronoun resolution  : \",len(c_list)+len(o_list))\n",
    "print(\"false_negative for subjects with pronoun resolution  : \",len(d_list))\n",
    "    \n",
    "#Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "#Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "print(\"Precision \",(float)(len(b_list)/(float)((len(b_list))+(len(c_list)+len(o_list)))))\n",
    "print(\"Recall \",(float)(len(b_list)/(float)(len(b_list)+len(d_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_negative for subjects befor pronoun resolution  :  17\n",
      "true_positive for subjects befor pronoun resolution  :  129\n",
      "false_positive for subjects befor pronoun resolution  :  8\n",
      "false_negative for subjects befor pronoun resolution  :  4\n",
      "Precision  0.9416058394160584\n",
      "Recall  0.9699248120300752\n"
     ]
    }
   ],
   "source": [
    "a_list,b_list,c_list,d_list,o_list=evaluation(dialog_id,final_subject_befor_resolution_res_preprocessing,subjects_no_resolution_preprocessed)\n",
    "\n",
    "print(\"true_negative for subjects befor pronoun resolution  : \",len(a_list))\n",
    "print(\"true_positive for subjects befor pronoun resolution  : \",len(b_list))\n",
    "print(\"false_positive for subjects befor pronoun resolution  : \",len(c_list)+len(o_list))\n",
    "print(\"false_negative for subjects befor pronoun resolution  : \",len(d_list))\n",
    "\n",
    "#Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "#Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "print(\"Precision \",(float)(len(b_list)/(float)((len(b_list))+(len(c_list)+len(o_list)))))\n",
    "print(\"Recall \",(float)(len(b_list)/(float)(len(b_list)+len(d_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_negative for events  :  17\n",
      "true_positive for events  :  132\n",
      "false_positive for events  :  5\n",
      "false_negative for events  :  4\n",
      "Precision  0.9635036496350365\n",
      "Recall  0.9705882352941176\n"
     ]
    }
   ],
   "source": [
    "a_list,b_list,c_list,d_list,o_list=evaluation(dialog_id,final_events_res_preprocessing,events_preprocessed)\n",
    "\n",
    "print(\"true_negative for events  : \",len(a_list))\n",
    "print(\"true_positive for events  : \",len(b_list))\n",
    "print(\"false_positive for events  : \",len(c_list)+len(o_list))\n",
    "print(\"false_negative for events  : \",len(d_list))\n",
    "    \n",
    "#Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "#Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "print(\"Precision \",(float)(len(b_list)/(float)((len(b_list))+(len(c_list)+len(o_list)))))\n",
    "print(\"Recall \",(float)(len(b_list)/(float)(len(b_list)+len(d_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some testing example on finding subjects functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[we]\n",
      "[(my, stomach)]\n",
      "None\n",
      "[(karla, sun)]\n",
      "[(my, brother)]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"my husband's parents flew out to meet my family when we got married in my hometown , so that was great \"\n",
    "),\"married\"))\n",
    "print(find_sub_for_verb(nlp(\"i've got a really bad headache and my stomach hurts\"),\"hurts\"))\n",
    "\n",
    "print(find_sub_for_verb(nlp(\"people divorced every year\"),\"divorced\"))\n",
    "print(find_sub_for_verb(nlp(\"karla's sun have an exam\"),\"have\"))\n",
    "print(find_sub_for_verb(nlp(\"my brother have an exam\"),\"have\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sam, george]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"sam and george have an exam\"),\"have\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(find_sub_for_verb(nlp(\"i'm interviewed on montana public radio today about my life\"),\"interviewed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[she, george, sam]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"i heard she and george and sam has passed the bar exam and married recently \"),\"married\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(find_sub_for_verb(nlp(\"george went for a job interview earlier and got the job \"),\"got\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_actionEvents(nlp(\"george went for a job interview earlier and got the job\"),verb_with_prepo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"george went for a job interview earlier and got the job\"),nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, wife), i]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"today my wife and i celebrate our 1st wedding anniversary \"),\"celebrate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[she]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"i heard she has passed the bar exam and married recently \"),\"passed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(joy, father)]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"joy's father died and he is very depressed\"),\"died\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Marla]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"Marla will have an surgery next week \"),\"have\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nathan, i]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"nathan and i married last week \"),\"married\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tomas]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"tomas divorced katty and take her alone .\"),\"divorced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[you, me]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"you and me will married this month .\"),\"married\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Marla]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"Marla will have an surgery next week \"),nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, stomach), george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"i've got a really bad headache and my stomach and george hurts\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[my]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"(you are invited to my wedding\"),nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(your, son)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"i heard your son recently graduated\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, family)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"my husband flew out to meet my family when we got married in my hometown \"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, thesis)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"defending my thesis was great\"),verbs_with_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(karla, sun)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"karla's sun passed the exam in 1999\"),verbs_with_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[you]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"do you want to go out to celebrate my good news\"),verbs_with_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george, i]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"should probably let u all know that george and i are now engaged\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, paper), george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"my paper and george was published in 1999\"),verbs_with_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[me]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"the studio interviewed me last year\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, boss)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"the studio interviewed my boss last year\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[my]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_direct_relation(nlp(\"my wedding was last week\"),noun_direct_relation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_adjectiveEvents(nlp(\"i'm very sick and i have a fever\"),adjectives))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"george will have an surgery next week \"),nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[his]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"kareem get to finish his masters \"),nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kareem]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"kareem get to finish masters \"),nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[we]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_actionEvents(nlp(\"we are moving to palmera last week\"),verb_with_prepo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kareem]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"kareem get to finish masters \"),nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
