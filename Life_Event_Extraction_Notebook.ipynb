{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0731 19:59:59.763079 28844 plugins.py:96] Plugin allennlp_models available\n",
      "I0731 19:59:59.788581 28844 archival.py:211] loading archive file C:/Users/Windows dunya/Desktop/fourth year/5th/I17-1099.Datasets/EMNLP_dataset/coref-spanbert-large-2021.03.10(3).tar.gz from cache at C:\\Users\\Windows dunya\\Desktop\\fourth year\\5th\\I17-1099.Datasets\\EMNLP_dataset\\coref-spanbert-large-2021.03.10(3).tar.gz\n",
      "I0731 19:59:59.795759 28844 archival.py:300] extracting archive file C:\\Users\\Windows dunya\\Desktop\\fourth year\\5th\\I17-1099.Datasets\\EMNLP_dataset\\coref-spanbert-large-2021.03.10(3).tar.gz to temp dir C:\\Users\\WINDOW~1\\AppData\\Local\\Temp\\tmpykki_15k\n",
      "W0731 20:00:12.973821 28844 params.py:20] error loading _jsonnet (this is expected on Windows), treating C:\\Users\\WINDOW~1\\AppData\\Local\\Temp\\tmpykki_15k\\config.json as plain json\n",
      "I0731 20:00:12.984021 28844 params.py:221] dataset_reader.type = coref\n",
      "I0731 20:00:12.985994 28844 params.py:221] dataset_reader.max_instances = None\n",
      "I0731 20:00:12.986993 28844 params.py:221] dataset_reader.manual_distributed_sharding = False\n",
      "I0731 20:00:12.987996 28844 params.py:221] dataset_reader.manual_multiprocess_sharding = False\n",
      "I0731 20:00:12.987996 28844 params.py:221] dataset_reader.max_span_width = 30\n",
      "I0731 20:00:12.989056 28844 params.py:221] dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0731 20:00:12.990994 28844 params.py:221] dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0731 20:00:12.991433 28844 params.py:221] dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0731 20:00:12.991993 28844 params.py:221] dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0731 20:00:12.992992 28844 params.py:221] dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0731 20:00:12.994039 28844 params.py:221] dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "I0731 20:00:13.006026 28844 params.py:221] dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0731 20:00:13.006469 28844 params.py:221] dataset_reader.max_sentences = 110\n",
      "I0731 20:00:13.006992 28844 params.py:221] dataset_reader.remove_singleton_clusters = False\n",
      "I0731 20:00:13.007993 28844 params.py:221] validation_dataset_reader.type = coref\n",
      "I0731 20:00:13.009260 28844 params.py:221] validation_dataset_reader.max_instances = None\n",
      "I0731 20:00:13.010022 28844 params.py:221] validation_dataset_reader.manual_distributed_sharding = False\n",
      "I0731 20:00:13.011021 28844 params.py:221] validation_dataset_reader.manual_multiprocess_sharding = False\n",
      "I0731 20:00:13.011993 28844 params.py:221] validation_dataset_reader.max_span_width = 30\n",
      "I0731 20:00:13.011993 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "I0731 20:00:13.013996 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0731 20:00:13.013996 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0731 20:00:13.014998 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "I0731 20:00:13.016555 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "I0731 20:00:13.017423 28844 params.py:221] validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "I0731 20:00:13.020996 28844 params.py:221] validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "I0731 20:00:13.022043 28844 params.py:221] validation_dataset_reader.max_sentences = None\n",
      "I0731 20:00:13.022996 28844 params.py:221] validation_dataset_reader.remove_singleton_clusters = False\n",
      "I0731 20:00:13.023994 28844 params.py:221] type = from_instances\n",
      "I0731 20:00:13.025118 28844 vocabulary.py:349] Loading token dictionary from C:\\Users\\WINDOW~1\\AppData\\Local\\Temp\\tmpykki_15k\\vocabulary.\n",
      "I0731 20:00:13.035027 28844 params.py:221] model.type = coref\n",
      "I0731 20:00:13.036993 28844 params.py:221] model.regularizer = None\n",
      "I0731 20:00:13.036993 28844 params.py:221] model.ddp_accelerator = None\n",
      "I0731 20:00:13.037992 28844 params.py:221] model.text_field_embedder.type = basic\n",
      "I0731 20:00:13.038992 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "I0731 20:00:13.040142 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "I0731 20:00:13.041992 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "I0731 20:00:13.043040 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.sub_module = None\n",
      "I0731 20:00:13.043040 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
      "I0731 20:00:13.043997 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
      "I0731 20:00:13.044996 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
      "I0731 20:00:13.044996 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
      "I0731 20:00:13.045995 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
      "I0731 20:00:13.046994 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
      "I0731 20:00:13.048148 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
      "I0731 20:00:13.049001 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
      "I0731 20:00:13.049001 28844 params.py:221] model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
      "I0731 20:00:17.947052 28844 params.py:221] model.context_layer.type = pass_through\n",
      "I0731 20:00:17.947377 28844 params.py:221] model.context_layer.input_dim = 1024\n",
      "I0731 20:00:17.949461 28844 params.py:221] model.mention_feedforward.input_dim = 3092\n",
      "I0731 20:00:17.949620 28844 params.py:221] model.mention_feedforward.num_layers = 2\n",
      "I0731 20:00:17.950453 28844 params.py:221] model.mention_feedforward.hidden_dims = 1500\n",
      "I0731 20:00:17.950453 28844 params.py:221] model.mention_feedforward.activations = relu\n",
      "I0731 20:00:17.951452 28844 params.py:221] type = relu\n",
      "I0731 20:00:17.952692 28844 params.py:221] model.mention_feedforward.dropout = 0.3\n",
      "I0731 20:00:18.009455 28844 params.py:221] model.antecedent_feedforward.input_dim = 9296\n",
      "I0731 20:00:18.010515 28844 params.py:221] model.antecedent_feedforward.num_layers = 2\n",
      "I0731 20:00:18.011453 28844 params.py:221] model.antecedent_feedforward.hidden_dims = 1500\n",
      "I0731 20:00:18.012453 28844 params.py:221] model.antecedent_feedforward.activations = relu\n",
      "I0731 20:00:18.013453 28844 params.py:221] type = relu\n",
      "I0731 20:00:18.014454 28844 params.py:221] model.antecedent_feedforward.dropout = 0.3\n",
      "I0731 20:00:18.136775 28844 params.py:221] model.feature_size = 20\n",
      "I0731 20:00:18.137454 28844 params.py:221] model.max_span_width = 30\n",
      "I0731 20:00:18.138453 28844 params.py:221] model.spans_per_word = 0.4\n",
      "I0731 20:00:18.138540 28844 params.py:221] model.max_antecedents = 50\n",
      "I0731 20:00:18.139453 28844 params.py:221] model.coarse_to_fine = True\n",
      "I0731 20:00:18.141639 28844 params.py:221] model.inference_order = 2\n",
      "I0731 20:00:18.142453 28844 params.py:221] model.lexical_dropout = 0.2\n",
      "I0731 20:00:18.142453 28844 params.py:221] model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x000001670C72A8D0>\n",
      "I0731 20:00:18.239861 28844 initializers.py:482] Initializing parameters\n",
      "I0731 20:00:18.245347 28844 initializers.py:501] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0731 20:00:18.247437 28844 initializers.py:507]    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "I0731 20:00:18.248465 28844 initializers.py:507]    _antecedent_feedforward._module._linear_layers.0.weight\n",
      "I0731 20:00:18.249346 28844 initializers.py:507]    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "I0731 20:00:18.249346 28844 initializers.py:507]    _antecedent_feedforward._module._linear_layers.1.weight\n",
      "I0731 20:00:18.250348 28844 initializers.py:507]    _antecedent_scorer._module.bias\n",
      "I0731 20:00:18.250348 28844 initializers.py:507]    _antecedent_scorer._module.weight\n",
      "I0731 20:00:18.251347 28844 initializers.py:507]    _attentive_span_extractor._global_attention._module.bias\n",
      "I0731 20:00:18.251347 28844 initializers.py:507]    _attentive_span_extractor._global_attention._module.weight\n",
      "I0731 20:00:18.252346 28844 initializers.py:507]    _coarse2fine_scorer.bias\n",
      "I0731 20:00:18.252346 28844 initializers.py:507]    _coarse2fine_scorer.weight\n",
      "I0731 20:00:18.253347 28844 initializers.py:507]    _distance_embedding.weight\n",
      "I0731 20:00:18.254346 28844 initializers.py:507]    _endpoint_span_extractor._span_width_embedding.weight\n",
      "I0731 20:00:18.254346 28844 initializers.py:507]    _mention_feedforward._module._linear_layers.0.bias\n",
      "I0731 20:00:18.255347 28844 initializers.py:507]    _mention_feedforward._module._linear_layers.0.weight\n",
      "I0731 20:00:18.255347 28844 initializers.py:507]    _mention_feedforward._module._linear_layers.1.bias\n",
      "I0731 20:00:18.256346 28844 initializers.py:507]    _mention_feedforward._module._linear_layers.1.weight\n",
      "I0731 20:00:18.256346 28844 initializers.py:507]    _mention_scorer._module.bias\n",
      "I0731 20:00:18.257346 28844 initializers.py:507]    _mention_scorer._module.weight\n",
      "I0731 20:00:18.257346 28844 initializers.py:507]    _span_updating_gated_sum._gate.bias\n",
      "I0731 20:00:18.258614 28844 initializers.py:507]    _span_updating_gated_sum._gate.weight\n",
      "I0731 20:00:18.258614 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "I0731 20:00:18.259349 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "I0731 20:00:18.260361 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "I0731 20:00:18.261346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "I0731 20:00:18.261346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "I0731 20:00:18.262382 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.263347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.263347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "I0731 20:00:18.264346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "I0731 20:00:18.265348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "I0731 20:00:18.266349 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "I0731 20:00:18.267349 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "I0731 20:00:18.268347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "I0731 20:00:18.268347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "I0731 20:00:18.269670 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "I0731 20:00:18.270347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "I0731 20:00:18.270347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "I0731 20:00:18.271354 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "I0731 20:00:18.272348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "I0731 20:00:18.273346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "I0731 20:00:18.273346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "I0731 20:00:18.274347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.275348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.276351 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "I0731 20:00:18.277347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "I0731 20:00:18.278347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "I0731 20:00:18.279347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "I0731 20:00:18.279347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "I0731 20:00:18.280348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "I0731 20:00:18.281347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "I0731 20:00:18.282348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "I0731 20:00:18.282348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "I0731 20:00:18.283999 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "I0731 20:00:18.284347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "I0731 20:00:18.285347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "I0731 20:00:18.286346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "I0731 20:00:18.287348 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "I0731 20:00:18.287810 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.288346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.289346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "I0731 20:00:18.289346 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "I0731 20:00:18.290347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "I0731 20:00:18.291347 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "I0731 20:00:18.291593 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "I0731 20:00:18.292537 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "I0731 20:00:18.293402 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "I0731 20:00:18.294403 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "I0731 20:00:18.295405 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "I0731 20:00:18.295405 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "I0731 20:00:18.296530 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "I0731 20:00:18.297402 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "I0731 20:00:18.298008 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "I0731 20:00:18.298402 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "I0731 20:00:18.299517 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.300403 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.300784 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "I0731 20:00:18.301892 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "I0731 20:00:18.301892 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "I0731 20:00:18.302931 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "I0731 20:00:18.302931 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "I0731 20:00:18.304155 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "I0731 20:00:18.304855 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "I0731 20:00:18.305376 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "I0731 20:00:18.305853 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "I0731 20:00:18.306562 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "I0731 20:00:18.306853 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "I0731 20:00:18.307713 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "I0731 20:00:18.307854 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "I0731 20:00:18.309114 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "I0731 20:00:18.309855 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.310919 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.312108 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "I0731 20:00:18.313109 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "I0731 20:00:18.314109 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "I0731 20:00:18.315109 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "I0731 20:00:18.315109 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "I0731 20:00:18.316231 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "I0731 20:00:18.317501 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "I0731 20:00:18.318810 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "I0731 20:00:18.319363 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "I0731 20:00:18.320218 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "I0731 20:00:18.320363 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "I0731 20:00:18.321362 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "I0731 20:00:18.321362 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "I0731 20:00:18.322393 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "I0731 20:00:18.323365 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.323798 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.324364 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "I0731 20:00:18.324364 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "I0731 20:00:18.326190 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "I0731 20:00:18.327617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "I0731 20:00:18.328619 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "I0731 20:00:18.329618 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "I0731 20:00:18.330617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
      "I0731 20:00:18.330617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "I0731 20:00:18.331617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "I0731 20:00:18.331617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "I0731 20:00:18.332751 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "I0731 20:00:18.333618 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "I0731 20:00:18.334017 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "I0731 20:00:18.334522 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "I0731 20:00:18.335578 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.336424 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.336529 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "I0731 20:00:18.337529 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "I0731 20:00:18.337529 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "I0731 20:00:18.338528 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "I0731 20:00:18.339620 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "I0731 20:00:18.339620 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "I0731 20:00:18.340718 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
      "I0731 20:00:18.340718 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "I0731 20:00:18.342051 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "I0731 20:00:18.342734 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "I0731 20:00:18.344582 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "I0731 20:00:18.344582 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "I0731 20:00:18.345582 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "I0731 20:00:18.346581 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "I0731 20:00:18.346900 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.347903 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.348067 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "I0731 20:00:18.348902 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "I0731 20:00:18.349900 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "I0731 20:00:18.350693 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "I0731 20:00:18.350903 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "I0731 20:00:18.352165 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "I0731 20:00:18.352903 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "I0731 20:00:18.352903 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "I0731 20:00:18.354045 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "I0731 20:00:18.354902 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "I0731 20:00:18.355268 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "I0731 20:00:18.355902 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "I0731 20:00:18.356902 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "I0731 20:00:18.357832 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "I0731 20:00:18.357859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.358862 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.360152 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "I0731 20:00:18.361469 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "I0731 20:00:18.361859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "I0731 20:00:18.362859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "I0731 20:00:18.362859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
      "I0731 20:00:18.363859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "I0731 20:00:18.363859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "I0731 20:00:18.365003 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "I0731 20:00:18.365875 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "I0731 20:00:18.367141 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "I0731 20:00:18.367860 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "I0731 20:00:18.368859 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "I0731 20:00:18.369860 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "I0731 20:00:18.370860 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "I0731 20:00:18.371860 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.372146 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.373148 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "I0731 20:00:18.374148 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "I0731 20:00:18.375774 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "I0731 20:00:18.376252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "I0731 20:00:18.377473 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
      "I0731 20:00:18.378253 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "I0731 20:00:18.379251 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "I0731 20:00:18.379251 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "I0731 20:00:18.380252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "I0731 20:00:18.381252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "I0731 20:00:18.382319 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "I0731 20:00:18.382319 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "I0731 20:00:18.383499 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "I0731 20:00:18.384253 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "I0731 20:00:18.385251 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.386280 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.387252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "I0731 20:00:18.387730 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "I0731 20:00:18.388590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "I0731 20:00:18.389250 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "I0731 20:00:18.390026 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "I0731 20:00:18.390252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "I0731 20:00:18.391307 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "I0731 20:00:18.391307 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "I0731 20:00:18.392252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "I0731 20:00:18.393251 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "I0731 20:00:18.394258 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "I0731 20:00:18.395252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "I0731 20:00:18.395252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "I0731 20:00:18.396282 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "I0731 20:00:18.397252 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.398394 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.399333 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "I0731 20:00:18.399333 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "I0731 20:00:18.400609 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
      "I0731 20:00:18.400609 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "I0731 20:00:18.401398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "I0731 20:00:18.402397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "I0731 20:00:18.402397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "I0731 20:00:18.403397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "I0731 20:00:18.403397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "I0731 20:00:18.404410 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "I0731 20:00:18.404410 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "I0731 20:00:18.405617 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "I0731 20:00:18.406397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "I0731 20:00:18.406707 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "I0731 20:00:18.407397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.407709 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.408397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "I0731 20:00:18.409400 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "I0731 20:00:18.409400 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "I0731 20:00:18.410506 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "I0731 20:00:18.411397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "I0731 20:00:18.411693 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "I0731 20:00:18.412399 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "I0731 20:00:18.413398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "I0731 20:00:18.413990 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "I0731 20:00:18.414397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "I0731 20:00:18.414999 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "I0731 20:00:18.415397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "I0731 20:00:18.416174 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "I0731 20:00:18.416397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "I0731 20:00:18.417397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.417397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.418397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "I0731 20:00:18.418661 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "I0731 20:00:18.419397 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "I0731 20:00:18.420399 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "I0731 20:00:18.420399 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "I0731 20:00:18.421482 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "I0731 20:00:18.422398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "I0731 20:00:18.422999 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "I0731 20:00:18.423398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "I0731 20:00:18.424398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "I0731 20:00:18.425398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "I0731 20:00:18.425398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "I0731 20:00:18.426398 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "I0731 20:00:18.426878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "I0731 20:00:18.427880 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.428881 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.429175 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
      "I0731 20:00:18.429878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "I0731 20:00:18.430878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "I0731 20:00:18.431314 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "I0731 20:00:18.432342 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "I0731 20:00:18.432881 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "I0731 20:00:18.433574 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "I0731 20:00:18.433878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "I0731 20:00:18.434583 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "I0731 20:00:18.434583 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "I0731 20:00:18.435818 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "I0731 20:00:18.436592 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "I0731 20:00:18.436878 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "I0731 20:00:18.437591 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "I0731 20:00:18.437791 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.438842 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.438842 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
      "I0731 20:00:18.439988 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "I0731 20:00:18.440590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "I0731 20:00:18.440590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "I0731 20:00:18.441590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "I0731 20:00:18.442943 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "I0731 20:00:18.443591 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "I0731 20:00:18.444591 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "I0731 20:00:18.445590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "I0731 20:00:18.445590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "I0731 20:00:18.446702 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "I0731 20:00:18.447589 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "I0731 20:00:18.447996 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "I0731 20:00:18.448591 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "I0731 20:00:18.449224 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.449590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.450590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "I0731 20:00:18.450590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "I0731 20:00:18.451630 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "I0731 20:00:18.451630 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "I0731 20:00:18.453015 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "I0731 20:00:18.453590 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "I0731 20:00:18.454141 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "I0731 20:00:18.455220 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "I0731 20:00:18.455220 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "I0731 20:00:18.456520 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "I0731 20:00:18.458445 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "I0731 20:00:18.459143 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "I0731 20:00:18.460142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "I0731 20:00:18.461143 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "I0731 20:00:18.461581 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.462142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.462142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "I0731 20:00:18.463183 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "I0731 20:00:18.464142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "I0731 20:00:18.464570 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "I0731 20:00:18.465698 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "I0731 20:00:18.466142 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "I0731 20:00:18.467433 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "I0731 20:00:18.468173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "I0731 20:00:18.468610 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "I0731 20:00:18.469172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "I0731 20:00:18.470172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "I0731 20:00:18.471171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "I0731 20:00:18.471577 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "I0731 20:00:18.472748 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "I0731 20:00:18.473289 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.474172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.475522 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "I0731 20:00:18.476171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "I0731 20:00:18.477172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "I0731 20:00:18.477172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "I0731 20:00:18.478172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "I0731 20:00:18.479173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "I0731 20:00:18.480171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "I0731 20:00:18.480623 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "I0731 20:00:18.481172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "I0731 20:00:18.482173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "I0731 20:00:18.482173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "I0731 20:00:18.483195 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "I0731 20:00:18.484568 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "I0731 20:00:18.485172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "I0731 20:00:18.485172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.486172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.487171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "I0731 20:00:18.488183 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "I0731 20:00:18.488183 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "I0731 20:00:18.489314 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "I0731 20:00:18.490432 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "I0731 20:00:18.491173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "I0731 20:00:18.491702 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "I0731 20:00:18.492172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "I0731 20:00:18.493173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "I0731 20:00:18.493173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "I0731 20:00:18.494173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "I0731 20:00:18.495281 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "I0731 20:00:18.495281 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "I0731 20:00:18.496459 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "I0731 20:00:18.497172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.497839 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.498173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "I0731 20:00:18.499172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "I0731 20:00:18.499172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "I0731 20:00:18.500282 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "I0731 20:00:18.500282 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "I0731 20:00:18.501173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "I0731 20:00:18.501173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "I0731 20:00:18.502443 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "I0731 20:00:18.503173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "I0731 20:00:18.503701 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "I0731 20:00:18.504173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "I0731 20:00:18.505173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "I0731 20:00:18.505173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "I0731 20:00:18.506533 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "I0731 20:00:18.507172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.508171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.509172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "I0731 20:00:18.509380 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "I0731 20:00:18.510173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "I0731 20:00:18.511171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "I0731 20:00:18.512196 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "I0731 20:00:18.512196 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "I0731 20:00:18.513598 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "I0731 20:00:18.514171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "I0731 20:00:18.515171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "I0731 20:00:18.515171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "I0731 20:00:18.516176 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "I0731 20:00:18.517174 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "I0731 20:00:18.517475 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "I0731 20:00:18.518171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "I0731 20:00:18.518171 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.519211 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.520172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "I0731 20:00:18.520563 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "I0731 20:00:18.521778 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "I0731 20:00:18.522172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "I0731 20:00:18.523173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "I0731 20:00:18.523173 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "I0731 20:00:18.524172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "I0731 20:00:18.524172 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "I0731 20:00:18.525280 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "I0731 20:00:18.526453 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "I0731 20:00:18.526479 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "I0731 20:00:18.527452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "I0731 20:00:18.527623 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "I0731 20:00:18.529001 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "I0731 20:00:18.529452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I0731 20:00:18.530139 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I0731 20:00:18.531127 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "I0731 20:00:18.531452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "I0731 20:00:18.532451 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "I0731 20:00:18.532451 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "I0731 20:00:18.533452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "I0731 20:00:18.533452 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "I0731 20:00:18.534473 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "I0731 20:00:18.535516 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "I0731 20:00:18.535516 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "I0731 20:00:18.536572 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "I0731 20:00:18.536572 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "I0731 20:00:18.537877 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "I0731 20:00:18.538614 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "I0731 20:00:18.539613 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "I0731 20:00:18.553882 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "I0731 20:00:18.554883 28844 initializers.py:507]    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "I0731 20:00:18.575756 28844 embedding.py:255] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0731 20:00:18.576756 28844 embedding.py:255] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0731 20:00:19.829408 28844 archival.py:243] removing temporary unarchived model dir at C:\\Users\\WINDOW~1\\AppData\\Local\\Temp\\tmpykki_15k\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy \n",
    "import os\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])\n",
    "\n",
    "\n",
    "java_path = \"Java/jre1.8.0_251/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "st = StanfordNERTagger(r'SentiSE-master/edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz',\n",
    "                           r'stanford-ner-2020-11-17/stanford-ner.jar',\n",
    "                           encoding='utf-8')\n",
    "\n",
    "\n",
    "predictor = Predictor.from_path(\"I17-1099.Datasets/EMNLP_dataset/coref-spanbert-large-2021.03.10(3).tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"77801468485442c6919e0352044ee305-0\" class=\"displacy\" width=\"1975\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">i</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">heard</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">she</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">passed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">bar</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">exam</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">married</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">recently</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-1\" stroke-width=\"2px\" d=\"M420,439.5 C420,264.5 735.0,264.5 735.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-2\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-3\" stroke-width=\"2px\" d=\"M245,439.5 C245,177.0 740.0,177.0 740.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,441.5 L748.0,429.5 732.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-4\" stroke-width=\"2px\" d=\"M945,439.5 C945,264.5 1260.0,264.5 1260.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-5\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-6\" stroke-width=\"2px\" d=\"M770,439.5 C770,177.0 1265.0,177.0 1265.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,441.5 L1273.0,429.5 1257.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-7\" stroke-width=\"2px\" d=\"M770,439.5 C770,89.5 1445.0,89.5 1445.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,441.5 L1453.0,429.5 1437.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-8\" stroke-width=\"2px\" d=\"M770,439.5 C770,2.0 1625.0,2.0 1625.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,441.5 L1633.0,429.5 1617.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-77801468485442c6919e0352044ee305-0-9\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,352.0 1780.0,352.0 1780.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-77801468485442c6919e0352044ee305-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1780.0,441.5 L1788.0,429.5 1772.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\"]\n",
    "COMPOUNDS = [\"compound\"]\n",
    "PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "\n",
    "doc=\"i heard she has passed the bar exam and married recently \"\n",
    "\n",
    "doc = nlp(doc)\n",
    "displacy.render(doc, style='dep',jupyter=True)\n",
    "\n",
    "\n",
    "#accept(nsubjectpass) finish approved completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i --> nsubj --> PRON --> I  ... heard ,,, 0\n",
      "heard --> ROOT --> VERB --> hear  ... heard ,,, 1\n",
      "she --> nsubj --> PRON --> she  ... passed ,,, 0\n",
      "has --> aux --> AUX --> have  ... passed ,,, 0\n",
      "passed --> ccomp --> VERB --> pass  ... heard ,,, 2\n",
      "the --> det --> DET --> the  ... exam ,,, 0\n",
      "bar --> compound --> NOUN --> bar  ... exam ,,, 0\n",
      "exam --> dobj --> NOUN --> exam  ... passed ,,, 2\n",
      "and --> cc --> CCONJ --> and  ... passed ,,, 0\n",
      "married --> conj --> VERB --> marry  ... passed ,,, 0\n",
      "recently --> advmod --> ADV --> recently  ... married ,,, 0\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(doc)\n",
    "for tok in doc: \n",
    "    head=tok.head\n",
    "    print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_,\"-->\",tok.lemma_,\" ...\", tok.head,\",,,\",tok.n_lefts) \n",
    "    if tok.pos_==\"VERB\":\n",
    "        for i in tok.rights:\n",
    "            if str(i.text)==\"divorce\" and i.dep_==\"dobj\":\n",
    "                print(\".........................\",i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intializing lists of  events keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_direct_relation=[\"wedding\",\"anniversary\",\"graduation\",\"birthday\",\"interview\",\"meeting\",\"resignation\",\"job\"]\n",
    "nouns=[\"pain\",\"birth\",\"surgery\",\"job\",\"cold\",\"flu\",\"fever\",\"wedding\",\"meeting\",\"interview\",\"offer\",\"promotion\",\"headache\",\"exam\",\"divorce\",\"contract\",\"relationship\",\"diploma\",\"masters\"]#as object to the aux verb\n",
    "verbs_without_need_objects=[\"wed\",\"divorce\",\"divorced\",\"elect\",\"engage\",\"die\",\"married\",\"nominate\",\"promote\",\"marry\",\"graduate\",\"bear\",\"hurt\",\"meet\",\"interview\"]\n",
    "verb_with_prepo=[\"move\",\"go\",\"travel\"]\n",
    "adjectives=[\"sick\",\"ill\",\"pregnant\"]\n",
    "pronoun=[\"my\",\"his\",\"her\",\"their\",\"your\"]\n",
    "prepo=[\"to\",\"into\",\"in\",\"on\",\"for\"]\n",
    "verbs_with_objects={\"celebrate\":[\"birthday\",\"birth\",\"graduation\",\"anniversary\",\"success\",\"news\"],\n",
    "                   \"graduate\":[\"collage\",\"school\",\"university\"],\n",
    "                   \"win\":[\"support\",\"award\",\"honor\",\"scholarship\",\"prize\",\"lawsuit\",\"pounds\",\"weight\"],\n",
    "                   \"lose\":[\"support\",\"award\",\"honor\",\"scholarship\",\"prize\",\"lawsuit\",\"pounds\",\"weight\"],\n",
    "                   \"admit\":[\"university\",\"collage\",\"offer\",\"school\"],\n",
    "                   \"pass\":[\"exam\",\"test\",\"semester\",\"midterms\",\"school\",\"interview\",\"meeting\"],\n",
    "                   \"present\":[\"essay\",\"thesis\",\"reading\",\"statment\",\"presentation\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"discuss\":[\"essay\",\"thesis\",\"reading\",\"statment\",\"presentation\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"defend\":[\"essay\",\"thesis\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"move\":[\"city\",\"home\",\"apartment\",\"town\"],\n",
    "                   \"travel\":[\"city\",\"home\",\"apartment\",\"town\"],\n",
    "                   \"contract\":[\"agreement\",\"meeting\"],\n",
    "                   \"act\":[\"role\",\"series\",\"movie\",\"theater\"],\n",
    "                   \"publish\":[\"book\",\"post\",\"cover\",\"copy\"],\n",
    "                   \"pregnant\":[\"baby\",\"boy\",\"girl\"],\n",
    "                   \"buy\":[\"car\",\"house\",\"phone\",\"laptope\"],\n",
    "                   \"accept\":[\"invite\",\"work\",\"school\",\"business \",\"university\",\"job\",\"offer\",\"college\",\"program\",\"project\",\"research\",\"paper\",\"invitation\"],\n",
    "                   \"visit\":[\"hospital\",\"doctor\",\"city\",\"country\"],\n",
    "                   \"go\":[\"trip\",\"vacation\",\"holiday\"],\n",
    "                   \"sing\":[\"song\",\"album\"],\n",
    "                   \"sold\":[\"car\",\"house\",\"phone\"],\n",
    "                   \"sign\":[\"deal\",\"contruct\"],\n",
    "                   \"finish\":[\"diploma\",\"masters\",\"trip\",\"vacation\",\"holiday\",\"college\",\"university\",\"job\",\"test\",\"exam\",\"school\",\"semester\",\"midterms\",\"album\",\"book\",\"essay\",\"thesis\",\"reading\",\"statment\",\"presentation\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"start\":[\"diploma\",\"masters\",\"trip\",\"vacation\",\"holiday\",\"college\",\"university\",\"job\",\"test\",\"exam\",\"school\",\"semester\",\"midterms\",\"album\",\"book\",\"essay\",\"thesis\",\"reading\",\"statment\",\"presentation\",\"dissertation\",\"project\",\"research\",\"paper\"],\n",
    "                   \"break\":[\"leg\",\"arm\",\"finger\",\"neck\",\"head\",\"Back\"],\n",
    "                   \"mark\":[\"anniversary\",\"birthday\",\"birth\",\"graduation\",\"success\",\"death\"],\n",
    "                   \"fail\":[\"test\",\"exam\",\"school\",\"semester\",\"midterms\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rules for extraction events patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rules:\n",
    "    def __init__(self,text,list_of_keywords=[],tokens=[],tokens_=[],pos_tags=[],dependency=[],lemma=[]):\n",
    "        self.text=text\n",
    "        self.list_of_keywords=list_of_keywords\n",
    "        self.tokens=[]\n",
    "        self.tokens_=[]\n",
    "        self.pos_tags=[]\n",
    "        self.dependency=[]\n",
    "        self.lemma=[]\n",
    "        for tok in self.text: \n",
    "            self.tokens.append(tok.text)\n",
    "            self.tokens_.append(tok)\n",
    "            self.pos_tags.append(tok.pos_)\n",
    "            self.dependency.append(tok.dep_)\n",
    "            self.lemma.append(tok.lemma_)\n",
    "        \n",
    "\n",
    "    def retrival_senetce_rule1(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            \n",
    "                if lemm in self.list_of_keywords and (self.pos_tags[i]=='VERB' ):\n",
    "                    #or self.pos_tags[i]=='ADV' or self.pos_tags[i]=='ADJ' or self.pos_tags[i]=='NOUN'\n",
    "                    return self.text,self.tokens[i]\n",
    "             \n",
    "\n",
    "            \n",
    "    def retrival_senetce_rule2(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if lemm in self.list_of_keywords and self.pos_tags[i]=='VERB':\n",
    "                for t in self.tokens_[i].rights:\n",
    "                    if t.dep_==\"dobj\" and str(t.lemma_) in self.list_of_keywords[lemm]:\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                    \n",
    "                    elif t.dep_==\"dobj\" :\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"prep\" :\n",
    "                                for t2 in t1.rights:\n",
    "                                    if t2.dep_==\"pobj\" and str(t2.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                        return self.text,self.tokens[i]+\" \"+t2.text\n",
    "\n",
    "                    elif t.dep_==\"oprd\" and str(t.lemma_) in self.list_of_keywords[lemm] :\n",
    "                        return self.text,lemmself.tokens[i]+\" \"+t.text\n",
    "\n",
    "                    elif t.dep_==\"prep\":\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"pobj\" and str(t1.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                return self.text,self.tokens[i]+\" \"+t1.text\n",
    "                            \n",
    "                            elif t1.dep_==\"pobj\":\n",
    "                                for t4 in t1.lefts:\n",
    "                                    if t4.dep_==\"compound\" and str(t4.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                        return self.text,self.tokens[i]+\" \"+t4.text\n",
    "                 \n",
    "                                    \n",
    "                for t in self.tokens_[i].lefts:\n",
    "                    if t.dep_==\"nsubjpass\" and str(t) in self.list_of_keywords[lemm]:\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                     \n",
    "            \n",
    "            \n",
    "    def retrival_senetce_rule3(self):  \n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if self.pos_tags[i]=='VERB'or self.pos_tags[i]== 'AUX':\n",
    "                for t in self.tokens_[i].rights:\n",
    "                    if t.dep_==\"dobj\" and str(t) in self.list_of_keywords:\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                    \n",
    "                    elif t.dep_==\"prep\":\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"pobj\" and str(t1) in self.list_of_keywords:\n",
    "                                return self.text,self.tokens[i]+\" \"+t1.text\n",
    "                    \n",
    "                    elif t.dep_==\"dobj\" :\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"prep\" :\n",
    "                                for t2 in t1.rights:\n",
    "                                    if t2.dep_==\"pobj\" and str(t2) in self.list_of_keywords:\n",
    "                                        return self.text,self.tokens[i]+\" \"+t2.text\n",
    "                                    \n",
    "                     \n",
    "            \n",
    "            \n",
    "        \n",
    "    def retrival_senetce_rule4(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if self.pos_tags[i]==\"AUX\":\n",
    "                for t in self.tokens_[i].rights:\n",
    "                    if t.dep_==\"acomp\" and str(t) in self.list_of_keywords and t.pos_==\"ADJ\":\n",
    "                        print(\"ad \", t.text)\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                    \n",
    "                \n",
    "            elif self.pos_tags[i]==\"VERB\":\n",
    "                for t in self.tokens_[i].rights:\n",
    "                     if t.dep_==\"oprd\" and str(t) in self.list_of_keywords and (t.pos_==\"NOUN\" or t.pos_==\"ADJ\"):\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                     \n",
    "            \n",
    "    \n",
    "    \n",
    "    def retrival_senetce_rule5(self): \n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if (self.pos_tags[i]==\"PRON\" or self.pos_tags[i]==\"PROPN\" ) :\n",
    "                head=self.tokens_[i].head\n",
    "                if self.tokens_[i].dep_==\"poss\" and str(head) in self.list_of_keywords and head.pos_==\"NOUN\":\n",
    "                        return self.text,self.tokens[i]+\" \"+head.text\n",
    "                    \n",
    "                elif self.tokens_[i].dep_==\"poss\" and head.pos_==\"NOUN\":\n",
    "                    for t in head.lefts:\n",
    "                        if str(t) in self.list_of_keywords and  t.dep_==\"compound\":\n",
    "                            return self.text,self.tokens[i]+\" \"+t.text\n",
    "                        \n",
    "                \n",
    "                            \n",
    "    def retrival_senetce_rule6(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if lemm in self.list_of_keywords and self.pos_tags[i]=='VERB':\n",
    "                for t in self.tokens_[i].rights:\n",
    "                    if t.dep_==\"prep\" and str(t) in prepo:\n",
    "                        return self.text,self.tokens[i]+\" \"+t.text\n",
    "                \"\"\"for t in self.tokens_[i].lefts:\n",
    "                    if t.dep_==\"aux\" and str(t) == \"to\":\n",
    "                        return None\"\"\"\n",
    "                    \n",
    "                    \n",
    "    def retrival_senetce_rule7(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if lemm in self.list_of_keywords and (self.pos_tags[i]=='VERB' or self.pos_tags[i]=='NOUN' or self.pos_tags[i]=='ADJ'):\n",
    "                    if self.tokens_[i].dep_==\"ROOT\" :\n",
    "                        for t in self.tokens_[i].rights:\n",
    "                            if t.dep_==\"dobj\" and str(t.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                return self.text\n",
    "\n",
    "                            elif t.dep_==\"dobj\" :\n",
    "                                for t1 in t.rights:\n",
    "                                    if t1.dep_==\"prep\" :\n",
    "                                        for t2 in t1.rights:\n",
    "                                            if t2.dep_==\"pobj\" and str(t2.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                                return self.text\n",
    "\n",
    "                            elif t.dep_==\"oprd\" and str(t.lemma_) in self.list_of_keywords[lemm] :\n",
    "                                return self.text\n",
    "\n",
    "                            elif t.dep_==\"prep\":\n",
    "                                for t1 in t.rights:\n",
    "                                    if t1.dep_==\"pobj\" and str(t1.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                        return self.text\n",
    "                                    elif t1.dep_==\"pobj\":\n",
    "                                        for t4 in t1.lefts:\n",
    "                                            if t4.dep_==\"compound\" and str(t4.lemma_) in self.list_of_keywords[lemm]:\n",
    "                                                return self.text\n",
    "        \n",
    "    \"\"\"\n",
    "                            \n",
    "    def retrival_senetce_rule10(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if (self.pos_tags[i]=='VERB' or self.pos_tags[i]==\"AUX\"):\n",
    "                    for t in self.tokens_[i].rights:\n",
    "                        if t.dep_==\"acomp\" and str(t) in self.list_of_keywords and (t.pos_==\"ADJ\" or t.pos_==\"VERB\" or t.pos_==\"ADV\"): \n",
    "                            return self.text\n",
    "                        \n",
    "    def retrival_senetce_rule11(self):\n",
    "        for i,lemm in enumerate(self.lemma):\n",
    "            if ( self.pos_tags[i]==\"AUX\" and self.dependency[i]==\"auxpass\"):\n",
    "                        head=self.tokens_[i].head\n",
    "                        if  str(head) in self.list_of_keywords and (head.pos_==\"ADJ\" or head.pos_==\"VERB\"): \n",
    "                            return self.text\n",
    "                            \n",
    "                            \"\"\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(you and me will married this month ., 'married')"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"you and me will married this month .\"\n",
    "doc=nlp(doc)\n",
    "p1=Rules(doc, verbs_without_need_objects)\n",
    "text=p1.retrival_senetce_rule1()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(he passed the bar exam, 'passed exam')"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"he passed the bar exam\"\n",
    "doc=nlp(doc)\n",
    "p2=Rules(doc, verbs_with_objects)\n",
    "text=p2.retrival_senetce_rule2()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(i got a surgery, 'got surgery')"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"i got a surgery\"\n",
    "doc=nlp(doc)\n",
    "p3=Rules(doc, nouns)\n",
    "text=p3.retrival_senetce_rule3()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad  sick\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(i am very sick, 'am sick')"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"i am very sick\"\n",
    "doc=nlp(doc)\n",
    "p4=Rules(doc, adjectives)\n",
    "text=p4.retrival_senetce_rule4()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(my wedding was last year, 'my wedding')"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"my wedding was last year\"\n",
    "doc=nlp(doc)\n",
    "p5=Rules(doc, noun_direct_relation)\n",
    "text=p5.retrival_senetce_rule5()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(i am moving to USA, 'moving to')"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=\"i am moving to USA\"\n",
    "doc=nlp(doc)\n",
    "p6=Rules(doc, verb_with_prepo)\n",
    "text=p6.retrival_senetce_rule6()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking persons subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[\"i\",\"we\",\"you\",\"they\",\"he\",\"she\",\"my\",\"his\",\"her\",\"their\",\"your\",\"our\",\"me\",\"him\",\"them\"]\n",
    "def is_person(text,subject,st=st):\n",
    "    x=0\n",
    "    tokens=[]\n",
    "    for token in text :\n",
    "        tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if str(token) == str(subject):  \n",
    "            lis=str(text).split()\n",
    "            for i,j in enumerate(lis):\n",
    "                j=j.replace(\"'s\",\"\")\n",
    "                lis[i]=j\n",
    "            #print(lis)\n",
    "            \n",
    "            #print(tagged)\n",
    "            if token.pos_==\"NOUN\":\n",
    "                tagged = st.tag(lis)\n",
    "                for i,j in enumerate(tagged):\n",
    "                    if 'PERSON' in j:\n",
    "                        if str(j[0])==str(subject):\n",
    "                            return True \n",
    "                        if str(j[0])!=str(subject):\n",
    "                            return False\n",
    "                for i,j in enumerate(tagged):\n",
    "                    if 'PERSON' not in j:\n",
    "                        x+=1\n",
    "                        if x==len(tokens):\n",
    "                            return False\n",
    "                    \n",
    "            elif token.pos_==\"PRON\" and (str(token) in p):\n",
    "                return True\n",
    "            elif token.pos_==\"PRON\" and (str(token) not in p):\n",
    "                return False\n",
    "            \n",
    "            elif token.pos_==\"PROPN\":\n",
    "                return True\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"george divorced katty and take her alone\"\n",
    "text=nlp(text)\n",
    "is_person(text,\"george\",st=st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rules for subjects extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pronoun=[\"me\",\"him\",\"her\",\"them\",\"you\",\"us\"]\n",
    "p_pronoun=[\"my\",\"his\",\"her\",\"their\",\"your\",\"our\"]\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def find_obj_for_verb(doc,verb):\n",
    "    tokens=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "            if str(token) == str(verb):\n",
    "                for t in tokens[i].rights:\n",
    "                    if t.dep_==\"dobj\" :\n",
    "                        return t\n",
    "\n",
    "                    elif t.dep_==\"prep\":\n",
    "                        for t1 in t.rights:\n",
    "                            if t1.dep_==\"pobj\" :\n",
    "                                return t1\n",
    "                                    \n",
    "                for t in tokens[i].lefts:\n",
    "                    if t.dep_==\"nsubjpass\" :\n",
    "                        if token.n_rights>0:\n",
    "                            for t1 in token.rights:\n",
    "                                if t1.dep_ == \"agent\":\n",
    "                                    return t\n",
    "                                    \n",
    "    \n",
    "    \n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    rightDeps=[]\n",
    "    for sub in subs:\n",
    "        for tok in sub.rights:\n",
    "            if tok.dep_ ==\"conj\" :\n",
    "                    #.................................................................\n",
    "                    subs=chek_pronoun_verb(tok)\n",
    "                    moreSubs.append(subs)\n",
    "            if len(moreSubs) > 0:\n",
    "                \n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "            \n",
    "        \n",
    "\n",
    "def chek_pronoun_verb(tok):\n",
    "    if  tok.n_lefts>0:\n",
    "        for s in (tok.lefts):\n",
    "            if  (s.dep_==\"poss\") and str(s) in p_pronoun :\n",
    "                return s\n",
    "            elif  (s.dep_==\"poss\") and (s.pos_==\"PROPN\" or s.pos_==\"NOUN\"):\n",
    "                return s\n",
    "            else:\n",
    "                return tok\n",
    "    else :\n",
    "        return tok\n",
    "\n",
    "def chek_left_sub(tok):\n",
    "    if  tok.n_lefts>0:\n",
    "        for s in tok.lefts:\n",
    "            if  (s.dep_==\"compound\") and (s.pos_==\"NOUN\" or s.pos_==\"PROPN\") :\n",
    "                return s\n",
    "            else :\n",
    "                return tok\n",
    "    else :\n",
    "        return tok\n",
    "\n",
    "def find_sub_for_verb(doc,verb):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    objs=[]\n",
    "    s=[]\n",
    "    e=[]\n",
    "    p=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "            if str(token) == str(verb):\n",
    "                for tok in token.lefts:                    \n",
    "                    if tok.dep_ == \"auxpass\" :\n",
    "                        if tok.pos_=='AUX' :\n",
    "                            for t1 in tok.lefts:\n",
    "                                if t1.dep_==\"nsubj\" or t1.dep_==\"nsubjpass\" :\n",
    "                                    subs.append(t1)\n",
    "                                            \n",
    "                                    \n",
    "                    elif tok.dep_ == \"nsubjpass\":\n",
    "                        objs.append(tok)\n",
    "                        if token.n_rights>0:\n",
    "                            for t in token.rights:\n",
    "                                if t.dep_ == \"agent\":\n",
    "                                    for t1 in t.rights:\n",
    "                                        if t1.dep_==\"pobj\":\n",
    "                                            subs.append(t1)\n",
    "                                else:\n",
    "                                    subs.append(tok)\n",
    "\n",
    "                        else:\n",
    "                            subs.append(tok)\n",
    "                           \n",
    "                        \n",
    "                    elif tok.dep_ == \"nsubj\" :\n",
    "                        if is_person(doc,tok)==True:\n",
    "                                subs.append(tok)\n",
    "                        \n",
    "                        else :\n",
    "                            for t3 in tok.lefts:\n",
    "                                if t3.dep_==\"poss\" and (t3.pos_==\"NOUN\" or t3.pos_==\"PROPN\" or t3.pos_==\"PRON\"):\n",
    "                                    subs.append(tok)\n",
    "                                else:    \n",
    "                                    for t2 in token.rights:\n",
    "                                        if t2.dep_ == \"nsubj\" :\n",
    "                                            #if is_person(doc,t2)==True:\n",
    "                                            subs.append(t2)\n",
    "                                    \n",
    "                                \n",
    "                                \n",
    "                        \n",
    "            #kareem get to finish his masters \n",
    "            if len(subs)==0:\n",
    "                if token.dep_ ==\"acomp\" or token.dep_ ==\"xcomp\":\n",
    "                        head=token.head\n",
    "                        if head.pos_==\"VERB\" or head.pos_==\"AUX\":\n",
    "                            for t in head.lefts:\n",
    "                                if t.dep_==\"nsubj\" or t.dep_==\"nsubjpass\":\n",
    "                                    subs.append(t)\n",
    "                                    \n",
    "                elif token.dep_==\"conj\":\n",
    "                    #print(\"conj 1\")\n",
    "                    h=token.head\n",
    "                    if h.pos_==\"VERB\":\n",
    "                        sub=find_sub_for_verb(doc,h)[0]\n",
    "                        if sub :\n",
    "                            subs.append(sub) \n",
    "                        else:\n",
    "                            subs.append(None)\n",
    "                        \n",
    "                \n",
    "                                \n",
    "                \n",
    "                    \n",
    "                \n",
    "            if len(subs) > 0:\n",
    "                subs = list(pd.Series(subs).drop_duplicates())\n",
    "                subs.extend(getSubsFromConjunctions(subs))\n",
    "                for i in subs:\n",
    "                    s.append(chek_pronoun_verb(i))\n",
    "               \n",
    "                for i in s:\n",
    "                    if is_person(doc,i)==False :\n",
    "                        s.remove(i)\n",
    "                \n",
    "                for i in range(len(s)):\n",
    "                    if s[i]!=subs[i] and str(s[i]) in p_pronoun:\n",
    "                        p.append((s[i],subs[i]))\n",
    "                    elif s[i]!=subs[i] and (s[i].pos_==\"PROPN\" or s[i].pos_==\"NOUN\"):\n",
    "                        p.append((s[i],subs[i]))\n",
    "                        \n",
    "                    elif s[i]==subs[i]:\n",
    "                        \n",
    "                        p.append(s[i])\n",
    "                    else:\n",
    "                        p.append(s[i])\n",
    "                        p.append(subs)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def findSubs_for_verbEvents(doc,list_=[]):\n",
    "    \n",
    "    subs=[]\n",
    "    objs=[]\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\" and str(token.lemma_) in list_:\n",
    "            obj=find_obj_for_verb(doc,token)\n",
    "            sub=find_sub_for_verb(doc,token)\n",
    "            if sub:\n",
    "                return sub\n",
    "                \n",
    "            else:\n",
    "                if obj!=None:\n",
    "                    if str(obj) in obj_pronoun:\n",
    "                        return [obj]\n",
    "\n",
    "\n",
    "\n",
    "def findSubs_for_direct_relation(doc,list_=[]):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if (tokens[i].pos_==\"PRON\" or tokens[i].pos_==\"PROPN\" ) :\n",
    "            head=tokens[i].head\n",
    "            if tokens[i].dep_==\"poss\" and str(head) in list_ and head.pos_==\"NOUN\":\n",
    "                subs.append(token) \n",
    "                    \n",
    "            elif tokens[i].dep_==\"poss\" and head.pos_==\"NOUN\":\n",
    "                for t in head.lefts:\n",
    "                    if str(t) in list_ and  t.dep_==\"compound\":\n",
    "                        subs.append(token) \n",
    "        if len(subs) > 0:\n",
    "                return subs \n",
    "            \n",
    "        \n",
    "def findSubs_for_nounsEvents(doc,list_=[]):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if str(token) in list_:\n",
    "            \n",
    "            \n",
    "            s=chek_pronoun_verb(token)\n",
    "            if str(s) == str(token):\n",
    "                \n",
    "                for i,token in enumerate(tokens):\n",
    "                    if tokens[i].pos_=='VERB'or tokens[i].pos_== 'AUX':\n",
    "                        obj=find_obj_for_verb(doc,token)\n",
    "                        sub=find_sub_for_verb(doc,token)\n",
    "                        for t in tokens[i].rights:\n",
    "                            if t.dep_==\"dobj\" and str(t) in list_:\n",
    "                                if sub :\n",
    "                                    if len(sub)>0:\n",
    "                                        return sub \n",
    "\n",
    "\n",
    "                            elif t.dep_==\"prep\":\n",
    "                                for t1 in t.rights:\n",
    "                                    if t1.dep_==\"pobj\" and str(t1) in list_:\n",
    "                                        if sub:\n",
    "                                            if len(sub)>0:\n",
    "                                                return sub \n",
    "\n",
    "                            elif t.dep_==\"dobj\" :\n",
    "                                for t1 in t.rights:\n",
    "                                    if t1.dep_==\"prep\" :\n",
    "                                        for t2 in t1.rights:\n",
    "                                            if t2.dep_==\"pobj\" and str(t2) in list_:\n",
    "                                                if sub:\n",
    "                                                    if len(sub)>0:\n",
    "                                                        return sub \n",
    "            \n",
    "            elif s !=None and str(s) != str(token) :\n",
    "                return [s]\n",
    "\n",
    "                       \n",
    "                                    \n",
    "\n",
    "def findSubs_for_adjectiveEvents(doc,list_=[]):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    for token in doc :\n",
    "         tokens.append(token)\n",
    "    for i,token in enumerate(tokens):\n",
    "            if tokens[i].pos_==\"AUX\":\n",
    "                obj=find_obj_for_verb(doc,token)\n",
    "                sub=find_sub_for_verb(doc,token)\n",
    "                for t in tokens[i].rights:\n",
    "                    if t.dep_==\"acomp\" and str(t) in list_ and t.pos_==\"ADJ\":\n",
    "                        if sub:\n",
    "                            if len(sub )>0:\n",
    "                                return sub\n",
    "                \n",
    "            elif tokens[i].pos_==\"VERB\":\n",
    "                obj=find_obj_for_verb(doc,token)\n",
    "                sub=find_sub_for_verb(doc,token)\n",
    "                for t in tokens[i].rights:\n",
    "                     if t.dep_==\"oprd\" and str(t) in list_ and (t.pos_==\"NOUN\" or t.pos_==\"ADJ\"):\n",
    "                        if sub :\n",
    "                            if len(sub)>0:\n",
    "                                return sub\n",
    "     \n",
    "                            \n",
    "def findSubs_for_actionEvents(doc,list_=[]):\n",
    "    tokens=[]\n",
    "    subs=[]\n",
    "    for token in doc :\n",
    "        tokens.append(token)\n",
    "        for i,token in enumerate(tokens):\n",
    "            if str(tokens[i].lemma_) in list_ and tokens[i].pos_=='VERB':\n",
    "                obj=find_obj_for_verb(doc,token)\n",
    "                sub=find_sub_for_verb(doc,token)\n",
    "                for t in tokens[i].rights:\n",
    "                    if t.dep_==\"prep\" :\n",
    "                        if sub:\n",
    "                            if len(sub)>0:\n",
    "                                return sub                              \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking events extraction and subjects extraction rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's why we want to have a look at other hotels .\n",
      "We donâ€™t want to spend too much on an extravagant wedding reception .\n",
      "Iguess you're right .\n",
      "\n",
      "\n",
      "\n",
      "george and my husband will married this week .\n",
      "....  ['Rule1 ', (george and my husband will married this week ., 'married'), [george, my]]\n",
      "\n",
      "\n",
      "\n",
      "george and mari will married this month .\n",
      "....  ['Rule1 ', (george and mari will married this month ., 'married'), [george, mari]]\n",
      "i wish a lot .\n",
      "\n",
      "\n",
      "\n",
      "i and me will married this month .\n",
      "....  ['Rule1 ', (i and me will married this month ., 'married'), [i, me]]\n",
      "i wish a lot .\n",
      "\n",
      "\n",
      "\n",
      "you and me will married this month .\n",
      "....  ['Rule1 ', (you and me will married this month ., 'married'), [you, me]]\n",
      "i wish a lot .\n",
      "\n",
      "\n",
      "\n",
      "I'll stay home this morning and rest , but if I feel better in the afternoon , I'm going to the meeting .\n",
      "....  ['Rule3 ', (i'll stay home this morning and rest , but if i feel better in the afternoon , i'm going to the meeting ., 'going meeting'), [i]]\n",
      "....  ['Rule6 ', (i'll stay home this morning and rest , but if i feel better in the afternoon , i'm going to the meeting ., 'going to'), [i]]\n",
      "\n",
      "\n",
      "\n",
      "How was your meeting with Abigail?\n",
      "....  ['Rule5 ', (how was your meeting with abigail?, 'your meeting'), [your]]\n",
      "we would be in a meeting and she would say , yesterday I was chatting with Tom .\n",
      "....  ['Rule3 ', (we would be in a meeting and she would say , yesterday i was chatting with tom ., 'be meeting'), [we]]\n",
      "\n",
      "\n",
      "\n",
      "yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side .\n",
      "....  ['Rule5 ', (yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side ., 'our wedding'), [our]]\n",
      "\n",
      "\n",
      "\n",
      "yesterday my husband and sally renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side .\n",
      "....  ['Rule5 ', (yesterday my husband and sally renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side ., 'our wedding'), [our]]\n",
      "\n",
      "\n",
      "\n",
      "my husband and george will married this week .\n",
      "....  ['Rule1 ', (my husband and george will married this week ., 'married'), [(my, husband), george]]\n",
      "\n",
      "\n",
      "\n",
      "Mom, did my sister call you, what are the results of her competition?\n",
      "she won a couple of addy awards for her package design .\n",
      "....  ['Rule2 ', (she won a couple of addy awards for her package design ., 'won awards'), [she]]\n",
      "Wow , that's great .\n",
      "\n",
      "\n",
      "\n",
      "why karolina did not come ?\n",
      "karolina's son has an exam tomorrow .\n",
      "....  ['Rule3 ', (karolina's son has an exam tomorrow ., 'has exam'), [(karolina, son)]]\n",
      "\n",
      "\n",
      "\n",
      "We have to go to John's house for consolation .\n",
      "....  ['Rule6 ', (we have to go to john's house for consolation ., 'go to'), [we]]\n",
      "consolation ?\n",
      "yes , dani marked the anniversary of his mother's death today .\n",
      "....  ['Rule2 ', (yes , dani marked the anniversary of his mother's death today ., 'marked anniversary'), [dani]]\n",
      "\n",
      "\n",
      "\n",
      "should probably let you all know that nathan and i are now engaged .\n",
      "....  ['Rule1 ', (should probably let you all know that nathan and i are now engaged ., 'engaged'), [nathan, i]]\n",
      "\n",
      "\n",
      "\n",
      "How many months are you pregnant ?\n",
      "ad  pregnant\n",
      "....  ['Rule4 ', (how many months are you pregnant ?, 'are pregnant'), [you]]\n",
      "tow month .\n",
      "\n",
      "\n",
      "\n",
      "what was your wedding ceremony like , Abigail ?\n",
      "....  ['Rule5 ', (what was your wedding ceremony like , abigail ?, 'your wedding'), [your]]\n",
      "my husband and I got married in a registry office with just two friends there as witnesses .\n",
      "....  ['Rule1 ', (my husband and i got married in a registry office with just two friends there as witnesses ., 'married'), [(my, husband), i]]\n",
      "But then we had three parties to celebrate .\n",
      "\n",
      "\n",
      "\n",
      "We're having a party at our house today, and you're invited .\n",
      "for what ?\n",
      "we celebrating my brother's graduation from university .\n",
      "....  ['Rule2 ', (we celebrating my brother's graduation from university ., 'celebrating graduation'), [we]]\n",
      "surely , it is my pleasure .\n",
      "\n",
      "\n",
      "\n",
      "Where are you going ?\n",
      "Back to live with my parents .\n",
      "That's something else I used to do before we were married .\n",
      "....  ['Rule1 ', (that's something else i used to do before we were married ., 'married'), [i]]\n",
      "\n",
      "\n",
      "\n",
      "yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side .\n",
      "....  ['Rule5 ', (yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side ., 'our wedding'), [our]]\n",
      "\n",
      "\n",
      "\n",
      "what do you think about Abigail and george ?\n",
      "Do you think they two will get married ?\n",
      "....  ['Rule1 ', (do you think they two will get married ?, 'married'), [they]]\n",
      "Yeah , you can count on it .\n",
      "\n",
      "\n",
      "\n",
      "What about Pamela ?\n",
      "I heard she has passed the bar exam .\n",
      "....  ['Rule2 ', (i heard she has passed the bar exam ., 'passed exam'), [she]]\n",
      "....  ['Rule3 ', (i heard she has passed the bar exam ., 'passed exam'), [she]]\n",
      "Yes , and she married recently .\n",
      "....  ['Rule1 ', (yes , and she married recently ., 'married'), [she]]\n",
      "\n",
      "\n",
      "\n",
      "Frank's getting married , do you believe this ?\n",
      "....  ['Rule1 ', (frank's getting married , do you believe this ?, 'married'), [frank]]\n",
      "Is he really ?\n",
      "Yes , he is .\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "f = open(\"Dialogs_data.txt\", \"r\")\n",
    "sentence=[]\n",
    "sayers=[]\n",
    "for line in f:\n",
    "\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        sayer=stripped_line[:stripped_line.find(\":\")]\n",
    "        sayers.append(sayer)\n",
    "        \n",
    "        stripped_line=stripped_line[stripped_line.find(\":\")+1:]\n",
    "        sentence.append(stripped_line)\n",
    "\n",
    "f.close()\n",
    "n=0\n",
    "\n",
    "for i in sentence :\n",
    "    print(i)\n",
    "    list_tuple=[]\n",
    "    t=i.lower()\n",
    "    doc=nlp(t)\n",
    "\n",
    "    \n",
    "    p1=Rules(doc, verbs_without_need_objects)\n",
    "    text=p1.retrival_senetce_rule1()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule1 \",text , findSubs_for_verbEvents(nlp(text[0]),verbs_without_need_objects)])\n",
    "\n",
    "        \n",
    "    p2=Rules(doc, verbs_with_objects)\n",
    "    text=p2.retrival_senetce_rule2()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule2 \",text ,findSubs_for_verbEvents(nlp(text[0]),verbs_with_objects)])\n",
    "             \n",
    "                \n",
    "    p3=Rules(doc, nouns)\n",
    "    text=p3.retrival_senetce_rule3()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule3 \",text ,findSubs_for_nounsEvents(nlp(text[0]),nouns)])\n",
    "    \n",
    "    \n",
    "    p4=Rules(doc, adjectives)\n",
    "    text=p4.retrival_senetce_rule4()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule4 \",text ,findSubs_for_adjectiveEvents(nlp(text[0]),adjectives)])\n",
    "    \n",
    "    \n",
    "    p5=Rules(doc, noun_direct_relation)\n",
    "    text=p5.retrival_senetce_rule5()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule5 \",text , findSubs_for_direct_relation(nlp(text[0]),noun_direct_relation)])\n",
    "    \n",
    "    \n",
    "    p6=Rules(doc, verb_with_prepo)\n",
    "    text=p6.retrival_senetce_rule6()\n",
    "    if text!=None:\n",
    "        list_tuple.append([\"Rule6 \",text ,findSubs_for_actionEvents(nlp(text[0]),verb_with_prepo)])\n",
    "    \n",
    "    \n",
    "    for i in list_tuple:\n",
    "        print(\".... \",i)\n",
    "        \n",
    "    if i==\"\":\n",
    "        n+=1\n",
    "        print(\"\\n\")\n",
    "print(n)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering all cases for rules conflict and aggregiation all possible (events and subjects) for the sentence in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_subjects(subject1,subject2):\n",
    "    p_feminine=[\"she\",\"her\"]\n",
    "    p_masculine=[\"he\",\"his\",\"him\"]\n",
    "    p_self_speaker=[\"i\",\"my\",\"me\"]\n",
    "    p_you=[\"you\",\"your\"]\n",
    "    p_they=[\"they\",\"their\"]\n",
    "    p_we=[\"we\",\"our\",\"us\"]\n",
    "    \n",
    "    if str(subject1)==str(subject2):\n",
    "        return True\n",
    "    elif str(subject1) in p_feminine and str(subject2) in p_feminine:\n",
    "        return True\n",
    "    elif str(subject1) in p_masculine and str(subject2) in p_masculine:\n",
    "        return True\n",
    "    elif str(subject1) in p_self_speaker and str(subject2) in p_self_speaker:\n",
    "        return True\n",
    "    elif str(subject1) in p_you and str(subject2) in p_you:\n",
    "        return True\n",
    "    elif str(subject1) in p_they and str(subject2) in p_they:\n",
    "        return True\n",
    "    elif str(subject1) in p_we and str(subject2) in p_we:\n",
    "        return True\n",
    "    else:\n",
    "        False\n",
    "\n",
    "\n",
    "    \n",
    "def aggregation_for_sentence(list_tuple):\n",
    "    ag=[]\n",
    "    events=[]\n",
    "    subjects=[]\n",
    "    sub=[]\n",
    "    s=[]\n",
    "    if len(list_tuple)>=1: \n",
    "        for i in range(len(list_tuple)):\n",
    "            #event\n",
    "            lis1=list_tuple[i][1]\n",
    "            #subject\n",
    "            lis2=list_tuple[i][2]\n",
    "            if lis2 !=None :\n",
    "                if lis1 !=None:\n",
    "                    ag.append([lis1[1],lis2])\n",
    "            \n",
    "            \n",
    "        for i in range(len(ag)):\n",
    "            events.append(ag[i][0])\n",
    "            \n",
    "            #has subject\n",
    "            if len(ag[i])==2:\n",
    "                sub_=ag[i][1]\n",
    "                for index in range(len(sub_)):\n",
    "                    if isinstance(sub_[index], tuple)==False:\n",
    "                        if sub_[index] not in s :\n",
    "                            s.append(sub_[index])\n",
    "                            if s!=None:\n",
    "                                if len(s)!=0:\n",
    "                                    for j in range(len(s)):\n",
    "   \n",
    "                                        subjects.append(s[j])\n",
    "                                  \n",
    "                                        \n",
    "                    else:\n",
    "                        subjects.append(sub_[index])\n",
    "                        \n",
    "        subjects = list(pd.Series(subjects).drop_duplicates())      \n",
    "        return events,subjects\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def filtering(list_tuple=[]):\n",
    "    \n",
    "    if len(list_tuple)>=6: \n",
    "        _6_rules(list_tuple)\n",
    "        if len(list_tuple)>=5:\n",
    "            _5_rules(list_tuple)\n",
    "            if len(list_tuple)>=4:\n",
    "                _4_rules(list_tuple)\n",
    "                if len(list_tuple)>=3:\n",
    "                    _3_rules(list_tuple)\n",
    "                    if len(list_tuple)>=2:\n",
    "                        _2_rules(list_tuple)\n",
    "    \n",
    "    \n",
    "    if len(list_tuple)>=5: \n",
    "        _5_rules(list_tuple)\n",
    "        if len(list_tuple)>=4:\n",
    "            _4_rules(list_tuple)\n",
    "            if len(list_tuple)>=3:\n",
    "                _3_rules(list_tuple)\n",
    "                if len(list_tuple)>=2:\n",
    "                    _2_rules(list_tuple)\n",
    "        \n",
    "    if len(list_tuple)>=4:\n",
    "        _4_rules(list_tuple)\n",
    "        if len(list_tuple)>=3:\n",
    "            _3_rules(list_tuple)\n",
    "            if len(list_tuple)>=2:\n",
    "                    _2_rules(list_tuple)\n",
    "        \n",
    "    if len(list_tuple)>=3:\n",
    "        _3_rules(list_tuple)\n",
    "        if len(list_tuple)>=2:\n",
    "            _2_rules(list_tuple)\n",
    "            \n",
    "    if len(list_tuple)>=2:\n",
    "        _2_rules(list_tuple)\n",
    "       \n",
    "                 \n",
    "def _6_rules(list_tuple=[]):   \n",
    "    \n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \" and list_tuple[3][0]==\"Rule4 \"  and list_tuple[4][0]==\"Rule5 \"  and list_tuple[5][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[5][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            list_tuple.remove(list_tuple[5])\n",
    "        \n",
    "def _5_rules(list_tuple=[]):\n",
    "    \n",
    "\n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \" and list_tuple[3][0]==\"Rule4 \"  and list_tuple[4][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \" and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule4 \"  and list_tuple[4][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \" and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule4 \"  and list_tuple[4][0]==\"Rule6 \"):\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \" and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule5 \"  and list_tuple[4][0]==\"Rule6 \"):\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \"  and list_tuple[4][0]==\"Rule6 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \"  and list_tuple[4][0]==\"Rule6 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[4][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[4])\n",
    "                            \n",
    "                            \n",
    "def _4_rules(list_tuple=[]):    \n",
    "                                \n",
    "\n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule4 \"  and list_tuple[3][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \"  and list_tuple[3][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \" and list_tuple[2][0]==\"Rule3 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[2][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule4 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule5 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule5 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "                            \n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \" and list_tuple[2][0]==\"Rule4 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule4 \" and list_tuple[2][0]==\"Rule5 \" and list_tuple[3][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[3][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[0])\n",
    "                            list_tuple.remove(list_tuple[3])\n",
    "\n",
    "def _3_rules(list_tuple=[]): \n",
    "       \n",
    "\n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule3 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \"  and list_tuple[2][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule4 \"  and list_tuple[2][0]==\"Rule5 \"):\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule4 \"  and list_tuple[2][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \"  and list_tuple[2][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[1][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule4 \"  and list_tuple[2][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[0])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule5 \"  and list_tuple[2][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[2][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            print(lis1[0]+\" \"+lis2[1]+\" \"+lis1[1])\n",
    "                            list_tuple.remove(list_tuple[0])\n",
    "                            list_tuple.remove(list_tuple[2])\n",
    "                            \n",
    "                            \n",
    "def _2_rules(list_tuple=[]): \n",
    "    \n",
    "\n",
    "            if (list_tuple[0][0]==\"Rule1 \"  and list_tuple[1][0]==\"Rule2 \") :\n",
    "                        lis=str(list_tuple[1][1][1]).split()\n",
    "                        if str(list_tuple[0][1][1]) == str(lis[0]):\n",
    "                            list_tuple.remove(list_tuple[0])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule3 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[1][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule2 \"  and list_tuple[1][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[1][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "\n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule5 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[1][1][1]).split()\n",
    "                        if str(lis1[1]) == str(lis2[1]):\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            \n",
    "            elif (list_tuple[0][0]==\"Rule3 \"  and list_tuple[1][0]==\"Rule6 \") :\n",
    "                        lis1=str(list_tuple[0][1][1]).split()\n",
    "                        lis2=str(list_tuple[1][1][1]).split()\n",
    "                        if str(lis1[0]) == str(lis2[0]):\n",
    "                            list_tuple.remove(list_tuple[1])\n",
    "                            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pronoun resolution section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[\"he\",\"she\",\"her\",\"his\",\"him\",\"they\",\"their\",\"them\",\"us\",\"we\",\"our\",\"me\",\"my\",\"you\",\"your\",\"i\"]  \n",
    "num_of_sentence_for_pronoun_resolution=2\n",
    "\n",
    "# getting corference clusters \n",
    "def pronoun_resolution_allen(sent,predictor=predictor):\n",
    "    \n",
    "    pred = predictor.predict(\n",
    "        document= sent\n",
    "    )\n",
    "\n",
    "    clusters = pred['clusters']\n",
    "    document = pred['document']\n",
    "    n = 0\n",
    "    doc = {}\n",
    "    for obj in document:\n",
    "        doc.update({n :  obj}) #what I'm doing here is creating a dictionary of each word with its respective index, making it easier later.\n",
    "        n = n+1\n",
    "\n",
    "    clus_all = []\n",
    "    cluster = []\n",
    "    clus_one = {}\n",
    "    s=\"\"\n",
    "    for i in range(0, len(clusters)):\n",
    "        one_cl = clusters[i]\n",
    "        for count in range(0, len(one_cl)):\n",
    "            obj = one_cl[count]            \n",
    "            for num in range((obj[0]), (obj[1]+1)):                \n",
    "                for n in doc:\n",
    "                    if num == n:\n",
    "                        if obj[1]-obj[0]>0:\n",
    "                            s+=\" \"+str(doc[n])\n",
    "                            if len(s.split())>=2 :\n",
    "                                cluster.append(s)\n",
    "                        elif obj[1]-obj[0]==0: \n",
    "                            cluster.append(doc[n])\n",
    "        clus_all.append(cluster)\n",
    "        cluster = [] \n",
    "    return clus_all\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#for checking if the results of pronoun resolution need more resolution like\n",
    "#my sister yesterday comes , she won awards (the results of subject is she .... after pronoun resolution we get my sister ... but until now we dont know \n",
    "#which the real subject so we use this function to again make pronoun resolution using my rule )\n",
    "def check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog):\n",
    "    count = element.count(' and ')                 \n",
    "    while (count >=0 ):                     \n",
    "        if \" my \" in element :\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" my \",\" \"+str(subb)+\" \")\n",
    "\n",
    "        elif element.endswith(' my')==True:\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" my\",\" \"+str(subb))\n",
    "\n",
    "        elif \" i \" in element :\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" i \",\" \"+str(subb)+\" \")\n",
    "            print(\"element .... \",element)\n",
    "\n",
    "        elif  element.endswith(' i')==True:\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" i\",\" \"+str(subb))\n",
    "            print(\"element .... \",element)\n",
    "\n",
    "        elif \" me \" in element :\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" me \",\" \"+str(subb)+\" \")\n",
    "            \n",
    "        elif element.endswith(' me')==True:\n",
    "            subb=my_pronoun_resolution_for_i(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" me\",\" \"+str(subb))\n",
    "\n",
    "\n",
    "        elif \" you \" in element :\n",
    "            subb=my_pronoun_resolution_for_you(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" you \",\" \"+str(subb)+\" \")\n",
    "            \n",
    "        elif element.endswith(' you')==True:\n",
    "            subb=my_pronoun_resolution_for_you(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" you\",\" \"+str(subb))\n",
    "\n",
    "\n",
    "        elif \" your \" in element :\n",
    "            subb=my_pronoun_resolution_for_you(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" your \",\" \"+str(subb)+\" \")\n",
    "            \n",
    "        elif element.endswith(' your')==True:\n",
    "            subb=my_pronoun_resolution_for_you(element,\" \",dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "            element=element.replace(\" your\",\" \"+str(subb))\n",
    "\n",
    "        elif (\" my \" not in element )and (\" i \" not in element) and (\" me \" not in element) and (\" your \" not in element) and (\" you \" not in element) and(element.endswith(' you')==False) and(element.endswith(' your')==False) and(element.endswith(' i')==False) and(element.endswith(' my')==False) and(element.endswith(' me')==False):\n",
    "            element=\" \"\n",
    "            \n",
    "        count-=1\n",
    "        \n",
    "    return element            \n",
    "\n",
    "\n",
    "\n",
    "# my rules for pronoun resolution for i\n",
    "def my_pronoun_resolution_for_i(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog):\n",
    "    subjects=\"\"\n",
    "    if str(sub2)==\" \":\n",
    "        subjects=sayers_of_dialog[dialog_number][index_of_sentence]\n",
    "    elif str(sub2) !=\" \":\n",
    "        c=sayers_of_dialog[dialog_number][index_of_sentence]+\" \"+str(sub2)\n",
    "        subjects=c\n",
    "    return subjects\n",
    "        \n",
    "    \n",
    "    \n",
    "# my rules for pronoun resolution for you\n",
    "def my_pronoun_resolution_for_you(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog,len_of_dialog):\n",
    "    subjects=\"\"\n",
    "    if len_of_dialog[dialog_number]-1==index_of_sentence:\n",
    "        if sayers_of_dialog[dialog_number][index_of_sentence-1]!=sayers_of_dialog[dialog_number][index_of_sentence]:\n",
    "            if str(sub2)==\" \": \n",
    "                subjects=sayers_of_dialog[dialog_number][index_of_sentence-1]\n",
    "            elif str(sub2)!=\" \":\n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence-1]+\" \"+str(sub2)\n",
    "                subjects=c\n",
    "    if len_of_dialog[dialog_number]-1>index_of_sentence:\n",
    "        if sayers_of_dialog[dialog_number][index_of_sentence+1]!=sayers_of_dialog[dialog_number][index_of_sentence]:\n",
    "            if str(sub2)==\" \": \n",
    "                subjects=sayers_of_dialog[dialog_number][index_of_sentence+1]\n",
    "            elif str(sub2)!=\" \": \n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence+1]+\" \"+str(sub2)\n",
    "                subjects=c\n",
    "    return subjects\n",
    "            \n",
    "            \n",
    "def my_pronoun_resolution_rules_for_we(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog,len_of_dialog):\n",
    "    subjects=\"\"\n",
    "    if len_of_dialog[dialog_number]-1==index_of_sentence:\n",
    "        if sayers_of_dialog[dialog_number][index_of_sentence-1]!=sayers_of_dialog[dialog_number][index_of_sentence]:\n",
    "            if str(sub2)==\" \": \n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence-1]+\" and \"+sayers_of_dialog[dialog_number][index_of_sentence]\n",
    "                subjects=c\n",
    "            elif str(sub2)!=\" \":\n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence-1]+\" and \"+sayers_of_dialog[dialog_number][index_of_sentence]+\" \"+str(sub2)\n",
    "                subjects=c\n",
    "\n",
    "    if len_of_dialog[dialog_number]-1>index_of_sentence:\n",
    "        if sayers_of_dialog[dialog_number][index_of_sentence+1]!=sayers_of_dialog[dialog_number][index_of_sentence]:\n",
    "            if str(sub2)==\" \": \n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence+1]+\" and \"+sayers_of_dialog[dialog_number][index_of_sentence]\n",
    "                subjects=c\n",
    "            elif str(sub2)!=\" \": \n",
    "                c=sayers_of_dialog[dialog_number][index_of_sentence+1]+\" and \"+sayers_of_dialog[dialog_number][index_of_sentence]+\" \"+str(sub2)\n",
    "                subjects=c\n",
    "    return subjects\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def check_type_of_subject(sub):\n",
    "    \n",
    "    \n",
    "    if isinstance(sub, tuple)==False:\n",
    "        return sub,\" \"\n",
    "    if isinstance(sub, tuple)==True:\n",
    "        return sub[0],sub[1]\n",
    "    \n",
    "\n",
    "\n",
    "def getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number):\n",
    "    sentence=[]\n",
    "    text=\"\"\n",
    "    index_of_sentence1=index_of_sentence\n",
    "    while(index_of_sentence1>=0 and num_of_sentence_for_pronoun_resolution>=0):\n",
    "        sentence.append(sentences_of_dialogs[dialog_number][index_of_sentence1])\n",
    "        index_of_sentence1-=1\n",
    "        num_of_sentence_for_pronoun_resolution-=1\n",
    "    for i in range(len(sentence)):\n",
    "        text+=\" \"+str(sentence[len(sentence)-i-1])\n",
    "    return text    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "def pronoun_resolution_for_all_cases_of_pronouns(aggrigation_list=[],dialog_number=0,index_of_sentence=0,sentences_of_dialogs=[],num_of_sentence_for_pronoun_resolution=3,sayers_of_dialog=[],len_of_dialog=[]):\n",
    "    b=aggrigation_list\n",
    "    \n",
    "    if len(b[0]) !=0 and  len(b[1]) !=0:\n",
    "        subjects=[]\n",
    "        \n",
    "        for i in range(len(b[1])):\n",
    "            \n",
    "            sub,sub2=check_type_of_subject(b[1][i])\n",
    "\n",
    "            if str(sub)==\"i\" or str(sub)==\"my\" or str(sub)==\"me\":\n",
    "                subjects.append(my_pronoun_resolution_for_i(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog))\n",
    "\n",
    "                \n",
    "            elif str(sub)==\"you\" or str(sub)==\"your\" :\n",
    "                subjects.append(my_pronoun_resolution_for_you(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog,len_of_dialog))\n",
    "\n",
    "                \n",
    "            elif str(sub)==\"her\" or str(sub)==\"she\" :\n",
    "                \n",
    "                text=getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number)\n",
    "                \n",
    "                print(\"....... more than one sentnce for making pro resolution ....... (\", (text) ,\")\",\"\\n\" )    \n",
    "                \n",
    "                resol=pronoun_resolution_allen(text)\n",
    "\n",
    "                if len(resol)!=0:\n",
    "                    for i in resol:\n",
    "                        if \"her\" in i or \"she\" in i :\n",
    "                            for element in i :\n",
    "                                if len(element.split())==1:\n",
    "                                    if element not in p:\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(element)\n",
    "                                        elif str(sub2)!=\" \": \n",
    "                                            c=element+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                elif len(element.split())==2:\n",
    "                                    ele=check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "                                    #like my sister after pronoun resolution should make pronoun again using my rules \n",
    "                                    if ele==\" \":\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(element)\n",
    "                                        elif str(sub2)!=\" \":\n",
    "                                            c=element+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                    else:\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(ele)\n",
    "                                        elif str(sub2)!=\" \":\n",
    "                                            c=ele+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                        \n",
    "                                    \n",
    "                                    \n",
    "\n",
    "\n",
    "\n",
    "            elif str(sub)==\"he\" or str(sub)==\"his\" or str(sub)==\"him\" :\n",
    "                \n",
    "                text=getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number)\n",
    "                \n",
    "                print(\"............... more than one sentnce for making pro resolution ...............(\", (text) ,\")\" ,\"\\n\" )    \n",
    "                \n",
    "                resol=pronoun_resolution_allen(text)\n",
    "\n",
    "                if len(resol)!=0:\n",
    "                    for i in resol:\n",
    "                        if \"his\" in i or \"he\" in i  or \"him\" in i :\n",
    "                            for element in i :\n",
    "                                if len(element.split())==1:\n",
    "                                    if element not in p:\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(element)\n",
    "                                        elif str(sub2)!=\" \": \n",
    "                                            c=element+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                elif len(element.split())==2:\n",
    "                                    #like my sister after pronoun resolution should make pronoun again using my rules \n",
    "                                    \n",
    "                                    ele=check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "                                    \n",
    "                                    if ele==\" \":   \n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(element)\n",
    "                                        elif str(sub2)!=\" \":\n",
    "                                            c=element+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                    else:\n",
    "                                        if str(sub2)==\" \": \n",
    "                                            subjects.append(ele)\n",
    "                                        elif str(sub2)!=\" \":\n",
    "                                            c=ele+\" \"+str(sub2)\n",
    "                                            subjects.append(c)\n",
    "                                        \n",
    "                                    \n",
    "\n",
    "\n",
    "            \n",
    "            elif str(sub)==\"they\" or str(sub)==\"their\" :\n",
    "                \n",
    "                text=getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number)\n",
    "                \n",
    "                print(\"............... more than one sentnce for making pro resolution ............... (\", (text) ,\")\",\"\\n\" )    \n",
    "                \n",
    "                resol=pronoun_resolution_allen(text)\n",
    "                \n",
    "                if len(resol)!=0:\n",
    "                    for i in resol:\n",
    "                        for element in i :\n",
    "                            if (\" and \" in element) and (\" they \" not in element) and (\" their \" not in element) and (element.endswith('and')==False ):\n",
    "                                if element.endswith('they')==False and element.endswith('their')==False:\n",
    "                                    if len(element.split())>=3:\n",
    "                                        ele=check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "                                            \n",
    "                                        if ele ==\" \":\n",
    "                                            if str(sub2)==\" \": \n",
    "                                                subjects.append(element)\n",
    "                                            elif str(sub2)!=\" \": \n",
    "                                                c=element+\" \"+str(sub2)\n",
    "                                                subjects.append(c)\n",
    "                                        else:\n",
    "                                            if str(sub2)==\" \": \n",
    "                                                subjects.append(ele)\n",
    "                                            elif str(sub2)!=\" \": \n",
    "                                                c=ele+\" \"+str(sub2)\n",
    "                                                subjects.append(c)\n",
    "\n",
    "\n",
    "            elif str(sub)==\"we\" or str(sub)==\"our\" or str(sub)==\"us\":\n",
    "                \n",
    "                text=getting_n_previous_sentences(index_of_sentence,num_of_sentence_for_pronoun_resolution,sentences_of_dialogs,dialog_number)\n",
    "                \n",
    "                    \n",
    "                print(\"............... more than one sentnce for making pro resolution ............... (\", (text) ,\")\" ,\"\\n\" )    \n",
    "                \n",
    "                resol=pronoun_resolution_allen(text)\n",
    "\n",
    "                if len(resol)!=0 :\n",
    "                    for i in resol:\n",
    "                        for element in i :\n",
    "                            if (\" and \" in element) and (\" we \" not in element) and (\" our \" not in element ) and (\" us \" not in element ) and element.endswith('and')==False :\n",
    "                                if element.endswith('we')==False and element.endswith('our')==False and element.endswith('us')==False:\n",
    "                                    if len(element.split())>=3:\n",
    "                                        ele=check_need_for_more_resol(element,dialog_number,index_of_sentence,sayers_of_dialog)\n",
    "\n",
    "                                        if ele==\" \":\n",
    "                                            if str(sub2)==\" \": \n",
    "                                                subjects.append(element)\n",
    "                                            elif str(sub2)!=\" \": \n",
    "                                                c=element+\" \"+str(sub2)\n",
    "                                                subjects.append(c)\n",
    "                                        else:\n",
    "                                            if str(sub2)==\" \": \n",
    "                                                subjects.append(ele)\n",
    "                                            elif str(sub2)!=\" \": \n",
    "                                                c=ele+\" \"+str(sub2)\n",
    "                                                subjects.append(c)\n",
    "                                            \n",
    "                if len(subjects)==0:\n",
    "                    subjects.append(my_pronoun_resolution_rules_for_we(sub,sub2,dialog_number,index_of_sentence,sayers_of_dialog,len_of_dialog))\n",
    "                    \n",
    "\n",
    "        subjects = list(pd.Series(subjects).drop_duplicates())  \n",
    "        return subjects\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregation the results of pronoun resolution and the results of all previous steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_between_filtering_and_resolution(aggregation_for_sentence,resolution):\n",
    "            agg=[]\n",
    "            b=aggregation_for_sentence\n",
    "            if resolution !=None and len(resolution)==0 and (len(b)==2):\n",
    "                agg.append(b)\n",
    "            if resolution !=None and len(resolution)!=0 and (len(b)==2):\n",
    "                if b[0]!=0 and b[1]!=0:\n",
    "                    if len(b[1])==1:\n",
    "                        if isinstance(b[1][0], tuple)==True:\n",
    "                            agg.append((b[0] ,resolution))\n",
    "                        elif isinstance(b[1][0], tuple)==False:\n",
    "                            if str(b[1][0]) in p :\n",
    "                                agg.append((b[0] ,resolution))\n",
    "                            elif str(b[1][0]) not in p :\n",
    "                                agg.append((b[0] ,b[1]))\n",
    "                    elif len(b[1])>1:\n",
    "                        if len(b[1])==len(resolution):\n",
    "                            agg.append((b[0] ,resolution))\n",
    "                            \n",
    "                        elif len(b[1])!=len(resolution):\n",
    "                            for i in range(len(b[1])):\n",
    "                                if i+1 < len(b[1]): \n",
    "                                    if equal_subjects(b[1][i],b[1][i+1]):\n",
    "                                        b[1].remove(b[1][i])\n",
    "                            for i in range(len(resolution)):\n",
    "                                for j in range(len(b[1])):\n",
    "                                    if isinstance(b[1][j] , tuple)==False:\n",
    "                                        if str(b[1][j]) in p :\n",
    "                                            b[1][j]=resolution[i]\n",
    "                                            continue\n",
    "                                    elif isinstance(b[1][j] , tuple)==True:\n",
    "                                        c=b[1][j]\n",
    "                                        if str(c[0]) in p :\n",
    "                                            \n",
    "                                            b[1][j]=resolution[i]\n",
    "                                            \n",
    "                                            continue\n",
    "                            agg.append((b[0] ,b[1]))           \n",
    "            return agg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "output=pandas.read_csv('labels.csv') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking all previous steps (extract events , and subjects with pronoun resolution) , applaying filtering and aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 23:53:36.985095 28844 warnings.py:110] C:\\Users\\Windows dunya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:164: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark   Susan is going to get a divorce .\n",
      "...............subject result befor resolution.............. [susan] \n",
      "\n",
      " ..................the final results....................  [(['get divorce'], [susan])] \n",
      "\n",
      "Jhon   How do you know that ?\n",
      "Mark   She told me that Peter and she has a quarrel last nigh , and she left this morning , bag and baggage .\n",
      "Jhon   I see . But I think you are making a fuss .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   these days too many people are getting divorced .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 23:53:43.505668 28844 warnings.py:110] C:\\Users\\Windows dunya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jhon   If they live together , then at least they're finding out if they're really compatible or not .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   what is happend with Mari and george ?\n",
      "Mark   They got a divorce at last .\n",
      "...............subject result befor resolution.............. [they] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  what is happend with Mari and george ? They got a divorce at last . ) \n",
      "\n",
      " ..................the final results....................  [(['got divorce'], [' Mari and george'])] \n",
      "\n",
      "Jhon   It's inevitable .\n",
      "Mark   Their love was built on the sand , and this is why their marriage has landed on the rocks.\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   what is happend with Mari?\n",
      "Mark   I heard she has passed the bar exam and married recently .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  what is happend with Mari? I heard she has passed the bar exam and married recently . ) \n",
      "\n",
      " ..................the final results....................  [(['married', 'passed exam'], ['Mari'])] \n",
      "\n",
      "Jhon   Oh yes .\n",
      "Mark   She had a beautiful wedding in Cozumel Mexico and we all attended .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  I heard she has passed the bar exam and married recently . Oh yes . She had a beautiful wedding in Cozumel Mexico and we all attended . ) \n",
      "\n",
      " ..................the final results....................  [(['had wedding'], ['She'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   That's why we want to have a look at other hotels .\n",
      "Abigail   We donâ€™t want to spend too much on an extravagant wedding reception .\n",
      "Jhon   Iguess you're right .\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   my husband's parents flew out to see my family when we got married in my hometown , so that was great .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  my husband's parents flew out to see my family when we got married in my hometown , so that was great . ) \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Jhon and Abigail'])] \n",
      "\n",
      "Jhon   Some people spend ridiculous amounts of money on extravagant wedding receptions , but we agreed that makes sense .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   what was your wedding ceremony like , Abigail ?\n",
      "...............subject result befor resolution.............. [your] \n",
      "\n",
      " ..................the final results....................  [(['your wedding'], ['Abigail'])] \n",
      "\n",
      "Abigail   my husband and I got married in a registry office with just two friends there as witnesses .\n",
      "...............subject result befor resolution.............. [(my, husband), i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Abigail husband', 'Abigail'])] \n",
      "\n",
      "Abigail   But then we had three parties to celebrate .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Is that your wedding ring Abigail ?\n",
      "...............subject result befor resolution.............. [your] \n",
      "\n",
      " ..................the final results....................  [(['your wedding'], ['Abigail'])] \n",
      "\n",
      "Abigail   I'm not married yet .\n",
      "Abigail   It's my engagement ring .\n",
      "Jhon   Well , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   You remember ? The island , the sound of the waves , the salty sea air and the sunshine.\n",
      "Mark   yes , it was wonderful but it's already been a year .\n",
      "Jhon   why not go again to celebrate out one-year anniversary ?\n",
      "Mark   We can go to the same beach , stay in the same hotel and enjoy a dinner in the same restaurant\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  yes , it was wonderful but it's already been a year . why not go again to celebrate out one-year anniversary ? We can go to the same beach , stay in the same hotel and enjoy a dinner in the same restaurant ) \n",
      "\n",
      " ..................the final results....................  [(['go to'], ['Jhon and Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   For the 100th anniversary of the opening of the library we are going to have a party .\n",
      "Jhon   That's a wonderful way to celebrate this grand old library.\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Actually , even though we have parades , most people just use the national day holiday to visit family or go shopping .\n",
      "Jhon   In the evening , many people watch special TV shows which celebrate national day .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   what did you say when he asked you how long it took you to learn English ?\n",
      "Jhon   I told him 28 years . And , he knows I'm 28 years old since I just celebrated my birthday last week .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['celebrated birthday'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Do you want to go out to celebrate my good news ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['celebrate news'], ['Jhon'])] \n",
      "\n",
      "Jhon   sure , where would you like to go ?\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Which university did you graduate from ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['graduate'], ['Jhon'])] \n",
      "\n",
      "Jhon   I graduated from Peking University .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['graduated university'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   How you celebrate your Valentineâ€™s Day with your wife ?\n",
      "Jhon   I will take a rain check .\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   Margaret , do you think I should enroll in the science course ?\n",
      "Margaret   I think so , If you want to graduated this year , you've got to take a science course .\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['graduated'], ['Abigail'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   When did you get started ?\n",
      "Abigail   I began blogging when I first went to the US for my strides .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['went to'], ['Abigail'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   I heard your son recently graduated.\n",
      "...............subject result befor resolution.............. [(your, son)] \n",
      "\n",
      " ..................the final results....................  [(['graduated'], ['Margaret son'])] \n",
      "\n",
      "Margaret   Yes , my little Paul is finally a doctor .\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   What about Pamela ?\n",
      "Margaret   I heard she has passed the bar exam .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  What about Pamela ? I heard she has passed the bar exam . ) \n",
      "\n",
      " ..................the final results....................  [(['passed exam'], ['Pamela'])] \n",
      "\n",
      "Abigail   Yes , and she married recently .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  What about Pamela ? I heard she has passed the bar exam . Yes , and she married recently . ) \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Pamela'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   What do you mean by \" us \" ?\n",
      "Abigail   Well , we used to talk to each other .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Where are you going ?\n",
      "Abigail   Back to live with my parents .\n",
      "Mark   That's something else I used to do before we were married .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I'm going to get married next month .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Mark'])] \n",
      "\n",
      "Jhon   Good news ! And congratulations !\n",
      "Mark   You are invited to my wedding .\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['invited wedding'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   when are you getting married ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Margaret'])] \n",
      "\n",
      "Margaret   some time next year .\n",
      "Margaret   We haven't set the date yet .\n",
      "Abigail   congratulations\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   How are you Abigail?\n",
      "Abigail   I'm doing really well .\n",
      "Abigail   I got married about three years ago .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Abigail'])] \n",
      "\n",
      "Abigail   I have two kids now .\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   I've fallen in love with george .\n",
      "Margaret   really ? Is he married ?\n",
      "...............subject result befor resolution.............. [he] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ...............(  I've fallen in love with george . really ? Is he married ? ) \n",
      "\n",
      " ..................the final results....................  [(['married'], ['george'])] \n",
      "\n",
      "Abigail   no , of course not . He is still single .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   what do you think about Abigail and george ?\n",
      "Jhon   Do you think they two will get married ?\n",
      "...............subject result befor resolution.............. [they] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  what do you think about Abigail and george ? Do you think they two will get married ? ) \n",
      "\n",
      " ..................the final results....................  [(['married'], [' Abigail and george'])] \n",
      "\n",
      "Mark   Yeah , you can count on it .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Frank's getting married , do you believe this ?\n",
      "...............subject result befor resolution.............. [frank] \n",
      "\n",
      " ..................the final results....................  [(['married'], [frank])] \n",
      "\n",
      "Mark   Is he really ?\n",
      "Jhon   Yes , he is .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Any good news ?\n",
      "Mark   Yes . I'Ve won the first prize in the math contest .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Mark'])] \n",
      "\n",
      "Jhon   Really ? Congratulations !\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I won first prize in the poetry contest .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Mark'])] \n",
      "\n",
      "Jhon   Come on ! You're pulling my leg .\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   I won a prize last week but it was a prize for beginners .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Abigail'])] \n",
      "\n",
      "Abigail   My prize was for the best player in the country .\n",
      "Mark   Now let's start playing chess seriously .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   george has loved writing since he was a very little boy .\n",
      "Jhon   yes , he won the first prize in a national composition contest when I was in middle school .\n",
      "...............subject result befor resolution.............. [he] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ...............(  george has loved writing since he was a very little boy . yes , he won the first prize in a national composition contest when I was in middle school . ) \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I buy a lottery ticket every week and I'm amazed that I haven won a small prize yet .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   mari won the decathlon in the 1976 Olympics , right ?\n",
      "Jhon   I read that she trained so much that he used to dream about jumping hurdles .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   You're a very good player\n",
      "Jhon   Not really , but once I won a prize .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   Any good news ?\n",
      "Margaret   I'Ve won the first prize in the math contest .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Margaret'])] \n",
      "\n",
      "Abigail   Really ? Congratulations !\n",
      "   \n",
      "\n",
      "\n",
      "Abigail   What time shall we meet at the bus stop ?\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  What time shall we meet at the bus stop ? ) \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Margaret and Abigail'])] \n",
      "\n",
      "Margaret   Let's meet at 12 thirty .\n",
      "Abigail   it will probably take us three or four hours to see all of the exhibits .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   How was your meeting with Abigail?\n",
      "...............subject result befor resolution.............. [your] \n",
      "\n",
      " ..................the final results....................  [(['your meeting'], ['Mark'])] \n",
      "\n",
      "Mark   we would be in a meeting and she would say , yesterday I was chatting with Tom .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  How was your meeting with Abigail? we would be in a meeting and she would say , yesterday I was chatting with Tom . ) \n",
      "\n",
      " ..................the final results....................  [(['be meeting'], [''])] \n",
      "\n",
      "Mark   She meant Tom Solomon\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I just want to get to my meeting !\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['get meeting'], ['Mark'])] \n",
      "\n",
      "Mark   I can't go to your party .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['go to'], ['Mark'])] \n",
      "\n",
      "Jhon   That's too bad .\n",
      "   \n",
      "\n",
      "\n",
      "george   I'll stay home this morning and rest , but if I feel better in the afternoon , I'm going to the meeting .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['going meeting'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Let's go today to the pool .\n",
      "Jhon   I couldn't do that .\n",
      "Jhon   I have an important meeting to go to today .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have meeting'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   I'll have to meet my girlfriend at the airport then .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Jhon'])] \n",
      "\n",
      "Jhon   We'd like to invite you for our dress party tomorrow evening .\n",
      "Mark   I will definitely come .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   I wanna finish my drink first .\n",
      "Mark   I'll meet you at Sammy's .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Mark'])] \n",
      "\n",
      "Jhon   I wait you .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Don't you know our movie starts at seven ?\n",
      "Jhon   And we were going to meet at the theater at five to seven .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  Don't you know our movie starts at seven ? And we were going to meet at the theater at five to seven . ) \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Mark and Jhon'])] \n",
      "\n",
      "Mark   oh , I forget that .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I was thinking that I'd like to invite you to watch a movie .\n",
      "george   I can meet you at the cinema gate .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mari   I'm sure you can do it very well .\n",
      "Mari   Then I'll meet you at six .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Mari'])] \n",
      "\n",
      "Mark   Is that at all right ?\n",
      "Mari   Yes .\n",
      "Mark   OK .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   where and when should I meet you ?\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['Jhon'])] \n",
      "\n",
      "Mark   we'll pick you up at your place at noon . Be there or be square !\n",
      "   \n",
      "\n",
      "\n",
      "george   what about Mark ?\n",
      "george   did you tell him?\n",
      "Mari   he knows I'm coming . Our meeting is set for 2 o'clock .\n",
      "...............subject result befor resolution.............. [our] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  what about Mark ? did you tell him? he knows I'm coming . Our meeting is set for 2 o'clock . ) \n",
      "\n",
      " ..................the final results....................  [(['our meeting'], ['george and Mari'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   We're only switching two days .\n",
      "Kriss   You can do legs on Friday .\n",
      "george   Aright . I'll meet you at the gym at 3:30 then .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['meet'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   When can we start working on this , george?\n",
      "george   Well , we could probably get started with a preparatory meeting this afternoon at 2:00 .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  When can we start working on this , george? Well , we could probably get started with a preparatory meeting this afternoon at 2:00 . ) \n",
      "\n",
      " ..................the final results....................  [(['started meeting'], ['Kriss and george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   I got hurt when fixing the light , even during office hours , I wouldn't get compensation from our company since repairing is not my responsibility .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['hurt'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   There are so many nasties on the internet and so many people who are trying to use the internet to hurt other users .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   What's the matter ?\n",
      "george   I've got a really bad headache and my stomach hurts .\n",
      "...............subject result befor resolution.............. [(my, stomach), i] \n",
      "\n",
      " ..................the final results....................  [(['hurts', 'got headache'], ['george stomach', 'george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   what's happend with Jhon .\n",
      "george   I think he's hurt his back .\n",
      "...............subject result befor resolution.............. [he] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ...............(  what's happend with Jhon . I think he's hurt his back . ) \n",
      "\n",
      " ..................the final results....................  [(['hurt'], ['Jhon'])] \n",
      "\n",
      "Kriss   What shall we do ?\n",
      "george   We'd better not move him .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   I was really very , very nervous just before I had the surgery , but the anaesthetist gave me an anaesthetic and the next thing I remember was waking up after the surgery .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['had surgery'], ['Mark'])] \n",
      "\n",
      "Mark   It must have really hurt afterwards .\n",
      "   \n",
      "\n",
      "\n",
      "george   Tell me a little bit about yourself , please .\n",
      "Kriss   I was born and raised in Beijing . I attended Peking University and received my bachelor's degree in Economics .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['born'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   Have you ever designed any programs concerning network ?\n",
      "Mark   Yes , I have designed some programs for the network with Visual C + + and I have passed the test for programmers - MUSE .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['passed test'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   What band did you pass in Japanese Language Proficiency Test ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['pass test'], ['george'])] \n",
      "\n",
      "george   I passed the Band two in LPT , but I will try to achieve Band one which is the highest level .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   How are your English and computer skills ?\n",
      "george   I have passed the CET - 4 and 6.As far as computer is concerned I can use the computer for word processing .\n",
      "   \n",
      "\n",
      "\n",
      "george   I'Ve got good news .\n",
      "Mark   what ?\n",
      "george   I have successfully passed the first two rounds of interview with ABC Company .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['passed interview'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mari   How many months are you pregnant ?\n",
      "ad  pregnant\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['are pregnant'], ['Abigail'])] \n",
      "\n",
      "Abigail   Three months .\n",
      "   \n",
      "\n",
      "\n",
      "george   It will be fun , you will get to know lots of people .\n",
      "Mark   Sounds great , I'd very much like to accept your invitation , thanks\n",
      "   \n",
      "\n",
      "\n",
      "george   I want to know whether you will come to the interview . So have you accepted offers from other companies ?\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offers', 'come interview'], ['george'])] \n",
      "\n",
      "Mark   No , I haven't got one by now .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   can I help you find something ?\n",
      "Jhon   Yes , actually I'm looking to buy a camera .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Hello . This is Mrs . Wilson .\n",
      "Wilson   I'd like to buy a new car .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['buy car'], ['Wilson'])] \n",
      "\n",
      "Wilson   Could you offer me a new type of the car , please ?\n",
      "   \n",
      "\n",
      "\n",
      "george   How bad did I do ?\n",
      "Mark   To be completely honest , you failed your test .\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['failed test'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   I need a new suit .\n",
      "george   why ?\n",
      "Tina   I have an important interview next week , so I really need to look sharp .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have interview'], ['Tina'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   That's why job interviews are important to let people know the real you that they can't see from a piece of paper .\n",
      "   \n",
      "\n",
      "\n",
      "george   Good morning . Thank you for the interview .\n",
      "Kriss   No problem . Now , do you prefer working with others or flying solo ?\n",
      "george   Actually , I enjoy both .\n",
      "   \n",
      "\n",
      "\n",
      "george   Hey , could you help me try and figure out how to get ready for my job interview ?\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['my interview'], ['george'])] \n",
      "\n",
      "Kriss   The most important thing to do is to make sure you know the company and what services or products it provides .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   If you pass the interview , the personnel department will inform you within two weeks .\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['pass interview'], ['george'])] \n",
      "\n",
      "george   But if I don't pass , will you call me ?\n",
      "Kriss   I'm sorry we won't .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Hi , George . I'm going to have a job interview next week .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have interview'], ['Kriss'])] \n",
      "\n",
      "Kriss   Could you give me some advice ?\n",
      "george   Sure . First of all , it's very important for you not to be late .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Any good news ?\n",
      "Mark   today i have been married to the most beautiful women in the world for 15 years .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Any good news ?\n",
      "george   my daughter will be born tonight at midnight .\n",
      "...............subject result befor resolution.............. [(my, daughter)] \n",
      "\n",
      " ..................the final results....................  [(['born'], ['george daughter'])] \n",
      "\n",
      "Kriss   Well , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   the romance studio interviewed me as a featured author .\n",
      "...............subject result befor resolution.............. [me] \n",
      "\n",
      " ..................the final results....................  [(['interviewed'], ['Kriss'])] \n",
      "\n",
      "george   oh , good i am very happy for you .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   I did not tell you ?\n",
      "Mari   about what ?\n",
      "Jhon   i'm interviewed on montana public radio today about my life.\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['interviewed'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   i'm getting married today .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Margaret'])] \n",
      "\n",
      "Mari   Well , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   today me and matt got engaged and here is my ring .\n",
      "...............subject result befor resolution.............. [me, matt] \n",
      "\n",
      " ..................the final results....................  [(['engaged'], ['Margaret', matt])] \n",
      "\n",
      "Mari   Well , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   should probably let you all know that nathan and i are now engaged .\n",
      "...............subject result befor resolution.............. [nathan, i] \n",
      "\n",
      " ..................the final results....................  [(['engaged'], [nathan, 'Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   I did not tell you what happen with tomas?\n",
      "Kriss   no , tell me.\n",
      "george   tomas divorced katty and take her alone .\n",
      "...............subject result befor resolution.............. [tomas] \n",
      "\n",
      " ..................the final results....................  [(['divorced'], [tomas])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Where was the engaged held?\n",
      "Mari   george and i got engaged in roma .\n",
      "...............subject result befor resolution.............. [george, i] \n",
      "\n",
      " ..................the final results....................  [(['engaged'], [george, 'Mari'])] \n",
      "\n",
      "Kriss   great , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   When did you get married?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['married'], ['Jhon'])] \n",
      "\n",
      "Jhon   two days from now .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   i have good news for you .\n",
      "Mark   really , what ?\n",
      "Jhon   i finally got accepted into howard university .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted university'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   i am very happy for mario .\n",
      "Jhon   what happens with him ?\n",
      "george   mario passed the final exam for this year .\n",
      "...............subject result befor resolution.............. [mario] \n",
      "\n",
      " ..................the final results....................  [(['passed exam'], [mario])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Why are you happy ?\n",
      "Mari   i've just signed my deal with authentik artists .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['signed deal'], ['Mari'])] \n",
      "\n",
      "Kriss   very good .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   what is the lastest news jhon ?\n",
      "Jhon   i just accepted the company's ( generous ) offer and will be starting my new job on monday 18 january .\n",
      "...............subject result befor resolution.............. [i, my] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer', 'my job'], ['Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Mark   Do you know what the news of our friend tom ?\n",
      "Kriss   tom got accepted into grad school and a new job in the past three days .\n",
      "...............subject result befor resolution.............. [tom] \n",
      "\n",
      " ..................the final results....................  [(['accepted school'], [tom])] \n",
      "\n",
      "great    great .\n",
      "   \n",
      "\n",
      "\n",
      "Margaret   i got accepted into syracuse university .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted university'], ['Margaret'])] \n",
      "\n",
      "Mari   I am proud of you .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   i got accepted into the business program at duke univ .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted program'], ['Mark'])] \n",
      "\n",
      "Kriss   very good , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Mari   what is the lastest news of bati?\n",
      "Kati   bati won the 2010 calgary choice awards in arts and culture .\n",
      "...............subject result befor resolution.............. [bati] \n",
      "\n",
      " ..................the final results....................  [(['won awards'], [bati])] \n",
      "\n",
      "Mari   that's great .\n",
      "   \n",
      "\n",
      "\n",
      "george   We're having a party at our house today, and you're invited .\n",
      "Mark   for what ?\n",
      "george   we celebrating my brother's graduation from university .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  We're having a party at our house today, and you're invited . for what ? we celebrating my brother's graduation from university . ) \n",
      "\n",
      " ..................the final results....................  [(['celebrating graduation'], ['Mark and george'])] \n",
      "\n",
      "Mark   surely , it is my pleasure .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   why you so sad ?\n",
      "Mari   i failed that exam .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['failed exam'], ['Mari'])] \n",
      "\n",
      "Bati   oh , i am sorry for you .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   Hi , Tina , I'Ve got good news .\n",
      "Tina   tell me .\n",
      "Bati   i was very honoured and humbled to win this award today .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['win award'], ['Bati'])] \n",
      "\n",
      "Tina   wow , i am very happy for you .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   What is the latest news of your working life ?\n",
      "Tina   i accepted a job offer in columbus and will begin august 2 .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer'], ['Tina'])] \n",
      "\n",
      "Kriss   great .\n",
      "   \n",
      "\n",
      "\n",
      "george   I'm so happy for karla , she finally has good news .\n",
      "Kriss   What is her latest news ?\n",
      "george   finally, karla's son passed the exam .\n",
      "...............subject result befor resolution.............. [(karla, son)] \n",
      "\n",
      " ..................the final results....................  [(['passed exam'], [(karla, son)])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Bati   where mira ? Why didn't she come with you ?\n",
      "Tina   mira celebrating her son's birthday today .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   Hi George, how are you, why didn't you come with our friends ? we are waiting for you .\n",
      "George   No , i can't , i celebrating my one year anniversary with my beautiful wife .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['celebrating anniversary'], ['George'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   Mom, did my sister call you, what are the results of her competition?\n",
      "Tina   she won a couple of addy awards for her package design .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  Mom, did my sister call you, what are the results of her competition? she won a couple of addy awards for her package design . ) \n",
      "\n",
      " ..................the final results....................  [(['won awards'], [' Tina sister'])] \n",
      "\n",
      "george   Wow , that's great .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   What is the latest news of your university life?\n",
      "Jhon   i got my first college acceptance letter and i got accepted into penn state university .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted university'], ['Jhon'])] \n",
      "\n",
      "Bati   great , it is a very good university .\n",
      "   \n",
      "\n",
      "\n",
      "Tina   you know what happen with tom ?\n",
      "Kriss   No , tell me .\n",
      "Tina   tom has been accepted to the georgia state university college of law .\n",
      "...............subject result befor resolution.............. [tom] \n",
      "\n",
      " ..................the final results....................  [(['accepted college'], [tom])] \n",
      "\n",
      "Kriss   great .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   i am very happy for our freind tom , finally he take back his right .\n",
      "Jhon   What are you talking about ?\n",
      "Kriss   tom won his lawsuit for stealing his car .\n",
      "...............subject result befor resolution.............. [tom] \n",
      "\n",
      " ..................the final results....................  [(['won lawsuit'], [tom])] \n",
      "\n",
      "Jhon   it is a good news .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Did you find a job recently ?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['find job'], ['Mark'])] \n",
      "\n",
      "Mark   yes , i started the new job on july 1 .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['started job'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   What are all these preparations?\n",
      "Jhon   today my wife and i celebrate our 1st wedding anniversary .\n",
      "...............subject result befor resolution.............. [(my, wife), i] \n",
      "\n",
      " ..................the final results....................  [(['celebrate anniversary'], ['Jhon wife', 'Jhon'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   John, you change a lot .\n",
      "Jhon   yes , i lost weight a lot after dieting .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['lost weight'], ['Jhon'])] \n",
      "\n",
      "Kriss   This is better for your health .\n",
      "   \n",
      "\n",
      "\n",
      "Tina   i want to celebrate my birthday tomorrow , so you are invite kriss .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['celebrate birthday'], ['Tina'])] \n",
      "\n",
      "Kriss   Certainly , it's my pleasure .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Will you work in your uncle's company ?\n",
      "Mark   No , i've accepted the job at stanford .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted job'], ['Mark'])] \n",
      "\n",
      "Jhon   really ?\n",
      "Mark   Yes , I am very excited .\n",
      "   \n",
      "\n",
      "\n",
      "george   i accepted their offer and am now the newest member of the hays reading branch finance and accountancy team .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tom   Mari left her house, you know ?\n",
      "Kriss   really ? why ?\n",
      "Tom   she bought a new house .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  Mari left her house, you know ? really ? why ? she bought a new house . ) \n",
      "\n",
      " ..................the final results....................  [(['bought house'], ['Mari'])] \n",
      "\n",
      "Kriss   I hope it's better .\n",
      "Tom   Yes , and it's very beautiful .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Did you start a new job?\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['start job'], ['george'])] \n",
      "\n",
      "george   Yes , on monday i start my new job as web producer for twin cities public tv's new weekly arts series .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['start job'], ['george'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   What happened to you ?\n",
      "Mark   i had broken my leg two days ago .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['broken leg'], ['Mark'])] \n",
      "\n",
      "george   Oh , i am so sorry .\n",
      "   \n",
      "\n",
      "\n",
      "Tom   i accepted an offer from google and started working .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer'], ['Tom'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tom   why you are happy ?\n",
      "Kriss   i am happy for mari , she won the national award for fashion .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won award'], ['Kriss'])] \n",
      "\n",
      "Tom   this is good news .\n",
      "   \n",
      "\n",
      "\n",
      "Frank   i accepted their offer and i am now the newest member of the hays reading branch finance and accountancy team .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['accepted offer'], ['Frank'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   i will start a new job tomorrow after looking for several months .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['start job'], ['Tomas'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   We have to go to John's house for consolation .\n",
      "...............subject result befor resolution.............. [we] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  We have to go to John's house for consolation . ) \n",
      "\n",
      " ..................the final results....................  [(['go to'], ['Mark and Jhon'])] \n",
      "\n",
      "Mark   consolation ?\n",
      "Jhon   yes , dani marked the anniversary of his mother's death today .\n",
      "...............subject result befor resolution.............. [dani] \n",
      "\n",
      " ..................the final results....................  [(['marked anniversary'], [dani])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   You won't congratulate me ?\n",
      "Bati   for what ?\n",
      "Kriss   I won a prize for the best student of this year .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['won prize'], ['Kriss'])] \n",
      "\n",
      "Bati   congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   I heard you graduated this year .\n",
      "...............subject result befor resolution.............. [you] \n",
      "\n",
      " ..................the final results....................  [(['graduated'], ['Tina'])] \n",
      "\n",
      "Tina   Yes , defending my thesis went great .\n",
      "Tomas   congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   I like Maria, she is a beautiful girl .\n",
      "Tomas   tony is in relationship with maria .\n",
      "...............subject result befor resolution.............. [tony] \n",
      "\n",
      " ..................the final results....................  [(['is relationship'], [tony])] \n",
      "\n",
      "Jhon   are you really sure?\n",
      "Tomas   Yes .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   why karolina did not come ?\n",
      "Tomas   karolina's son has an exam tomorrow , she help him with his lessons .\n",
      "...............subject result befor resolution.............. [(karolina, son)] \n",
      "\n",
      " ..................the final results....................  [(['has exam'], [(karolina, son)])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   i got a promotion at job .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['got promotion'], ['Tina'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   I will leave this university ?\n",
      "Tina   why ?\n",
      "Tomas   i've got an offer from cambridge .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['got offer'], ['Tomas'])] \n",
      "\n",
      "Tina   that's great .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   you look so tired .\n",
      "Tina   i am very sick .\n",
      "ad  sick\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['am sick'], ['Tina'])] \n",
      "\n",
      "Tina   i have a fever .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have fever'], ['Tina'])] \n",
      "\n",
      "Bati   i will call the doctor .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   i'm excited ! i have a job interview tomorrow .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['have interview'], ['Jhon'])] \n",
      "\n",
      "Kriss   i wish you the best .\n",
      "   \n",
      "\n",
      "\n",
      "george   i have a bad news .\n",
      "Tina   What's up ?\n",
      "george   mira will have an surgery next week .\n",
      "...............subject result befor resolution.............. [mira] \n",
      "\n",
      " ..................the final results....................  [(['have surgery'], [mira])] \n",
      "\n",
      "Tina   oh , my darling .\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   jim make a contract with his manager .\n",
      "...............subject result befor resolution.............. [jim] \n",
      "\n",
      " ..................the final results....................  [(['make contract'], [jim])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   What's the news of you and your wife?\n",
      "Kriss   i started the new job in the American Communications Company .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['started job'], ['Kriss'])] \n",
      "\n",
      "Jhon   ezabela got a job at boston college and she will be moving back to USA soon .\n",
      "...............subject result befor resolution.............. [ezabela] \n",
      "\n",
      " ..................the final results....................  [(['got job'], [ezabela])] \n",
      "\n",
      "Kriss   It's very good news .\n",
      "   \n",
      "\n",
      "\n",
      "Tina   What is the news of your children, sister?\n",
      "Mari   mario passed the final exam for this year .\n",
      "...............subject result befor resolution.............. [mario] \n",
      "\n",
      " ..................the final results....................  [(['passed exam'], [mario])] \n",
      "\n",
      "Mari   kareem get to finish his masters .\n",
      "...............subject result befor resolution.............. [his] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ...............(  What is the news of your children, sister? mario passed the final exam for this year . kareem get to finish his masters . ) \n",
      "\n",
      " ..................the final results....................  [(['finish masters'], ['kareem'])] \n",
      "\n",
      "Mari   ezabela got a 95% on her first tough exam of the semester .\n",
      "...............subject result befor resolution.............. [her] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  mario passed the final exam for this year . kareem get to finish his masters . ezabela got a 95% on her first tough exam of the semester . ) \n",
      "\n",
      " ..................the final results....................  [(['got exam'], ['ezabela'])] \n",
      "\n",
      "Tina   I am happy for you my sister and proud of your children .\n",
      "   \n",
      "\n",
      "\n",
      "Bati   yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side .\n",
      "...............subject result befor resolution.............. [our] \n",
      "\n",
      "............... the 3 sentences for making pro resolution ............... (  yesterday my husband and i renewed our wedding vows on our 20 year together anniversary with our two teenagers by our side . ) \n",
      "\n",
      "element ....   Bati husband and Bati\n",
      " ..................the final results....................  [(['our wedding'], [' Bati husband and Bati'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   today is my wedding day .\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['my wedding'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   today is my birthday .\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['my birthday'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Kriss   today is my 11th wedding anniversary .\n",
      "...............subject result befor resolution.............. [my] \n",
      "\n",
      " ..................the final results....................  [(['my anniversary'], ['Kriss'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "Tina   i am happy for bati .\n",
      "Jhon   why ?\n",
      "Tina   she finally start moving into her new house.\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  i am happy for bati . why ? she finally start moving into her new house. ) \n",
      "\n",
      " ..................the final results....................  [(['moving into'], ['bati'])] \n",
      "\n",
      "Jhon   oh , she finished preparing the new house ?\n",
      "Tina   Yes .\n",
      "   \n",
      "\n",
      "\n",
      "george   Where will you spend your vacation ?\n",
      "Mark   i will be moving to london at the end of the week , and i will stay there the next three month .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['moving to'], ['Mark'])] \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "george   i'll be traveling to USA to see a moral monday protest .\n",
      "   \n",
      "\n",
      "\n",
      "Jhon   Why are you packing your bags ?\n",
      "Kriss   i am moving to bristol .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['moving to'], ['Kriss'])] \n",
      "\n",
      "Jhon   I will miss you a lot .\n",
      "   \n",
      "\n",
      "\n",
      "george   what is the lastest news about mira ?\n",
      "Bati   finally , she got the apartment! she is moving on february .\n",
      "...............subject result befor resolution.............. [she] \n",
      "\n",
      "....... the 3 sentences for making pro resolution ....... (  what is the lastest news about mira ? finally , she got the apartment! she is moving on february . ) \n",
      "\n",
      " ..................the final results....................  [(['moving on'], ['mira'])] \n",
      "\n",
      "george   very good , congratulations .\n",
      "   \n",
      "\n",
      "\n",
      "Tomas   luke and i will be moving to san francisco later this month .\n",
      "...............subject result befor resolution.............. [luke, i] \n",
      "\n",
      " ..................the final results....................  [(['moving to'], [luke, 'Tomas'])] \n",
      "\n",
      "Jhon   when will you come back ?\n",
      "Tomas   after three month .\n",
      "   \n",
      "\n",
      "\n",
      "Mark   i am moving to US in 3 weeks .\n",
      "...............subject result befor resolution.............. [i] \n",
      "\n",
      " ..................the final results....................  [(['moving to'], ['Mark'])] \n",
      "\n",
      "Mari   Are you going to settle there ?\n",
      "Mark   i Have not decided yet .\n",
      "   \n",
      "\n",
      "\n",
      "Tom   I'm sad for Joy .\n",
      "Bati   why ? what's happend ?\n",
      "Tom   Joy's father died and he is very depressed .\n",
      "...............subject result befor resolution.............. [(joy, father)] \n",
      "\n",
      " ..................the final results....................  [(['died'], [(joy, father)])] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"Dialogs_data.txt\", \"r\")\n",
    "\n",
    "def splitting_sayers_and_sentences():\n",
    "    sentence=[]\n",
    "    sayers=[]\n",
    "    for line in f:\n",
    "        stripped_line = line.strip()\n",
    "        sayer=stripped_line[:stripped_line.find(\":\")]\n",
    "        sayers.append(sayer)\n",
    "        stripped_line=stripped_line[stripped_line.find(\":\")+1:]\n",
    "        sentence.append(stripped_line)    \n",
    "    return sayers,sentence\n",
    "\n",
    "sayers,sentence = splitting_sayers_and_sentences() \n",
    "\n",
    "\n",
    "def get_sayers_of_all_dialogs(sayers):\n",
    "    sayers_of_all_dialogs=[]\n",
    "    sy=[]\n",
    "    for i in sayers :\n",
    "        if i!=\"\":\n",
    "            sy.append(i)\n",
    "        if i==\"\" and len(sy)!=0 :\n",
    "            sayers_of_all_dialogs.append(sy)\n",
    "            sy=[]\n",
    "    return sayers_of_all_dialogs\n",
    "            \n",
    "    \n",
    "sayers_of_all_dialogs = get_sayers_of_all_dialogs(sayers)        \n",
    "            \n",
    "    \n",
    "def get_len_of_all_dialogs(sayers):\n",
    "    len_of_all_dialogs=[]\n",
    "    n=0\n",
    "    for i in sayers :\n",
    "        if i!=\"\":\n",
    "            n+=1\n",
    "        if i==\"\" and len_of_all_dialogs!=0:\n",
    "            len_of_all_dialogs.append(n)\n",
    "            n=0\n",
    "    return len_of_all_dialogs\n",
    "\n",
    "\n",
    "len_of_all_dialogs = get_len_of_all_dialogs(sayers)\n",
    "            \n",
    "        \n",
    "def get_sentences_of_all_dialogs(sentence):\n",
    "    sentences_of_all_dialogs=[]\n",
    "    sentences_of_each_dialog=[]\n",
    "    for i in sentence :\n",
    "        if i!=\"\":\n",
    "            sentences_of_each_dialog.append(i)\n",
    "        if i==\"\":\n",
    "            sentences_of_all_dialogs.append(sentences_of_each_dialog)\n",
    "            sentences_of_each_dialog=[]\n",
    "    return sentences_of_all_dialogs    \n",
    "\n",
    "sentences_of_all_dialogs = get_sentences_of_all_dialogs(sentence)\n",
    "\n",
    "\n",
    "def get_final_results(sentence):\n",
    "    index=-1\n",
    "    subject_result_befor_resolution=[]\n",
    "    final_results=[]   \n",
    "    #counter for index of sentences\n",
    "\n",
    "\n",
    "    index_of_sentence_in_dialog=[]\n",
    "    index_of_sentences_in_all_dialogs=[]\n",
    "    #  dialog number \n",
    "    n=0\n",
    "\n",
    "    aggreigation_dialog=[]\n",
    "    \n",
    "    for j,i in enumerate(sentence) :\n",
    "        print(sayers[j],\" \",i)\n",
    "        list_tuple=[]\n",
    "        t=i.lower()\n",
    "        doc=nlp(t)\n",
    "\n",
    "        if i!=\"\":\n",
    "            index+=1\n",
    "\n",
    "            index_of_sentence_in_dialog.append(index)\n",
    "\n",
    "        p1=Rules(doc, verbs_without_need_objects)\n",
    "        text=p1.retrival_senetce_rule1()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule1 \",text , findSubs_for_verbEvents(nlp(text[0]),verbs_without_need_objects)])\n",
    "\n",
    "\n",
    "        p2=Rules(doc, verbs_with_objects)\n",
    "        text=p2.retrival_senetce_rule2()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule2 \",text ,findSubs_for_verbEvents(nlp(text[0]),verbs_with_objects)])    \n",
    "\n",
    "        p3=Rules(doc, nouns)\n",
    "        text=p3.retrival_senetce_rule3()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule3 \",text ,findSubs_for_nounsEvents(nlp(text[0]),nouns)])\n",
    "\n",
    "\n",
    "        p4=Rules(doc, adjectives)\n",
    "        text=p4.retrival_senetce_rule4()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule4 \",text ,findSubs_for_adjectiveEvents(nlp(text[0]),adjectives)])\n",
    "\n",
    "\n",
    "        p5=Rules(doc, noun_direct_relation)\n",
    "        text=p5.retrival_senetce_rule5()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule5 \",text , findSubs_for_direct_relation(nlp(text[0]),noun_direct_relation)])\n",
    "\n",
    "\n",
    "        p6=Rules(doc, verb_with_prepo)\n",
    "        text=p6.retrival_senetce_rule6()\n",
    "        if text!=None:\n",
    "            list_tuple.append([\"Rule6 \",text ,findSubs_for_actionEvents(nlp(text[0]),verb_with_prepo)])\n",
    "        # for rules conflict\n",
    "        filtering(list_tuple)\n",
    "        # for aggregation all subjects or all events for the sentence \n",
    "        if aggregation_for_sentence(list_tuple)!=None:\n",
    "\n",
    "            if aggregation_for_sentence(list_tuple)[0]!=\"\":\n",
    "                b=aggregation_for_sentence(list_tuple)\n",
    "\n",
    "                if len(b[1])!=0:\n",
    "                    id__=str(n+1)+\"_\"+str(index)\n",
    "\n",
    "                    print(\"...............subject result befor resolution..............\",b[1],\"\\n\" )\n",
    "                    subject_result_befor_resolution.append((b[1],id__))\n",
    "\n",
    "                # make pronoun resolution for getting the real person name    \n",
    "                resolution=pronoun_resolution_for_all_cases_of_pronouns(b,n,index,sentences_of_all_dialogs,num_of_sentence_for_pronoun_resolution,sayers_of_all_dialogs,len_of_all_dialogs)\n",
    "                # for aggregating the results of filtering and aggregating sentence with the results of pronoun resolution\n",
    "                aggrigation=aggregation_between_filtering_and_resolution(b,resolution)\n",
    "\n",
    "                if len(aggrigation)!=0:\n",
    "                    print(\" ..................the final results.................... \",aggrigation,\"\\n\" )\n",
    "                    id_=str(n+1)+\"_\"+str(index)\n",
    "                    final_results.append((aggrigation,id_))\n",
    "\n",
    "\n",
    "\n",
    "        if i==\"\" and len(index_of_sentence_in_dialog)!=0 :\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "            index_of_sentences_in_all_dialogs.append(index_of_sentence_in_dialog)\n",
    "            index=-1\n",
    "\n",
    "            index_of_sentence_in_dialog=[]\n",
    "            n+=1\n",
    "            print(\"\\n\")\n",
    "    return  final_results \n",
    "\n",
    "final_results=get_final_results(sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the results of our task and the labeled dataset for comparing results and evaluating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['get divorce'], [susan])]\n",
      "events ==>  ['get divorce']  ||| subjects ==>  [susan]  ||| dialog number ==>  1_0\n",
      "\n",
      "\n",
      "[(['got divorce'], [' Mari and george'])]\n",
      "events ==>  ['got divorce']  ||| subjects ==>  [' Mari and george']  ||| dialog number ==>  3_1\n",
      "\n",
      "\n",
      "[(['married', 'passed exam'], ['Mari'])]\n",
      "events ==>  ['married', 'passed exam']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  4_1\n",
      "\n",
      "\n",
      "[(['had wedding'], ['She'])]\n",
      "events ==>  ['had wedding']  ||| subjects ==>  ['She']  ||| dialog number ==>  4_3\n",
      "\n",
      "\n",
      "[(['married'], ['Jhon and Abigail'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Jhon and Abigail']  ||| dialog number ==>  6_0\n",
      "\n",
      "\n",
      "[(['your wedding'], ['Abigail'])]\n",
      "events ==>  ['your wedding']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  7_0\n",
      "\n",
      "\n",
      "[(['married'], ['Abigail husband', 'Abigail'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Abigail husband', 'Abigail']  ||| dialog number ==>  7_1\n",
      "\n",
      "\n",
      "[(['your wedding'], ['Abigail'])]\n",
      "events ==>  ['your wedding']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  8_0\n",
      "\n",
      "\n",
      "[(['go to'], ['Jhon and Mark'])]\n",
      "events ==>  ['go to']  ||| subjects ==>  ['Jhon and Mark']  ||| dialog number ==>  9_3\n",
      "\n",
      "\n",
      "[(['celebrated birthday'], ['Jhon'])]\n",
      "events ==>  ['celebrated birthday']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  12_1\n",
      "\n",
      "\n",
      "[(['celebrate news'], ['Jhon'])]\n",
      "events ==>  ['celebrate news']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  13_0\n",
      "\n",
      "\n",
      "[(['graduate'], ['Jhon'])]\n",
      "events ==>  ['graduate']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  14_0\n",
      "\n",
      "\n",
      "[(['graduated university'], ['Jhon'])]\n",
      "events ==>  ['graduated university']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  14_1\n",
      "\n",
      "\n",
      "[(['graduated'], ['Abigail'])]\n",
      "events ==>  ['graduated']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  16_1\n",
      "\n",
      "\n",
      "[(['went to'], ['Abigail'])]\n",
      "events ==>  ['went to']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  17_1\n",
      "\n",
      "\n",
      "[(['graduated'], ['Margaret son'])]\n",
      "events ==>  ['graduated']  ||| subjects ==>  ['Margaret son']  ||| dialog number ==>  18_0\n",
      "\n",
      "\n",
      "[(['passed exam'], ['Pamela'])]\n",
      "events ==>  ['passed exam']  ||| subjects ==>  ['Pamela']  ||| dialog number ==>  19_1\n",
      "\n",
      "\n",
      "[(['married'], ['Pamela'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Pamela']  ||| dialog number ==>  19_2\n",
      "\n",
      "\n",
      "[(['married'], ['Mark'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  21_2\n",
      "\n",
      "\n",
      "[(['married'], ['Mark'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  22_0\n",
      "\n",
      "\n",
      "[(['invited wedding'], ['Mark'])]\n",
      "events ==>  ['invited wedding']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  22_2\n",
      "\n",
      "\n",
      "[(['married'], ['Margaret'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Margaret']  ||| dialog number ==>  23_0\n",
      "\n",
      "\n",
      "[(['married'], ['Abigail'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  24_2\n",
      "\n",
      "\n",
      "[(['married'], ['george'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['george']  ||| dialog number ==>  25_1\n",
      "\n",
      "\n",
      "[(['married'], [' Abigail and george'])]\n",
      "events ==>  ['married']  ||| subjects ==>  [' Abigail and george']  ||| dialog number ==>  26_1\n",
      "\n",
      "\n",
      "[(['married'], [frank])]\n",
      "events ==>  ['married']  ||| subjects ==>  [frank]  ||| dialog number ==>  27_0\n",
      "\n",
      "\n",
      "[(['won prize'], ['Mark'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  28_1\n",
      "\n",
      "\n",
      "[(['won prize'], ['Mark'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  29_0\n",
      "\n",
      "\n",
      "[(['won prize'], ['Abigail'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  30_0\n",
      "\n",
      "\n",
      "[(['won prize'], ['george'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['george']  ||| dialog number ==>  31_1\n",
      "\n",
      "\n",
      "[(['won prize'], ['Mark'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  32_0\n",
      "\n",
      "\n",
      "[(['won prize'], ['Jhon'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  34_1\n",
      "\n",
      "\n",
      "[(['won prize'], ['Margaret'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Margaret']  ||| dialog number ==>  35_1\n",
      "\n",
      "\n",
      "[(['meet'], ['Margaret and Abigail'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Margaret and Abigail']  ||| dialog number ==>  36_0\n",
      "\n",
      "\n",
      "[(['your meeting'], ['Mark'])]\n",
      "events ==>  ['your meeting']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  37_0\n",
      "\n",
      "\n",
      "[(['be meeting'], [''])]\n",
      "events ==>  ['be meeting']  ||| subjects ==>  ['']  ||| dialog number ==>  37_1\n",
      "\n",
      "\n",
      "[(['get meeting'], ['Mark'])]\n",
      "events ==>  ['get meeting']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  38_0\n",
      "\n",
      "\n",
      "[(['go to'], ['Mark'])]\n",
      "events ==>  ['go to']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  38_1\n",
      "\n",
      "\n",
      "[(['going meeting'], ['george'])]\n",
      "events ==>  ['going meeting']  ||| subjects ==>  ['george']  ||| dialog number ==>  39_0\n",
      "\n",
      "\n",
      "[(['have meeting'], ['Jhon'])]\n",
      "events ==>  ['have meeting']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  40_2\n",
      "\n",
      "\n",
      "[(['meet'], ['Jhon'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  41_0\n",
      "\n",
      "\n",
      "[(['meet'], ['Mark'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  42_1\n",
      "\n",
      "\n",
      "[(['meet'], ['Mark and Jhon'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Mark and Jhon']  ||| dialog number ==>  43_1\n",
      "\n",
      "\n",
      "[(['meet'], ['george'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['george']  ||| dialog number ==>  44_1\n",
      "\n",
      "\n",
      "[(['meet'], ['Mari'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  45_1\n",
      "\n",
      "\n",
      "[(['meet'], ['Jhon'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  46_0\n",
      "\n",
      "\n",
      "[(['our meeting'], ['george and Mari'])]\n",
      "events ==>  ['our meeting']  ||| subjects ==>  ['george and Mari']  ||| dialog number ==>  47_2\n",
      "\n",
      "\n",
      "[(['meet'], ['george'])]\n",
      "events ==>  ['meet']  ||| subjects ==>  ['george']  ||| dialog number ==>  48_2\n",
      "\n",
      "\n",
      "[(['started meeting'], ['Kriss and george'])]\n",
      "events ==>  ['started meeting']  ||| subjects ==>  ['Kriss and george']  ||| dialog number ==>  49_1\n",
      "\n",
      "\n",
      "[(['hurt'], ['Kriss'])]\n",
      "events ==>  ['hurt']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  50_0\n",
      "\n",
      "\n",
      "[(['hurts', 'got headache'], ['george stomach', 'george'])]\n",
      "events ==>  ['hurts', 'got headache']  ||| subjects ==>  ['george stomach', 'george']  ||| dialog number ==>  52_1\n",
      "\n",
      "\n",
      "[(['hurt'], ['Jhon'])]\n",
      "events ==>  ['hurt']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  53_1\n",
      "\n",
      "\n",
      "[(['had surgery'], ['Mark'])]\n",
      "events ==>  ['had surgery']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  54_0\n",
      "\n",
      "\n",
      "[(['born'], ['Kriss'])]\n",
      "events ==>  ['born']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  55_1\n",
      "\n",
      "\n",
      "[(['passed test'], ['Mark'])]\n",
      "events ==>  ['passed test']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  56_1\n",
      "\n",
      "\n",
      "[(['pass test'], ['george'])]\n",
      "events ==>  ['pass test']  ||| subjects ==>  ['george']  ||| dialog number ==>  57_0\n",
      "\n",
      "\n",
      "[(['passed interview'], ['george'])]\n",
      "events ==>  ['passed interview']  ||| subjects ==>  ['george']  ||| dialog number ==>  59_2\n",
      "\n",
      "\n",
      "[(['are pregnant'], ['Abigail'])]\n",
      "events ==>  ['are pregnant']  ||| subjects ==>  ['Abigail']  ||| dialog number ==>  60_0\n",
      "\n",
      "\n",
      "[(['accepted offers', 'come interview'], ['george'])]\n",
      "events ==>  ['accepted offers', 'come interview']  ||| subjects ==>  ['george']  ||| dialog number ==>  62_0\n",
      "\n",
      "\n",
      "[(['buy car'], ['Wilson'])]\n",
      "events ==>  ['buy car']  ||| subjects ==>  ['Wilson']  ||| dialog number ==>  64_1\n",
      "\n",
      "\n",
      "[(['failed test'], ['george'])]\n",
      "events ==>  ['failed test']  ||| subjects ==>  ['george']  ||| dialog number ==>  65_1\n",
      "\n",
      "\n",
      "[(['have interview'], ['Tina'])]\n",
      "events ==>  ['have interview']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  66_2\n",
      "\n",
      "\n",
      "[(['my interview'], ['george'])]\n",
      "events ==>  ['my interview']  ||| subjects ==>  ['george']  ||| dialog number ==>  69_0\n",
      "\n",
      "\n",
      "[(['pass interview'], ['george'])]\n",
      "events ==>  ['pass interview']  ||| subjects ==>  ['george']  ||| dialog number ==>  70_0\n",
      "\n",
      "\n",
      "[(['have interview'], ['Kriss'])]\n",
      "events ==>  ['have interview']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  71_0\n",
      "\n",
      "\n",
      "[(['married'], ['Mark'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  72_1\n",
      "\n",
      "\n",
      "[(['born'], ['george daughter'])]\n",
      "events ==>  ['born']  ||| subjects ==>  ['george daughter']  ||| dialog number ==>  73_1\n",
      "\n",
      "\n",
      "[(['interviewed'], ['Kriss'])]\n",
      "events ==>  ['interviewed']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  74_0\n",
      "\n",
      "\n",
      "[(['interviewed'], ['Jhon'])]\n",
      "events ==>  ['interviewed']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  75_2\n",
      "\n",
      "\n",
      "[(['married'], ['Margaret'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Margaret']  ||| dialog number ==>  76_0\n",
      "\n",
      "\n",
      "[(['engaged'], ['Margaret', matt])]\n",
      "events ==>  ['engaged']  ||| subjects ==>  ['Margaret', matt]  ||| dialog number ==>  77_0\n",
      "\n",
      "\n",
      "[(['engaged'], [nathan, 'Kriss'])]\n",
      "events ==>  ['engaged']  ||| subjects ==>  [nathan, 'Kriss']  ||| dialog number ==>  78_0\n",
      "\n",
      "\n",
      "[(['divorced'], [tomas])]\n",
      "events ==>  ['divorced']  ||| subjects ==>  [tomas]  ||| dialog number ==>  79_2\n",
      "\n",
      "\n",
      "[(['engaged'], [george, 'Mari'])]\n",
      "events ==>  ['engaged']  ||| subjects ==>  [george, 'Mari']  ||| dialog number ==>  80_1\n",
      "\n",
      "\n",
      "[(['married'], ['Jhon'])]\n",
      "events ==>  ['married']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  81_0\n",
      "\n",
      "\n",
      "[(['accepted university'], ['Jhon'])]\n",
      "events ==>  ['accepted university']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  82_2\n",
      "\n",
      "\n",
      "[(['passed exam'], [mario])]\n",
      "events ==>  ['passed exam']  ||| subjects ==>  [mario]  ||| dialog number ==>  83_2\n",
      "\n",
      "\n",
      "[(['signed deal'], ['Mari'])]\n",
      "events ==>  ['signed deal']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  84_1\n",
      "\n",
      "\n",
      "[(['accepted offer', 'my job'], ['Jhon'])]\n",
      "events ==>  ['accepted offer', 'my job']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  85_1\n",
      "\n",
      "\n",
      "[(['accepted school'], [tom])]\n",
      "events ==>  ['accepted school']  ||| subjects ==>  [tom]  ||| dialog number ==>  86_1\n",
      "\n",
      "\n",
      "[(['accepted university'], ['Margaret'])]\n",
      "events ==>  ['accepted university']  ||| subjects ==>  ['Margaret']  ||| dialog number ==>  87_0\n",
      "\n",
      "\n",
      "[(['accepted program'], ['Mark'])]\n",
      "events ==>  ['accepted program']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  88_0\n",
      "\n",
      "\n",
      "[(['won awards'], [bati])]\n",
      "events ==>  ['won awards']  ||| subjects ==>  [bati]  ||| dialog number ==>  89_1\n",
      "\n",
      "\n",
      "[(['celebrating graduation'], ['Mark and george'])]\n",
      "events ==>  ['celebrating graduation']  ||| subjects ==>  ['Mark and george']  ||| dialog number ==>  90_2\n",
      "\n",
      "\n",
      "[(['failed exam'], ['Mari'])]\n",
      "events ==>  ['failed exam']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  91_1\n",
      "\n",
      "\n",
      "[(['win award'], ['Bati'])]\n",
      "events ==>  ['win award']  ||| subjects ==>  ['Bati']  ||| dialog number ==>  92_2\n",
      "\n",
      "\n",
      "[(['accepted offer'], ['Tina'])]\n",
      "events ==>  ['accepted offer']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  93_1\n",
      "\n",
      "\n",
      "[(['passed exam'], [(karla, son)])]\n",
      "events ==>  ['passed exam']  ||| subjects ==>  [(karla, son)]  ||| dialog number ==>  94_2\n",
      "\n",
      "\n",
      "[(['celebrating anniversary'], ['George'])]\n",
      "events ==>  ['celebrating anniversary']  ||| subjects ==>  ['George']  ||| dialog number ==>  96_1\n",
      "\n",
      "\n",
      "[(['won awards'], [' Tina sister'])]\n",
      "events ==>  ['won awards']  ||| subjects ==>  [' Tina sister']  ||| dialog number ==>  97_1\n",
      "\n",
      "\n",
      "[(['accepted university'], ['Jhon'])]\n",
      "events ==>  ['accepted university']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  98_1\n",
      "\n",
      "\n",
      "[(['accepted college'], [tom])]\n",
      "events ==>  ['accepted college']  ||| subjects ==>  [tom]  ||| dialog number ==>  99_2\n",
      "\n",
      "\n",
      "[(['won lawsuit'], [tom])]\n",
      "events ==>  ['won lawsuit']  ||| subjects ==>  [tom]  ||| dialog number ==>  100_2\n",
      "\n",
      "\n",
      "[(['find job'], ['Mark'])]\n",
      "events ==>  ['find job']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  101_0\n",
      "\n",
      "\n",
      "[(['started job'], ['Mark'])]\n",
      "events ==>  ['started job']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  101_1\n",
      "\n",
      "\n",
      "[(['celebrate anniversary'], ['Jhon wife', 'Jhon'])]\n",
      "events ==>  ['celebrate anniversary']  ||| subjects ==>  ['Jhon wife', 'Jhon']  ||| dialog number ==>  102_1\n",
      "\n",
      "\n",
      "[(['lost weight'], ['Jhon'])]\n",
      "events ==>  ['lost weight']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  103_1\n",
      "\n",
      "\n",
      "[(['celebrate birthday'], ['Tina'])]\n",
      "events ==>  ['celebrate birthday']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  104_0\n",
      "\n",
      "\n",
      "[(['accepted job'], ['Mark'])]\n",
      "events ==>  ['accepted job']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  105_1\n",
      "\n",
      "\n",
      "[(['accepted offer'], ['george'])]\n",
      "events ==>  ['accepted offer']  ||| subjects ==>  ['george']  ||| dialog number ==>  106_0\n",
      "\n",
      "\n",
      "[(['bought house'], ['Mari'])]\n",
      "events ==>  ['bought house']  ||| subjects ==>  ['Mari']  ||| dialog number ==>  107_2\n",
      "\n",
      "\n",
      "[(['start job'], ['george'])]\n",
      "events ==>  ['start job']  ||| subjects ==>  ['george']  ||| dialog number ==>  108_0\n",
      "\n",
      "\n",
      "[(['start job'], ['george'])]\n",
      "events ==>  ['start job']  ||| subjects ==>  ['george']  ||| dialog number ==>  108_1\n",
      "\n",
      "\n",
      "[(['broken leg'], ['Mark'])]\n",
      "events ==>  ['broken leg']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  109_1\n",
      "\n",
      "\n",
      "[(['accepted offer'], ['Tom'])]\n",
      "events ==>  ['accepted offer']  ||| subjects ==>  ['Tom']  ||| dialog number ==>  110_0\n",
      "\n",
      "\n",
      "[(['won award'], ['Kriss'])]\n",
      "events ==>  ['won award']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  111_1\n",
      "\n",
      "\n",
      "[(['accepted offer'], ['Frank'])]\n",
      "events ==>  ['accepted offer']  ||| subjects ==>  ['Frank']  ||| dialog number ==>  112_0\n",
      "\n",
      "\n",
      "[(['start job'], ['Tomas'])]\n",
      "events ==>  ['start job']  ||| subjects ==>  ['Tomas']  ||| dialog number ==>  113_0\n",
      "\n",
      "\n",
      "[(['go to'], ['Mark and Jhon'])]\n",
      "events ==>  ['go to']  ||| subjects ==>  ['Mark and Jhon']  ||| dialog number ==>  114_0\n",
      "\n",
      "\n",
      "[(['marked anniversary'], [dani])]\n",
      "events ==>  ['marked anniversary']  ||| subjects ==>  [dani]  ||| dialog number ==>  114_2\n",
      "\n",
      "\n",
      "[(['won prize'], ['Kriss'])]\n",
      "events ==>  ['won prize']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  115_2\n",
      "\n",
      "\n",
      "[(['graduated'], ['Tina'])]\n",
      "events ==>  ['graduated']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  116_0\n",
      "\n",
      "\n",
      "[(['is relationship'], [tony])]\n",
      "events ==>  ['is relationship']  ||| subjects ==>  [tony]  ||| dialog number ==>  117_1\n",
      "\n",
      "\n",
      "[(['has exam'], [(karolina, son)])]\n",
      "events ==>  ['has exam']  ||| subjects ==>  [(karolina, son)]  ||| dialog number ==>  118_1\n",
      "\n",
      "\n",
      "[(['got promotion'], ['Tina'])]\n",
      "events ==>  ['got promotion']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  119_0\n",
      "\n",
      "\n",
      "[(['got offer'], ['Tomas'])]\n",
      "events ==>  ['got offer']  ||| subjects ==>  ['Tomas']  ||| dialog number ==>  120_2\n",
      "\n",
      "\n",
      "[(['am sick'], ['Tina'])]\n",
      "events ==>  ['am sick']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  121_1\n",
      "\n",
      "\n",
      "[(['have fever'], ['Tina'])]\n",
      "events ==>  ['have fever']  ||| subjects ==>  ['Tina']  ||| dialog number ==>  121_2\n",
      "\n",
      "\n",
      "[(['have interview'], ['Jhon'])]\n",
      "events ==>  ['have interview']  ||| subjects ==>  ['Jhon']  ||| dialog number ==>  122_0\n",
      "\n",
      "\n",
      "[(['have surgery'], [mira])]\n",
      "events ==>  ['have surgery']  ||| subjects ==>  [mira]  ||| dialog number ==>  123_2\n",
      "\n",
      "\n",
      "[(['make contract'], [jim])]\n",
      "events ==>  ['make contract']  ||| subjects ==>  [jim]  ||| dialog number ==>  124_0\n",
      "\n",
      "\n",
      "[(['started job'], ['Kriss'])]\n",
      "events ==>  ['started job']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  125_1\n",
      "\n",
      "\n",
      "[(['got job'], [ezabela])]\n",
      "events ==>  ['got job']  ||| subjects ==>  [ezabela]  ||| dialog number ==>  125_2\n",
      "\n",
      "\n",
      "[(['passed exam'], [mario])]\n",
      "events ==>  ['passed exam']  ||| subjects ==>  [mario]  ||| dialog number ==>  126_1\n",
      "\n",
      "\n",
      "[(['finish masters'], ['kareem'])]\n",
      "events ==>  ['finish masters']  ||| subjects ==>  ['kareem']  ||| dialog number ==>  126_2\n",
      "\n",
      "\n",
      "[(['got exam'], ['ezabela'])]\n",
      "events ==>  ['got exam']  ||| subjects ==>  ['ezabela']  ||| dialog number ==>  126_3\n",
      "\n",
      "\n",
      "[(['our wedding'], [' Bati husband and Bati'])]\n",
      "events ==>  ['our wedding']  ||| subjects ==>  [' Bati husband and Bati']  ||| dialog number ==>  127_0\n",
      "\n",
      "\n",
      "[(['my wedding'], ['Kriss'])]\n",
      "events ==>  ['my wedding']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  128_0\n",
      "\n",
      "\n",
      "[(['my birthday'], ['Kriss'])]\n",
      "events ==>  ['my birthday']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  129_0\n",
      "\n",
      "\n",
      "[(['my anniversary'], ['Kriss'])]\n",
      "events ==>  ['my anniversary']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  130_0\n",
      "\n",
      "\n",
      "[(['moving into'], ['bati'])]\n",
      "events ==>  ['moving into']  ||| subjects ==>  ['bati']  ||| dialog number ==>  131_2\n",
      "\n",
      "\n",
      "[(['moving to'], ['Mark'])]\n",
      "events ==>  ['moving to']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  132_1\n",
      "\n",
      "\n",
      "[(['moving to'], ['Kriss'])]\n",
      "events ==>  ['moving to']  ||| subjects ==>  ['Kriss']  ||| dialog number ==>  134_1\n",
      "\n",
      "\n",
      "[(['moving on'], ['mira'])]\n",
      "events ==>  ['moving on']  ||| subjects ==>  ['mira']  ||| dialog number ==>  135_1\n",
      "\n",
      "\n",
      "[(['moving to'], [luke, 'Tomas'])]\n",
      "events ==>  ['moving to']  ||| subjects ==>  [luke, 'Tomas']  ||| dialog number ==>  136_0\n",
      "\n",
      "\n",
      "[(['moving to'], ['Mark'])]\n",
      "events ==>  ['moving to']  ||| subjects ==>  ['Mark']  ||| dialog number ==>  137_0\n",
      "\n",
      "\n",
      "[(['died'], [(joy, father)])]\n",
      "events ==>  ['died']  ||| subjects ==>  [(joy, father)]  ||| dialog number ==>  138_2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# my results events and subjects \n",
    "def get_sub_event_id_from_result(final_results):\n",
    "    dialog_id_results = []\n",
    "    events_results = []\n",
    "    subjects_results = []\n",
    "    for i in final_results:\n",
    "        b = i[0]\n",
    "        print(b)\n",
    "        dialog_id_results.append(i[1])\n",
    "        print(\"events ==> \", str(b[0][0]), \" ||| subjects ==> \", str(b[0][1]), \" ||| dialog number ==> \", i[1])\n",
    "        print(\"\\n\")\n",
    "        events_results.append(b[0][0])\n",
    "        subjects_results.append(b[0][1])\n",
    "    return dialog_id_results, events_results, subjects_results\n",
    "\n",
    "dialog_id_results, events_results, subjects_results = get_sub_event_id_from_result(final_results)\n",
    "\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events ==>  get divorce  ||| subjects ==>  Susan  ||| subjects befor resolution ==>  Susan  ||| dialog number ==>  1_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  2\n",
      "\n",
      "\n",
      "events ==>  got divorce  ||| subjects ==>  Mari,george  ||| subjects befor resolution ==>  they  ||| dialog number ==>  3_1\n",
      "\n",
      "\n",
      "events ==>  married,passed exam  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  she  ||| dialog number ==>  4_1\n",
      "\n",
      "\n",
      "events ==>  had wedding  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  she  ||| dialog number ==>  4_3\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  5\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail,Abigail husband  ||| subjects befor resolution ==>  we  ||| dialog number ==>  6_0\n",
      "\n",
      "\n",
      "events ==>  your wedding  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  your  ||| dialog number ==>  7_0\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail husband,Abigail  ||| subjects befor resolution ==>  my husband,i  ||| dialog number ==>  7_1\n",
      "\n",
      "\n",
      "events ==>  your wedding  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  your  ||| dialog number ==>  8_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  9_2\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  9_3\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  10\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  11\n",
      "\n",
      "\n",
      "events ==>  celebrated birthday  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  12_1\n",
      "\n",
      "\n",
      "events ==>  celebrate news  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  my  ||| dialog number ==>  13_0\n",
      "\n",
      "\n",
      "events ==>  graduated  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  you  ||| dialog number ==>  14_0\n",
      "\n",
      "\n",
      "events ==>  graduated University  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  14_1\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  15\n",
      "\n",
      "\n",
      "events ==>  graduated  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  you  ||| dialog number ==>  16_1\n",
      "\n",
      "\n",
      "events ==>  went to  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  i  ||| dialog number ==>  17_1\n",
      "\n",
      "\n",
      "events ==>  graduated  ||| subjects ==>  Margaret son  ||| subjects befor resolution ==>  your son  ||| dialog number ==>  18_0\n",
      "\n",
      "\n",
      "events ==>  passed exam  ||| subjects ==>  Pamela  ||| subjects befor resolution ==>  she  ||| dialog number ==>  19_1\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Pamela  ||| subjects befor resolution ==>  she  ||| dialog number ==>  19_2\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  20\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail,Mark  ||| subjects befor resolution ==>  we  ||| dialog number ==>  21_2\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  22_0\n",
      "\n",
      "\n",
      "events ==>  invited wedding  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  my  ||| dialog number ==>  22_2\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Margaret  ||| subjects befor resolution ==>  you  ||| dialog number ==>  23_0\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  i  ||| dialog number ==>  24_2\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  george  ||| subjects befor resolution ==>  he  ||| dialog number ==>  25_1\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Abigail,george  ||| subjects befor resolution ==>  they  ||| dialog number ==>  26_1\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Frank  ||| subjects befor resolution ==>  frank  ||| dialog number ==>  27_0\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  28_1\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  29_0\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  i  ||| dialog number ==>  30_0\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  george  ||| subjects befor resolution ==>  he  ||| dialog number ==>  31_1\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  32_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  33\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  34_1\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Margaret  ||| subjects befor resolution ==>  i  ||| dialog number ==>  35_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Margaret,Abigail  ||| subjects befor resolution ==>  we  ||| dialog number ==>  36_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  36_1\n",
      "\n",
      "\n",
      "events ==>  your meeting  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  your  ||| dialog number ==>  37_0\n",
      "\n",
      "\n",
      "events ==>  be meeting  ||| subjects ==>  Mark,Abigail  ||| subjects befor resolution ==>  we  ||| dialog number ==>  37_1\n",
      "\n",
      "\n",
      "events ==>  get meeting  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  my  ||| dialog number ==>  38_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  38_1\n",
      "\n",
      "\n",
      "events ==>  going meeting  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  39_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  40_0\n",
      "\n",
      "\n",
      "events ==>  have meeting  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  40_2\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  41_0\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  42_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Jhon,Mark  ||| subjects befor resolution ==>  we  ||| dialog number ==>  43_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  44_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  i  ||| dialog number ==>  45_1\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  46_0\n",
      "\n",
      "\n",
      "events ==>  our meeting  ||| subjects ==>  Mark,Mari  ||| subjects befor resolution ==>  our  ||| dialog number ==>  47_2\n",
      "\n",
      "\n",
      "events ==>  meet  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  48_2\n",
      "\n",
      "\n",
      "events ==>  started meeting  ||| subjects ==>  george,Kriss  ||| subjects befor resolution ==>  we  ||| dialog number ==>  49_1\n",
      "\n",
      "\n",
      "events ==>  hurt  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  50_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  51_0\n",
      "\n",
      "\n",
      "events ==>  hurts,got headache  ||| subjects ==>  george stomach,george  ||| subjects befor resolution ==>  my stomach,i  ||| dialog number ==>  52_1\n",
      "\n",
      "\n",
      "events ==>  hurt  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  he  ||| dialog number ==>  53_1\n",
      "\n",
      "\n",
      "events ==>  had surgery  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  54_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  54_1\n",
      "\n",
      "\n",
      "events ==>  born  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  55_1\n",
      "\n",
      "\n",
      "events ==>  passed test  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  56_1\n",
      "\n",
      "\n",
      "events ==>  passed test  ||| subjects ==>  george  ||| subjects befor resolution ==>  you  ||| dialog number ==>  57_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  58\n",
      "\n",
      "\n",
      "events ==>  passed interview  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  59_2\n",
      "\n",
      "\n",
      "events ==>  are pregnant  ||| subjects ==>  Abigail  ||| subjects befor resolution ==>  you  ||| dialog number ==>  60_0\n",
      "\n",
      "\n",
      "events ==>  accept invitation  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  61_1\n",
      "\n",
      "\n",
      "events ==>  accepted offers,come interview  ||| subjects ==>  mark  ||| subjects befor resolution ==>  you  ||| dialog number ==>  62_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  63\n",
      "\n",
      "\n",
      "events ==>  buy car  ||| subjects ==>  Wilson  ||| subjects befor resolution ==>  i  ||| dialog number ==>  64_1\n",
      "\n",
      "\n",
      "events ==>  failed test  ||| subjects ==>  george  ||| subjects befor resolution ==>  you  ||| dialog number ==>  65_1\n",
      "\n",
      "\n",
      "events ==>  have interview  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  66_2\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  67\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  68_0\n",
      "\n",
      "\n",
      "events ==>  my interview  ||| subjects ==>  george  ||| subjects befor resolution ==>  my  ||| dialog number ==>  69_0\n",
      "\n",
      "\n",
      "events ==>  pass interview  ||| subjects ==>  george  ||| subjects befor resolution ==>  you  ||| dialog number ==>  70_0\n",
      "\n",
      "\n",
      "events ==>  have interview  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  71_0\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  72_1\n",
      "\n",
      "\n",
      "events ==>  born  ||| subjects ==>  george daughter  ||| subjects befor resolution ==>  my daughter  ||| dialog number ==>  73_1\n",
      "\n",
      "\n",
      "events ==>  interviewed  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  me  ||| dialog number ==>  74_0\n",
      "\n",
      "\n",
      "events ==>  interviewed  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  75_2\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Margaret  ||| subjects befor resolution ==>  i  ||| dialog number ==>  76_0\n",
      "\n",
      "\n",
      "events ==>  engaged  ||| subjects ==>  Margaret,matt  ||| subjects befor resolution ==>  me,matt  ||| dialog number ==>  77_0\n",
      "\n",
      "\n",
      "events ==>  engaged  ||| subjects ==>  nathan,Kriss  ||| subjects befor resolution ==>  nathan,i  ||| dialog number ==>  78_0\n",
      "\n",
      "\n",
      "events ==>  divorced  ||| subjects ==>  tomas  ||| subjects befor resolution ==>  tomas  ||| dialog number ==>  79_2\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  80_0\n",
      "\n",
      "\n",
      "events ==>  engaged  ||| subjects ==>  george,Mari  ||| subjects befor resolution ==>  george,i  ||| dialog number ==>  80_1\n",
      "\n",
      "\n",
      "events ==>  married  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  you  ||| dialog number ==>  81_0\n",
      "\n",
      "\n",
      "events ==>  accepted university  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  82_2\n",
      "\n",
      "\n",
      "events ==>  passed exam  ||| subjects ==>  mario  ||| subjects befor resolution ==>  mario  ||| dialog number ==>  83_2\n",
      "\n",
      "\n",
      "events ==>  signed deal  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  i  ||| dialog number ==>  84_1\n",
      "\n",
      "\n",
      "events ==>  accepted offer,my job  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  I,my  ||| dialog number ==>  85_1\n",
      "\n",
      "\n",
      "events ==>  accepted school  ||| subjects ==>  tom  ||| subjects befor resolution ==>  tom  ||| dialog number ==>  86_1\n",
      "\n",
      "\n",
      "events ==>  accepted university  ||| subjects ==>  Margaret  ||| subjects befor resolution ==>  i  ||| dialog number ==>  87_0\n",
      "\n",
      "\n",
      "events ==>  accepted program  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  88_0\n",
      "\n",
      "\n",
      "events ==>  won awards  ||| subjects ==>  bati  ||| subjects befor resolution ==>  bati  ||| dialog number ==>  89_1\n",
      "\n",
      "\n",
      "events ==>  celebrating graduation  ||| subjects ==>  george's family  ||| subjects befor resolution ==>  we  ||| dialog number ==>  90_2\n",
      "\n",
      "\n",
      "events ==>  failed exam  ||| subjects ==>  Mari  ||| subjects befor resolution ==>  i  ||| dialog number ==>  91_1\n",
      "\n",
      "\n",
      "events ==>  win award  ||| subjects ==>  Bati  ||| subjects befor resolution ==>  i  ||| dialog number ==>  92_2\n",
      "\n",
      "\n",
      "events ==>  accepted offer  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  93_1\n",
      "\n",
      "\n",
      "events ==>  passed exam  ||| subjects ==>  karla son  ||| subjects befor resolution ==>  karla son  ||| dialog number ==>  94_2\n",
      "\n",
      "\n",
      "events ==>  celebrating birthday  ||| subjects ==>  mira  ||| subjects befor resolution ==>  mira  ||| dialog number ==>  95_1\n",
      "\n",
      "\n",
      "events ==>  celebrating anniversary  ||| subjects ==>  George  ||| subjects befor resolution ==>  i  ||| dialog number ==>  96_1\n",
      "\n",
      "\n",
      "events ==>  won awards  ||| subjects ==>  george sister  ||| subjects befor resolution ==>  she  ||| dialog number ==>  97_1\n",
      "\n",
      "\n",
      "events ==>  accepted university  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  98_1\n",
      "\n",
      "\n",
      "events ==>  accepted college  ||| subjects ==>  tom  ||| subjects befor resolution ==>  tom  ||| dialog number ==>  99_2\n",
      "\n",
      "\n",
      "events ==>  won lawsuit  ||| subjects ==>  tom  ||| subjects befor resolution ==>  tom  ||| dialog number ==>  100_2\n",
      "\n",
      "\n",
      "events ==>  find job  ||| subjects ==>  mark  ||| subjects befor resolution ==>  you  ||| dialog number ==>  101_0\n",
      "\n",
      "\n",
      "events ==>  started job  ||| subjects ==>  mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  101_1\n",
      "\n",
      "\n",
      "events ==>  celebrate anniversary  ||| subjects ==>  Jhon wife,Jhon  ||| subjects befor resolution ==>  my wife,i  ||| dialog number ==>  102_1\n",
      "\n",
      "\n",
      "events ==>  lost weight  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  103_1\n",
      "\n",
      "\n",
      "events ==>  celebrate birthday  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  104_0\n",
      "\n",
      "\n",
      "events ==>  accepted job  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  105_1\n",
      "\n",
      "\n",
      "events ==>  accepted offer  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  106_0\n",
      "\n",
      "\n",
      "events ==>  bought house  ||| subjects ==>  mari  ||| subjects befor resolution ==>  she  ||| dialog number ==>  107_2\n",
      "\n",
      "\n",
      "events ==>  start job  ||| subjects ==>  george  ||| subjects befor resolution ==>  you  ||| dialog number ==>  108_0\n",
      "\n",
      "\n",
      "events ==>  start job  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  108_1\n",
      "\n",
      "\n",
      "events ==>  broken leg  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  109_1\n",
      "\n",
      "\n",
      "events ==>  accepted offer  ||| subjects ==>  Tom  ||| subjects befor resolution ==>  i  ||| dialog number ==>  110_0\n",
      "\n",
      "\n",
      "events ==>  won award  ||| subjects ==>  mari  ||| subjects befor resolution ==>  she  ||| dialog number ==>  111_1\n",
      "\n",
      "\n",
      "events ==>  accepted offer  ||| subjects ==>  Frank  ||| subjects befor resolution ==>  i  ||| dialog number ==>  112_0\n",
      "\n",
      "\n",
      "events ==>  start job  ||| subjects ==>  Tomas  ||| subjects befor resolution ==>  i  ||| dialog number ==>  113_0\n",
      "\n",
      "\n",
      "events ==>  None  ||| subjects ==>  None  ||| subjects befor resolution ==>  None  ||| dialog number ==>  114_0\n",
      "\n",
      "\n",
      "events ==>  marked anniversary  ||| subjects ==>  dani  ||| subjects befor resolution ==>  dani  ||| dialog number ==>  114_2\n",
      "\n",
      "\n",
      "events ==>  won prize  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  115_2\n",
      "\n",
      "\n",
      "events ==>  graduated  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  you  ||| dialog number ==>  116_0\n",
      "\n",
      "\n",
      "events ==>  defending thesis  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  my  ||| dialog number ==>  116_1\n",
      "\n",
      "\n",
      "events ==>  is relationship  ||| subjects ==>  tony  ||| subjects befor resolution ==>  tony  ||| dialog number ==>  117_1\n",
      "\n",
      "\n",
      "events ==>  has exam  ||| subjects ==>  karolina son  ||| subjects befor resolution ==>  karolina son  ||| dialog number ==>  118_1\n",
      "\n",
      "\n",
      "events ==>  got promotion  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  119_0\n",
      "\n",
      "\n",
      "events ==>  got offer  ||| subjects ==>  Tomas  ||| subjects befor resolution ==>  i  ||| dialog number ==>  120_2\n",
      "\n",
      "\n",
      "events ==>  am sick  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  121_1\n",
      "\n",
      "\n",
      "events ==>  have fever  ||| subjects ==>  Tina  ||| subjects befor resolution ==>  i  ||| dialog number ==>  121_2\n",
      "\n",
      "\n",
      "events ==>  have interview  ||| subjects ==>  Jhon  ||| subjects befor resolution ==>  i  ||| dialog number ==>  122_0\n",
      "\n",
      "\n",
      "events ==>  have surgery  ||| subjects ==>  mira  ||| subjects befor resolution ==>  mira  ||| dialog number ==>  123_2\n",
      "\n",
      "\n",
      "events ==>  make contract  ||| subjects ==>  Jim  ||| subjects befor resolution ==>  jim  ||| dialog number ==>  124_0\n",
      "\n",
      "\n",
      "events ==>  started job  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  125_1\n",
      "\n",
      "\n",
      "events ==>  got job  ||| subjects ==>  ezabela  ||| subjects befor resolution ==>  ezabela  ||| dialog number ==>  125_2\n",
      "\n",
      "\n",
      "events ==>  passed exam  ||| subjects ==>  mario  ||| subjects befor resolution ==>  mario  ||| dialog number ==>  126_1\n",
      "\n",
      "\n",
      "events ==>  finish masters  ||| subjects ==>  kareem  ||| subjects befor resolution ==>  his  ||| dialog number ==>  126_2\n",
      "\n",
      "\n",
      "events ==>  got exam  ||| subjects ==>  ezabela  ||| subjects befor resolution ==>  her  ||| dialog number ==>  126_3\n",
      "\n",
      "\n",
      "events ==>  our wedding  ||| subjects ==>  Bati husband,Bati  ||| subjects befor resolution ==>  our  ||| dialog number ==>  127_0\n",
      "\n",
      "\n",
      "events ==>  my wedding  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  my  ||| dialog number ==>  128_0\n",
      "\n",
      "\n",
      "events ==>  my birthday  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  my  ||| dialog number ==>  129_0\n",
      "\n",
      "\n",
      "events ==>  my anniversary  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  my  ||| dialog number ==>  130_0\n",
      "\n",
      "\n",
      "events ==>  moving into  ||| subjects ==>  bati  ||| subjects befor resolution ==>  she  ||| dialog number ==>  131_2\n",
      "\n",
      "\n",
      "events ==>  moving to  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  132_1\n",
      "\n",
      "\n",
      "events ==>  traveling to  ||| subjects ==>  george  ||| subjects befor resolution ==>  i  ||| dialog number ==>  133_0\n",
      "\n",
      "\n",
      "events ==>  moving to  ||| subjects ==>  Kriss  ||| subjects befor resolution ==>  i  ||| dialog number ==>  134_1\n",
      "\n",
      "\n",
      "events ==>  moving on  ||| subjects ==>  mira  ||| subjects befor resolution ==>  she  ||| dialog number ==>  135_1\n",
      "\n",
      "\n",
      "events ==>  moving to  ||| subjects ==>  luke,Tomas  ||| subjects befor resolution ==>  luke, i  ||| dialog number ==>  136_0\n",
      "\n",
      "\n",
      "events ==>  moving to  ||| subjects ==>  Mark  ||| subjects befor resolution ==>  i  ||| dialog number ==>  137_0\n",
      "\n",
      "\n",
      "events ==>  died  ||| subjects ==>  Joy father  ||| subjects befor resolution ==>  joy father  ||| dialog number ==>  138_2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#getting events and subjects from labeled dataset\n",
    "def get_sub_event_id_from_labeledData(output):\n",
    "    events=[]\n",
    "    subjects=[]\n",
    "    dialog_id=[]\n",
    "    subjects_no_resolution=[]\n",
    "    for i in range(len(output)):\n",
    "        events.append(output[\"events\"][i])\n",
    "        subjects.append(output[\"subjects\"][i])\n",
    "        dialog_id.append(output[\"dialog_id\"][i])\n",
    "        subjects_no_resolution.append(output[\"subjects_no_resolution\"][i])\n",
    "    for i in range(len(events)):\n",
    "        print(\"events ==> \",events[i], \" ||| subjects ==> \",subjects[i],\" ||| subjects befor resolution ==> \",subjects_no_resolution[i],\" ||| dialog number ==> \",dialog_id[i])\n",
    "        print(\"\\n\")\n",
    "    return dialog_id,events,subjects,subjects_no_resolution\n",
    "\n",
    "dialog_id,events,subjects,subjects_no_resolution = get_sub_event_id_from_labeledData(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocessing on labeled dataset for easier matching with results\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing_on_labeledData(events,subjects,subjects_no_resolution):\n",
    "    events_preprocessed = []\n",
    "    subjects_preprocessed = []\n",
    "    subjects_no_resolution_preprocessed = []\n",
    "    for i, j in enumerate(events):\n",
    "\n",
    "        # sentence has more one events like 'passed exam,married' transformed to [passed exam,married]\n",
    "        if \",\" in str(events[i]):\n",
    "            b = []\n",
    "            c = events[i]\n",
    "            b.append(c[:c.find(\",\")])\n",
    "            b.append(c[c.find(\",\") + 1:])\n",
    "\n",
    "            events_preprocessed.append(b)\n",
    "        if \",\" not in str(events[i]):\n",
    "            events_preprocessed.append([events[i]])\n",
    "\n",
    "        # the same for subjects after resolution\n",
    "    for i, j in enumerate(subjects):\n",
    "        if \",\" in str(subjects[i]):\n",
    "            d = []\n",
    "            c = subjects[i]\n",
    "            d.append(c[:c.find(\",\")])\n",
    "            d.append(c[c.find(\",\") + 1:])\n",
    "\n",
    "            subjects_preprocessed.append(d)\n",
    "        if \",\" not in str(subjects[i]):\n",
    "            subjects_preprocessed.append([subjects[i]])\n",
    "\n",
    "        # the same for subjects befor resolution\n",
    "    for i, j in enumerate(subjects_no_resolution):\n",
    "        if \",\" in str(subjects_no_resolution[i]):\n",
    "            e = []\n",
    "            c = subjects_no_resolution[i]\n",
    "            e.append(c[:c.find(\",\")])\n",
    "            e.append(c[c.find(\",\") + 1:])\n",
    "\n",
    "            subjects_no_resolution_preprocessed.append(e)\n",
    "\n",
    "        if \",\" not in str(subjects_no_resolution[i]):\n",
    "            subjects_no_resolution_preprocessed.append([subjects_no_resolution[i]])\n",
    "    return events_preprocessed,subjects_preprocessed,subjects_no_resolution_preprocessed\n",
    "\n",
    "events_preprocessed,subjects_preprocessed,subjects_no_resolution_preprocessed = preprocessing_on_labeledData(events,subjects,subjects_no_resolution)\n",
    "\n",
    "\n",
    "\n",
    "# all this for subjects results after pronoun resolution\n",
    "\n",
    "# we and they resolution maybe get subjects like \"yara and hadia\" will transformed to ['yara','hadia']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_on_resultSubjects(subjects_results):\n",
    "    subjects_results_preprocessed = []\n",
    "    for i, j in enumerate(subjects_results):\n",
    "        if \" and \" in str(subjects_results[i]):\n",
    "            b = []\n",
    "            c = subjects_results[i][0]\n",
    "            b.append(c[:c.find(\" and \")])\n",
    "            b.append(c[c.find(\" and \") + 5:])\n",
    "            b.sort()\n",
    "            subjects_results_preprocessed.append(b)\n",
    "        else:\n",
    "            subjects_results_preprocessed.append(subjects_results[i])\n",
    "\n",
    "        # subjects like joy's father .. my results return it like [('joy','father')] so will transformed to ['joy father']\n",
    "        for k, l in enumerate(subjects_results[i]):\n",
    "            if isinstance(subjects_results[i][k], tuple) == True:\n",
    "                c=subjects_results[i][k]\n",
    "                \n",
    "                subjects_results_preprocessed[i][k] = str(c[0]) + \" \" + str(c[1])\n",
    "                \n",
    "            else:\n",
    "                subjects_results_preprocessed[i][k]=subjects_results_preprocessed[i][k]\n",
    "                \n",
    "    return subjects_results_preprocessed\n",
    "subjects_results_preprocessed = preprocessing_on_resultSubjects(subjects_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[susan], [they], [she], [she], [we], [your], [(my, husband), i], [your], [we], [i], [you], [you], [i], [you], [i], [(your, son)], [she], [she], [i], [i], [my], [you], [i], [he], [they], [frank], [i], [i], [i], [he], [i], [i], [i], [we], [your], [we], [my], [i], [i], [i], [i], [i], [we], [i], [i], [i], [our], [i], [we], [i], [(my, stomach), i], [he], [i], [i], [i], [you], [i], [you], [i], [i], [you], [i], [my], [you], [i], [i], [(my, daughter)], [me], [i], [i], ['Margaret', matt], [nathan, 'Kriss'], [tomas], [george, 'Mari'], [you], [i], [mario], [i], ['Jhon'], [tom], [i], [i], [bati], [we], [i], [i], [i], ['karla son'], [i], [she], [i], [tom], [tom], [you], [i], [(my, wife), i], [i], [i], [i], [i], [she], [you], [i], [i], [i], [i], [i], [i], [we], [dani], [i], [you], [tony], ['karolina son'], [i], [i], [i], [i], [i], [mira], [jim], [i], [ezabela], [mario], [his], [her], [our], [my], [my], [my], [she], [i], [i], [she], [luke, 'Tomas'], [i], ['joy father']]\n"
     ]
    }
   ],
   "source": [
    "def get_subject_result_befor_resolution_withoutID(subject_result_befor_resolution):\n",
    "    subject_result_befor_resolution_withoutID = []\n",
    "    for i in subject_result_befor_resolution:\n",
    "        b = i[0]\n",
    "        subject_result_befor_resolution_withoutID.append(b)\n",
    "    return subject_result_befor_resolution_withoutID\n",
    "\n",
    "subject_result_befor_resolution_withoutID = get_subject_result_befor_resolution_withoutID(subject_result_befor_resolution)\n",
    "\n",
    "\n",
    "print(subject_result_befor_resolution_withoutID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_on_subject_result_befor_resolution_withoutID(subject_result_befor_resolution_withoutID):\n",
    "    for i,j in enumerate(subject_result_befor_resolution_withoutID):        \n",
    "        for k,l in enumerate(subject_result_befor_resolution_withoutID[i]):\n",
    "            if k<=len(subject_result_befor_resolution_withoutID[i])   :\n",
    "\n",
    "                if isinstance(l,tuple)==True:\n",
    "\n",
    "                    subject_result_befor_resolution_withoutID[i][k]=str(l[0])+\" \"+str(l[1])\n",
    "                else:  \n",
    "                    subject_result_befor_resolution_withoutID[i][k]=subject_result_befor_resolution_withoutID[i][k]\n",
    "\n",
    "                \n",
    "    return subject_result_befor_resolution_withoutID\n",
    "\n",
    "\n",
    "subject_result_befor_resolution_withoutID = preprocessing_on_subject_result_befor_resolution_withoutID(subject_result_befor_resolution_withoutID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[susan], [they], [she], [she], [we], [your], ['my husband', i], [your], [we], [i], [you], [you], [i], [you], [i], ['your son'], [she], [she], [i], [i], [my], [you], [i], [he], [they], [frank], [i], [i], [i], [he], [i], [i], [i], [we], [your], [we], [my], [i], [i], [i], [i], [i], [we], [i], [i], [i], [our], [i], [we], [i], ['my stomach', i], [he], [i], [i], [i], [you], [i], [you], [i], [i], [you], [i], [my], [you], [i], [i], ['my daughter'], [me], [i], [i], ['Margaret', matt], [nathan, 'Kriss'], [tomas], [george, 'Mari'], [you], [i], [mario], [i], ['Jhon'], [tom], [i], [i], [bati], [we], [i], [i], [i], ['karla son'], [i], [she], [i], [tom], [tom], [you], [i], ['my wife', i], [i], [i], [i], [i], [she], [you], [i], [i], [i], [i], [i], [i], [we], [dani], [i], [you], [tony], ['karolina son'], [i], [i], [i], [i], [i], [mira], [jim], [i], [ezabela], [mario], [his], [her], [our], [my], [my], [my], [she], [i], [i], [she], [luke, 'Tomas'], [i], ['joy father']]\n"
     ]
    }
   ],
   "source": [
    "print(subject_result_befor_resolution_withoutID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill lists with None values with the same labeled data long\n",
    "def fill_with_null(dialog_id):\n",
    "    null_subject=[]\n",
    "    null_subject_befor_resolution=[]\n",
    "    null_events=[]\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        null_subject.append([['None'],dialog_id[i]])\n",
    "        null_events.append([['None'],dialog_id[i]])\n",
    "        null_subject_befor_resolution.append([['None'],dialog_id[i]])\n",
    "    return null_subject,null_events,null_subject_befor_resolution\n",
    "\n",
    "null_subject,null_events,null_subject_befor_resolution = fill_with_null(dialog_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[susan], '1_0'], [['None'], '2'], [[' Mari', 'george'], '3_1'], [['Mari'], '4_1'], [['She'], '4_3'], [['None'], '5'], [['Abigail', 'Jhon'], '6_0'], [['Abigail'], '7_0'], [['Abigail husband', 'Abigail'], '7_1'], [['Abigail'], '8_0'], [['None'], '9_2'], [['Jhon', 'Mark'], '9_3'], [['None'], '10'], [['None'], '11'], [['Jhon'], '12_1'], [['Jhon'], '13_0'], [['Jhon'], '14_0'], [['Jhon'], '14_1'], [['None'], '15'], [['Abigail'], '16_1'], [['Abigail'], '17_1'], [['Margaret son'], '18_0'], [['Pamela'], '19_1'], [['Pamela'], '19_2'], [['None'], '20'], [['Mark'], '21_2'], [['Mark'], '22_0'], [['Mark'], '22_2'], [['Margaret'], '23_0'], [['Abigail'], '24_2'], [['george'], '25_1'], [[' Abigail', 'george'], '26_1'], [[frank], '27_0'], [['Mark'], '28_1'], [['Mark'], '29_0'], [['Abigail'], '30_0'], [['george'], '31_1'], [['Mark'], '32_0'], [['None'], '33'], [['Jhon'], '34_1'], [['Margaret'], '35_1'], [['Abigail', 'Margaret'], '36_0'], [['None'], '36_1'], [['Mark'], '37_0'], [[''], '37_1'], [['Mark'], '38_0'], [['Mark'], '38_1'], [['george'], '39_0'], [['None'], '40_0'], [['Jhon'], '40_2'], [['Jhon'], '41_0'], [['Mark'], '42_1'], [['Jhon', 'Mark'], '43_1'], [['george'], '44_1'], [['Mari'], '45_1'], [['Jhon'], '46_0'], [['Mari', 'george'], '47_2'], [['george'], '48_2'], [['Kriss', 'george'], '49_1'], [['Kriss'], '50_0'], [['None'], '51_0'], [['george stomach', 'george'], '52_1'], [['Jhon'], '53_1'], [['Mark'], '54_0'], [['None'], '54_1'], [['Kriss'], '55_1'], [['Mark'], '56_1'], [['george'], '57_0'], [['None'], '58'], [['george'], '59_2'], [['Abigail'], '60_0'], [['None'], '61_1'], [['george'], '62_0'], [['None'], '63'], [['Wilson'], '64_1'], [['george'], '65_1'], [['Tina'], '66_2'], [['None'], '67'], [['None'], '68_0'], [['george'], '69_0'], [['george'], '70_0'], [['Kriss'], '71_0'], [['Mark'], '72_1'], [['george daughter'], '73_1'], [['Kriss'], '74_0'], [['Jhon'], '75_2'], [['Margaret'], '76_0'], [['Margaret', matt], '77_0'], [[nathan, 'Kriss'], '78_0'], [[tomas], '79_2'], [['None'], '80_0'], [[george, 'Mari'], '80_1'], [['Jhon'], '81_0'], [['Jhon'], '82_2'], [[mario], '83_2'], [['Mari'], '84_1'], [['Jhon'], '85_1'], [[tom], '86_1'], [['Margaret'], '87_0'], [['Mark'], '88_0'], [[bati], '89_1'], [['Mark', 'george'], '90_2'], [['Mari'], '91_1'], [['Bati'], '92_2'], [['Tina'], '93_1'], [['karla son'], '94_2'], [['None'], '95_1'], [['George'], '96_1'], [[' Tina sister'], '97_1'], [['Jhon'], '98_1'], [[tom], '99_2'], [[tom], '100_2'], [['Mark'], '101_0'], [['Mark'], '101_1'], [['Jhon wife', 'Jhon'], '102_1'], [['Jhon'], '103_1'], [['Tina'], '104_0'], [['Mark'], '105_1'], [['george'], '106_0'], [['Mari'], '107_2'], [['george'], '108_0'], [['george'], '108_1'], [['Mark'], '109_1'], [['Tom'], '110_0'], [['Kriss'], '111_1'], [['Frank'], '112_0'], [['Tomas'], '113_0'], [['Jhon', 'Mark'], '114_0'], [[dani], '114_2'], [['Kriss'], '115_2'], [['Tina'], '116_0'], [['None'], '116_1'], [[tony], '117_1'], [['karolina son'], '118_1'], [['Tina'], '119_0'], [['Tomas'], '120_2'], [['Tina'], '121_1'], [['Tina'], '121_2'], [['Jhon'], '122_0'], [[mira], '123_2'], [[jim], '124_0'], [['Kriss'], '125_1'], [[ezabela], '125_2'], [[mario], '126_1'], [['kareem'], '126_2'], [['ezabela'], '126_3'], [[' Bati husband', 'Bati'], '127_0'], [['Kriss'], '128_0'], [['Kriss'], '129_0'], [['Kriss'], '130_0'], [['bati'], '131_2'], [['Mark'], '132_1'], [['None'], '133_0'], [['Kriss'], '134_1'], [['mira'], '135_1'], [[luke, 'Tomas'], '136_0'], [['Mark'], '137_0'], [['joy father'], '138_2']]\n"
     ]
    }
   ],
   "source": [
    "def fill_null_with_value_of_subjects_results(null_subject,subjects_results_preprocessing):\n",
    "\n",
    "    for i,j in enumerate(null_subject):\n",
    "        for k,l in enumerate(subjects_results_preprocessing):\n",
    "            if str(dialog_id_results[k]).strip() == str(null_subject[i][1]).strip():\n",
    "                null_subject[i][0]=subjects_results_preprocessing[k]\n",
    "                null_events[i][0]=events_results[k]\n",
    "                null_subject_befor_resolution[i][0]=subject_result_befor_resolution_withoutID[k]\n",
    "    return null_subject,null_events,null_subject_befor_resolution\n",
    "final_subjects_res_preprocessing,final_events_res_preprocessing,final_subject_befor_resolution_res_preprocessing = fill_null_with_value_of_subjects_results(null_subject,subjects_results_preprocessed)\n",
    "print(final_subjects_res_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating the 3 steps ( events extraction ,subject extraction , pronoun resolution )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compar null_subject with subjects_ from labeled data\n",
    "#compar null_event with events_ from labeled data\n",
    "\n",
    "def evaluation(dialog_id=[],final_subjects_res_preprocessing=[],subjects_preprocessed=[]):\n",
    "\n",
    "    count_true_negative=0\n",
    "    count_true_positive=0\n",
    "    count_false_positive=0\n",
    "    count_false_negative=0\n",
    "    a_list=set()\n",
    "    b_list=set()\n",
    "    c_list=set()\n",
    "    d_list=set()\n",
    "    o_list=set()\n",
    "\n",
    "    count_other=0\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                    for c,m in enumerate(final_subjects_res_preprocessing[i][0]):\n",
    "                        if str(subjects_preprocessed[i][k]).lower().strip()==str(final_subjects_res_preprocessing[i][0][c]).lower().strip() and str(subjects_preprocessed[i][k]).lower().strip()==\"none\" and str(final_subjects_res_preprocessing[i][0][c]).lower().strip()==\"none\":\n",
    "                            a_list.add(dialog_id[i])\n",
    "                            count_true_negative+=1\n",
    "\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if dialog_id[i] not in a_list:\n",
    "            if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                    for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                        for c,m in enumerate(final_subjects_res_preprocessing[i][0]):                        \n",
    "                            if str(subjects_preprocessed[i][k]).lower().strip()==str(final_subjects_res_preprocessing[i][0][c]).lower().strip() and str(subjects_preprocessed[i][k]).lower().strip()!=\"none\" and str(final_subjects_res_preprocessing[i][0][c]).lower().strip()!=\"none\":\n",
    "                                count_true_positive+=1\n",
    "                                b_list.add(dialog_id[i])\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if dialog_id[i] not in a_list and dialog_id[i] not in b_list:\n",
    "            if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                    for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                        for c,m in enumerate(final_subjects_res_preprocessing[i][0]):                    \n",
    "                            if str(subjects_preprocessed[i][k]).lower().strip()!=str(final_subjects_res_preprocessing[i][0][c]).lower().strip() and str(subjects_preprocessed[i][k]).lower().strip()==\"none\" and str(final_subjects_res_preprocessing[i][0][c]).lower().strip()!=\"none\":\n",
    "                                count_false_positive+=1\n",
    "                                c_list.add(dialog_id[i])\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if dialog_id[i] not in a_list and dialog_id[i] not in b_list and dialog_id[i] not in c_list:\n",
    "            if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                    for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                        for c,m in enumerate(final_subjects_res_preprocessing[i][0]):                        \n",
    "                            if str(subjects_preprocessed[i][k]).lower().strip()!=str(final_subjects_res_preprocessing[i][0][c]).lower().strip() and str(subjects_preprocessed[i][k]).lower().strip()!=\"none\" and str(final_subjects_res_preprocessing[i][0][c]).lower().strip()==\"none\":\n",
    "                                count_false_negative+=1\n",
    "                                d_list.add(dialog_id[i])\n",
    "\n",
    "    for i,j in enumerate(dialog_id):\n",
    "        if dialog_id[i] not in a_list and dialog_id[i] not in b_list and dialog_id[i] not in c_list and dialog_id[i] not in d_list:\n",
    "            if str(dialog_id[i])==str(final_subjects_res_preprocessing[i][1]):\n",
    "                    for k,l in enumerate(subjects_preprocessed[i]):\n",
    "                        for c,m in enumerate(final_subjects_res_preprocessing[i][0]):                            \n",
    "                            if (str(subjects_preprocessed[i][k]).lower().strip()!=str(final_subjects_res_preprocessing[i][0][c]).lower().strip()) and (str(subjects_preprocessed[i][k]).lower().strip()!=\"none\") and (str(final_subjects_res_preprocessing[i][0][c]).lower().strip()!=\"none\"):\n",
    "                                count_other+=1\n",
    "                                o_list.add(dialog_id[i])\n",
    "                    \n",
    "    \n",
    "    \n",
    "    return a_list,b_list,c_list,d_list,o_list\n",
    "    \n",
    "#compare between subjects_ which is the labeled and preprocessed subject in data  .. with subject results after fill empty results \n",
    "#with Null value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_negative for subjects with pronoun resolution  :  17\n",
      "true_positive for subjects with pronoun resolution  :  127\n",
      "false_positive for subjects with pronoun resolution  :  10\n",
      "false_negative for subjects with pronoun resolution  :  4\n",
      "Precision  0.927007299270073\n",
      "Recall  0.9694656488549618\n"
     ]
    }
   ],
   "source": [
    "a_list,b_list,c_list,d_list,o_list=evaluation(dialog_id,final_subjects_res_preprocessing,subjects_preprocessed)\n",
    "\n",
    "print(\"true_negative for subjects with pronoun resolution  : \",len(a_list))\n",
    "print(\"true_positive for subjects with pronoun resolution  : \",len(b_list))\n",
    "print(\"false_positive for subjects with pronoun resolution  : \",len(c_list)+len(o_list))\n",
    "print(\"false_negative for subjects with pronoun resolution  : \",len(d_list))\n",
    "    \n",
    "#Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "#Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "print(\"Precision \",(float)(len(b_list)/(float)((len(b_list))+(len(c_list)+len(o_list)))))\n",
    "print(\"Recall \",(float)(len(b_list)/(float)(len(b_list)+len(d_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_negative for subjects befor pronoun resolution  :  17\n",
      "true_positive for subjects befor pronoun resolution  :  129\n",
      "false_positive for subjects befor pronoun resolution  :  8\n",
      "false_negative for subjects befor pronoun resolution  :  4\n",
      "Precision  0.9416058394160584\n",
      "Recall  0.9699248120300752\n"
     ]
    }
   ],
   "source": [
    "a_list,b_list,c_list,d_list,o_list=evaluation(dialog_id,final_subject_befor_resolution_res_preprocessing,subjects_no_resolution_preprocessed)\n",
    "\n",
    "print(\"true_negative for subjects befor pronoun resolution  : \",len(a_list))\n",
    "print(\"true_positive for subjects befor pronoun resolution  : \",len(b_list))\n",
    "print(\"false_positive for subjects befor pronoun resolution  : \",len(c_list)+len(o_list))\n",
    "print(\"false_negative for subjects befor pronoun resolution  : \",len(d_list))\n",
    "\n",
    "#Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "#Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "print(\"Precision \",(float)(len(b_list)/(float)((len(b_list))+(len(c_list)+len(o_list)))))\n",
    "print(\"Recall \",(float)(len(b_list)/(float)(len(b_list)+len(d_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_negative for events  :  17\n",
      "true_positive for events  :  132\n",
      "false_positive for events  :  5\n",
      "false_negative for events  :  4\n",
      "Precision  0.9635036496350365\n",
      "Recall  0.9705882352941176\n"
     ]
    }
   ],
   "source": [
    "a_list,b_list,c_list,d_list,o_list=evaluation(dialog_id,final_events_res_preprocessing,events_preprocessed)\n",
    "\n",
    "print(\"true_negative for events  : \",len(a_list))\n",
    "print(\"true_positive for events  : \",len(b_list))\n",
    "print(\"false_positive for events  : \",len(c_list)+len(o_list))\n",
    "print(\"false_negative for events  : \",len(d_list))\n",
    "    \n",
    "#Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "#Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "print(\"Precision \",(float)(len(b_list)/(float)((len(b_list))+(len(c_list)+len(o_list)))))\n",
    "print(\"Recall \",(float)(len(b_list)/(float)(len(b_list)+len(d_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some testing example on finding subjects functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[we]\n",
      "[(my, stomach)]\n",
      "None\n",
      "[(karla, sun)]\n",
      "[(my, brother)]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"my husband's parents flew out to meet my family when we got married in my hometown , so that was great \"\n",
    "),\"married\"))\n",
    "print(find_sub_for_verb(nlp(\"i've got a really bad headache and my stomach hurts\"),\"hurts\"))\n",
    "\n",
    "print(find_sub_for_verb(nlp(\"people divorced every year\"),\"divorced\"))\n",
    "print(find_sub_for_verb(nlp(\"karla's sun have an exam\"),\"have\"))\n",
    "print(find_sub_for_verb(nlp(\"my brother have an exam\"),\"have\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sam, george]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"sam and george have an exam\"),\"have\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(find_sub_for_verb(nlp(\"i'm interviewed on montana public radio today about my life\"),\"interviewed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[she, george, sam]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"i heard she and george and sam has passed the bar exam and married recently \"),\"married\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(find_sub_for_verb(nlp(\"george went for a job interview earlier and got the job \"),\"got\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_actionEvents(nlp(\"george went for a job interview earlier and got the job\"),verb_with_prepo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"george went for a job interview earlier and got the job\"),nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, wife), i]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"today my wife and i celebrate our 1st wedding anniversary \"),\"celebrate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[she]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"i heard she has passed the bar exam and married recently \"),\"passed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(joy, father)]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"joy's father died and he is very depressed\"),\"died\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Marla]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"Marla will have an surgery next week \"),\"have\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nathan, i]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"nathan and i married last week \"),\"married\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tomas]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"tomas divorced katty and take her alone .\"),\"divorced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[you, me]\n"
     ]
    }
   ],
   "source": [
    "print(find_sub_for_verb(nlp(\"you and me will married this month .\"),\"married\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Marla]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"Marla will have an surgery next week \"),nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, stomach), george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"i've got a really bad headache and my stomach and george hurts\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[my]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"(you are invited to my wedding\"),nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(your, son)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"i heard your son recently graduated\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, family)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"my husband flew out to meet my family when we got married in my hometown \"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, thesis)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"defending my thesis was great\"),verbs_with_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(karla, sun)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"karla's sun passed the exam in 1999\"),verbs_with_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[you]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"do you want to go out to celebrate my good news\"),verbs_with_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george, i]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"should probably let u all know that george and i are now engaged\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, paper), george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"my paper and george was published in 1999\"),verbs_with_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[me]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"the studio interviewed me last year\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(my, boss)]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_verbEvents(nlp(\"the studio interviewed my boss last year\"),verbs_without_need_objects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[my]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_direct_relation(nlp(\"my wedding was last week\"),noun_direct_relation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_adjectiveEvents(nlp(\"i'm very sick and i have a fever\"),adjectives))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[george]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"george will have an surgery next week \"),nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[his]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"kareem get to finish his masters \"),nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kareem]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"kareem get to finish masters \"),nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[we]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_actionEvents(nlp(\"we are moving to palmera last week\"),verb_with_prepo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kareem]\n"
     ]
    }
   ],
   "source": [
    "print(findSubs_for_nounsEvents(nlp(\"kareem get to finish masters \"),nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
